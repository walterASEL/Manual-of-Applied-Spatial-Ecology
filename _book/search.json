[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manual of Applied Spatial Ecology",
    "section": "",
    "text": "New version of manual after conversion to terra and sf packages”"
  },
  {
    "objectID": "Preface.html",
    "href": "Preface.html",
    "title": "2  Preface",
    "section": "",
    "text": "The purpose of this manual is to assist researchers on methods for data management and analysis using the R environment or other software after data has been collected in the field. The impetus behind this manual was from many years of frustration in trying to analyze data in R using code and forgetting how it was done upon completion of a study. We wanted to find a way to avoid needing to search computers for folders to find old R code then try to remember what we did to the data to get the code to run properly. Over the years, advancements in data handling and manipulation, GIS capabilities, and methods of estimators for home range, movements, resource selection, and spatial epidemiology have occurred within the R environment. Program R is free and used by researchers world-wide but R also provides a platform to create and display spatial layers without the need for the variety of GUI software, free or otherwise. Furthermore, analyzing spatial data in R enables statistical analysis of data without the errors that may arise from bringing data from spreadsheet or GIS software to statistical programs. We would like to stress that this manual is not the authority on all topics presented herein. Our goal was to create an online manual that could be easily followed by researchers, biologists, or graduate students to analyze their data in R. Although the user would benefit from general introductory knowledge of using R and ArcMap, most of the manual is for mid-level users of R that need guidance beyond the basics of introductory R and GIS coureses. We also provide numerous citations throughout each section should the user choose to learn the theory or more details behind each topic. In addition, this manual provides a handy outline of course materials for an Applied Spatial Ecology course that will surely expand or change as the field evolves. As time permits and errors are brought to our attention, we plan to update and correct problems so be sure to send any corrections or comments our way.\nAny use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government.\nRecommended citation:\nWalter, W.D. Manual of Applied Spatial Ecology. Walter Applied Spatial Ecology Lab, Pennsylvania State University, University Park. Access Date. http://ecosystems.psu.edu/research/labs/walter-lab.\nAcknowledgments\nNumerous colleagues have provide assistance with R packages they created or with code they have provided in some other form. We would be remiss if we failed to thank these colleagues for their hard work:\nSharon Baruch-Mordo, The Nature Conservancy, Fort Collins, CO 80524 ; sbaruch-mordo@tnc.org Simon Benhamou, Centre d’Ecologie Fonctionnelle et Evolutive, France Clément Calenge, Data Analysis Support Unit, Directorate for Studies and Research, National Office of Hunting and Wild Fauna, Saint Benoist - 78610 Auffargis, France Mevin Hooten, Colorado Cooperative Fish and Wildlife Research Unit, 201 Wagar Bldg, Colorado State University, Fort Collins, CO 8052 ; Mevin.Hooten@colostate.edu Bill Kanapaux, Pennsylvania Cooperative Fish and Wildlife Research Unit, 406 Forest Resources Bldg., Pennyslvania State University, University Park, PA 16802 ; wjk15@psu.edu Bart Kranstauber, Max Planck Institute for Ornithology, Eberhard-Gwinner-Str., 82319 Seewiesen; bart.kranstauber@uni-konstanz.de Ryan Nielsen, West Inc.,415 W. 17th St. Suite 200, Cheyenne, WY 82001 ; rnielson@westinc. com Glen Sargeant, Northern Prairie Wildlife Research Center, Jamestown, North Dakota; glen_sargeant@usgs.gov Peter Singleton, USDA Forest Service, Pacific Northwest Research Station in Wenatchee, WA; singlep@u.washington.edu Marcó Smolla, Max Planck Institute for Ornithology, Dept. Migration and Immuno-ecology, Am Obstberg 1, 78315 Radolfzell, Germany; msmolla@orn.mpg.de Tyler Wagner, US Geological Survey, Pennsylvania Cooperative Fish and Wildlife Research Unit, 402 Forest Resources Bldg., Pennsylvania State University, University Park, PA 16802 ; twagner@psu.edu"
  },
  {
    "objectID": "Intro.html",
    "href": "Intro.html",
    "title": "\n3  Introduction\n",
    "section": "",
    "text": "The original Manual of Applied Spatial Ecology (hereafter referred to as Manual) went online in 2014 with a 173 page manual created in Latex and scripts/datasets available on GitHub a short time thereafter. Since the scripts were .R files, they were then converted into .Rmd files in attempts to create stand-alone pdfs for each chapter. Latex of the entire compilation was too time consuming to maintain so .Rmds for each exercise were an early solution. The resulting chapter pdfs were created with every update and version of each exercise pushed to GitHub to easily update code or package changes. Although the manual pdf was not edited or changed since 2016, the pdfs within each exercise on GitHub were updated each year.\nFast forward to 2023, I was challenged with whether it was even worth maintaining this Manual considering so many R packages and code were required to be published with most manuscripts. In addition, with the increase in spatial data available and R code to prepare spatial data (some within my own lab), several chapters in the Manual seem outdated with no reason to maintain these chapters (Chapter 2 for example). Combine that with deprecating of packages rgdal and raster for their equivalents, sf and terra, respectively, most exercises in the Manual would no longer be functional after December 2023.\nThe impetus for creating this manual in 2014, however, was a place to store code and revisit functions and code I created or found online in one place to use on new projects. GitHub provides a nice portal to store and organize code that might not be published and for my exercises to be available for graduate students in my course at The Pennsylvania State University - . Even though I have countless scripts and functions that are not available in the Manual, maintaining R code (GitHub) and a pdf manual (bookdown or similar) has never been easier. With this in mind, the link on my lab webpage will stay as an archive as long as the university allows it, but the new manual and all updates will move to my GitHub page under a new repository."
  },
  {
    "objectID": "MDprojections.html",
    "href": "MDprojections.html",
    "title": "\n4  Transformations between coordinate systems\n",
    "section": "",
    "text": "Transformations in ArcMap can be the most troublesome component of spatial analysis that is often overlooked as the reason for errors in data analysis. We will briefly go into the 2 most common problems requiring our assistance from collaborators and potential solutions.\n\nWhat coordinate system were the data collected in?\n\nIt seems that every GPS collar, handheld GPS unit, GIS landcover layer, etc. has been created using a different coordinate system and it’s not the one you have at your study site. Or perhaps NAD 1927 was used and you decided to be modern and want to use NAD 1983. Regardless, the coordinate systems must match even though ArcMap often overlays them with “on the fly projections”. The “on the fly” component of ArcMap is great for visualization but not for spatial ecologists that need data analysis. We often can determine which coordinate system the data were created in using the metadata to define a coordinate system or project the data into a coordinate system for data analysis.\n\nA Toolbox in ArcMap may not extract data or clip data properly\n\nAs mentioned previously, data collected with a GPS collar or handheld GPS may not be in the same geographic or projected coordinate system as the GIS layers you download or receive from collaborators (i.e., Digital Elevation Data, National Land Cover Data). As you attempt to use a Toolbox function, such as clipping National Land Cover Data within the extent of your GPS locations, an error may result.\nWe will now explore some transformations of data in R to help understand what Projections and Transformations are all about. The dataset that follows is for a project in Colorado with mule deer equipped with GPS collars that collected locations every 3 hours. The purpose of the study was to determine mule deer use of agricultural crops, sunflowers in this case, in response to years of damage complaints from farmers. We will use this subset of dataset in later exercises as well.\n\nOpen the script “MDprojections.Rmd” and run code directly from the script\n\n2. First we need to load the packages needed for the exercise\n\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs&lt;-\"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\nutm.crs &lt;-\"+proj=utm +zone=12 +datum=WGS84\"\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(26912)\nalbers.crs &lt;- st_crs(5070)\n\n4. Read in some shapefiles of the study area\n\nstudy.states&lt;-st_read(\"data/MDcounties.shp\")\n\nReading layer `MDcounties' from data source \n  `/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-of-Applied-Spatial-Ecology/data/MDcounties.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 38 features and 9 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -112.9059 ymin: 36.99228 xmax: -105.4277 ymax: 41.25376\nGeodetic CRS:  WGS 84\n\nplot(st_geometry(study.states), col=\"grey\")\n\n\n\n#Let's zoom into the region we have locations instead of county level\nstudy.zoom&lt;- st_read(\"data/MDzoom.shp\")\n\nReading layer `MDzoom' from data source \n  `/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-of-Applied-Spatial-Ecology/data/MDzoom.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -109.2433 ymin: 37.56257 xmax: -108.6488 ymax: 37.95425\nGeodetic CRS:  WGS 84\n\nplot(st_geometry(study.zoom), col=\"grey\")\n\n\n\n\n5. Import the csv file that contains all the mule deer locations by ID\n\nmuleys &lt;-read.csv(\"data/muleysexample.csv\",header=T)\n\n6. Create an sf object of raw mule deer locations with projection defined similar to study site shapefile (i.e., WGS84) then remove obvious outliers using st_crop\n\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.3,ymax=39.0)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\n\n7. Now let’s project both the mule deer locations and study site shapefile to NAD83 UTM Zone 12 (Fig. 1.4, 1.5)\n\nMDzoomUTM12 &lt;-st_transform(study.zoom, CRS=utm.crs)\nplot(st_geometry(MDzoomUTM12), col=\"bisque\")\nclass(MDzoomUTM12)\nst_crs(MDzoomUTM12)\n\n#projection for mule deer locations\ndeerUTM12 &lt;-st_transform(deer.spdf, CRS=utm.crs)\nplot(st_geometry(deerUTM12), axes=T)\nclass(deerUTM12)\nst_crs(deerUTM12)\n\n#See new projected coordinates in UTM 12N for the first 5 locations compared to similar line of code above in latitude/longitude\nst_coordinates(deerUTM12[1:5,])"
  },
  {
    "objectID": "TimeLagScript.html",
    "href": "TimeLagScript.html",
    "title": "\n5  Import and Format Datasets\n",
    "section": "",
    "text": "1. Open the script “TimeLagScript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(chron)\nlibrary(sf)\n#library(RAtmosphere)#This package has not been updated for newer versions of R\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs&lt;-\"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\nutm.crs &lt;-\"+proj=utm +zone=12 +datum=WGS84\"\nll.crs=st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n\nDetermine the name of your file (“temp” in our case here).\n\n\ntemp &lt;- read.csv(\"data/Y2005_UTM_date.csv\", header=T)\n\n\nIt is often necessary to determine the time lag between successive locations within your dataset\n\n\n# Modify time to include seconds \ntemp$time &lt;- paste(as.character(temp$LMT_TIME),\"00\",sep=\":\") \n\n# Convert to chron date \ntemp$date_time &lt;- chron(as.character(temp$LMT_DATE),temp$time,format=c(dates=\"m/d/y\",times=\"h:m:s\")) \n\n# Calculate difference in time in minutes \ntimediff &lt;- diff(temp$date_time)*24*60 \nsummary(timediff)\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n   7.00000   30.00000   30.00000   72.01485   60.00000 7859.00000 \n\n# Remove first entry without any difference \ntemp &lt;- temp[-1,] \n\n# Assign timediff column to original \"temp\" dataset for use later\ntemp$timediff &lt;- as.numeric(timediff) \n\n6. We can then either export this file as an excel file for use in other programs or use it in R in subsequent analysis\n\n#write.csv(temp, \"TimeDiffdata.csv\", row.names=TRUE,sep=\" \", col.names=TRUE, quote=TRUE, na=\"NA\")\n\n7. Next we can add code below to include night and day into datasets and to also to account for daylight savings. Package RAtmosphere will eliminate the need for the chunk of code below if working on an earlier version of R (i.e., code not available for R 3.2.1 plus). Thanks to Duane Diefenbach, PA Coop Unit Leader, for compiling all this from online sources.\nWe may first need to create a SPDF and transform to Lat Long then return to a Data Frame You only need this section of code if you need to acquire Lat Long coordinates for dataset\n\nll.crs&lt;-\"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\nutm.crs &lt;-\"+proj=utm +zone=12 +datum=WGS84\"\ndataspdf &lt;- st_as_sf(temp, coords = c(\"UTMe\", \"UTMn\"), crs = utm.crs)\n\ndatall &lt;-st_transform(dataspdf, ll.crs)\ntemp &lt;- data.frame(datall)\n\n8. Separate times into categories “Day” and “Night” based on sunrise-sunset table by running function below or simply using the RAtmosphere package\n9. First run line of code below with d being the day of year, Lat is latitude in decimal degrees, and Long is longitude in decimal degrees (negative == West) available at suncalc suncalc This method is copied from: Teets, D.A. 2003. Predicting sunrise and sunset times. The College Mathematics Journal 34(4):317-321.\nAt the default location the estimates of sunrise and sunset are within seven minutes of the correct times (http://aa.usno.navy.mil/data/docs/RS_OneYear.php) with a mean of 2.4 minutes error.\nNOTE: Function is in package RAtmosphere but does not work in newer version of R along with other issues!\n\nsuncalc &lt;- function(d,Lat=39.14133,Long=-106.7722){\n  \n  ## Function to convert degrees to radians\n  rad &lt;- function(x)pi*x/180\n  \n  ##Radius of the earth (km)\n  R=6378\n  \n  ##Radians between the xy-plane and the ecliptic plane\n  epsilon=rad(23.45)\n\n  ##Convert observer's latitude to radians\n  L=rad(Lat)\n\n  ## Calculate offset of sunrise based on longitude (min)\n  ## If Long is negative, then the mod represents degrees West of\n  ## a standard time meridian, so timing of sunrise and sunset should\n  ## be made later.\n  ##NOTE: If working with UTC times use timezone = -4*(abs(Long)%%15)*sign(Long)\n  timezone = -6*(abs(Long)%%15)*sign(Long)\n\n  ## The earth's mean distance from the sun (km)\n  r = 149598000\n\n  theta = 2*pi/365.25*(d-80)\n\n  z.s = r*sin(theta)*sin(epsilon)\n  r.p = sqrt(r^2-z.s^2)\n\n  t0 = 1440/(2*pi)*acos((R-z.s*sin(L))/(r.p*cos(L)))\n  \n  ##a kludge adjustment for the radius of the sun\n  that = t0+5 \n\n  ## Adjust \"noon\" for the fact that the earth's orbit is not circular:\n  n = 720-10*sin(4*pi*(d-80)/365.25)+8*sin(2*pi*d/365.25)\n\n  ## now sunrise and after sunset are:\n  sunrise = (n-that+timezone)/60\n  sunset = (n+that+timezone)/60\n  suntime &lt;- cbind(sunrise,sunset)  \n\n  return(suntime)\n}\n\n11. Read in location data and retain lat, lon, and date\n\ntemp$Date &lt;- paste((temp$Year),substr(temp$date_time, 2,3),substr(temp$date_time, 5,6),sep=\"-\")\n\n#calculate calendar day and center of locations\ncalday &lt;- as.numeric(as.Date(temp$Date)-as.Date(\"2005-01-01\"), units=\"days\")\ndat1 &lt;- cbind(temp,calday)\nmoda &lt;- format(as.Date(temp$Date),\"%d-%b\")\n\ndat1 &lt;- cbind(dat1, suncalc(dat1$calday, Lat=dat1$LATITUDE, Long=dat1$LONGITUDE),moda)\nhrchar &lt;- as.character(substr(dat1$time,1,2))\nhr &lt;- as.numeric(as.character(substr(dat1$time,1,2)))\nminchar &lt;- as.character(substr(dat1$time,4,5))\nmin &lt;- as.numeric(minchar)\nlocalhr &lt;- hr+min/60\ndat1 &lt;- cbind(dat1,hr,hrchar,minchar,localhr)\nDiel &lt;- ifelse(localhr&lt;dat1$sunrise | localhr&gt;dat1$sunset, 'Night', 'Day')\ndat1 &lt;- cbind(dat1,Diel)\n\n\ndat1[15:50,]"
  },
  {
    "objectID": "SoilScript.html",
    "href": "SoilScript.html",
    "title": "\n6  Manipulate Polygon Layer\n",
    "section": "",
    "text": "1. Open the script “SoilScript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(sf)\nlibrary(terra)\n\n3. Import shapefile of soils dataset\n\nsoils&lt;-st_read(\"data/Soil_Properties.shp\")\n\nReading layer `Soil_Properties' from data source \n  `/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-of-Applied-Spatial-Ecology/data/Soil_Properties.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 12812 features and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 408283 ymin: 4456763 xmax: 504816.1 ymax: 4538989\nProjected CRS: NAD83 / UTM zone 13N\n\nplot(soils)\n\n\n\nnames(soils) #get attribute data\n\n [1] \"Join_Count\" \"TARGET_FID\" \"Join_Cou_1\" \"TARGET_F_1\" \"AREASYMBOL\"\n [6] \"SPATIALVER\" \"MUSYM\"      \"MUKEY\"      \"SdvOutpu_1\" \"SdvOutpu_2\"\n[11] \"SdvOutpu_3\" \"geometry\"  \n\n#Rename original category headings to something more familiar\nsoils$Clay &lt;- soils$SdvOutpu_1\nsoils$pH &lt;- soils$SdvOutpu_2\nsoils$CEC &lt;- soils$SdvOutpu_3\n\n4.Shapefiles contain several features associated with each polygons that collectively make up the entire layer so let’s explore\n\nsoils #a data frame with geometry type \"polygon\" associated with 10 features\nbbox(soils) #boundary box\nst_crs(soils) #projection information\nsoils[1, ] #will bring up data associated with the first polygon\n\n5. Select portions of the data that fit some set criteria\n\n#Highlights the areas that Percent Clay polygons are over 30%\nplot(st_geometry(soils))\nhigh.clay &lt;- soils[soils$Clay&gt;30,]\nplot(st_geometry(high.clay), border=\"red\", add=TRUE)\n\n##Highlights the areas that Cation Exchange Capacity is greater than 14\nhigh.CEC&lt;- soils[soils$CEC&gt;14,]\nplot(st_geometry(high.CEC), border=\"green\", add=TRUE)\n\n##Highlights the areas that soil pH is greater than 8\nhigh.pH &lt;- soils[soils$pH&gt;8,]\nplot(st_geometry(high.pH), border=\"yellow\", add=TRUE)\n\n\n\n\n6. Bring in locations of harvested mule deer\n\n#Import mule deer locations from harvested animals tested for CWD\n#Note: Locations have been offset or altered so do not reflect actual locations of samples\nmule &lt;- read.csv(\"data/MDjitterclip.csv\", header=T)\ncrs&lt;-\"+proj=utm +zone=13 +datum=WGS84 +no_defs +towgs84=0,0,0\"\ncoords &lt;- st_as_sf(mule, coords = c(\"x\", \"y\"), crs = st_crs(soils))\n\nplot(st_geometry(coords), col=\"blue\")\n\n\n\n#par(new=TRUE)\n\n7. Let’s generate random points with the extent of the soil layer\n\n#Sampling points in a Spatial Object \"type=regular\" will give a regular grid\nsamples&lt;-st_sample(soils, 1000, type=\"random\")\n\nplot(st_geometry(soils), col=\"wheat\")\nplot(st_geometry(coords), col=\"blue\", add=T)\nplot(st_geometry(samples), col=\"red\", add=T)\n\n\n\n\n8. Extract and tally Clay soil types for random samples and mule deer locations\n\n#Matches points with polygons:\nsoils.idx&lt;- st_intersects(samples,soils)\nsoil.samples &lt;- soils[unlist(soils.idx), \"Clay\", drop = TRUE] # drop geometry\n# locs &lt;- SpatialPoints(coords)\n# locs@proj4string &lt;- soils@proj4string\nsoils.locations&lt;- st_intersects(coords, soils)\nsoils.locs &lt;- soils[unlist(soils.locations), \"Clay\", drop = TRUE]\n#Tally clay soil types for random samples\nobs.tbl &lt;- table(soil.samples[soil.samples])\nobs.tbl\n\n\n10.3 11.5 11.7 12.4 12.6 14.5 16.5 19.8 20.7 21.1 21.5 24.7 25.8   26 26.1 26.7 \n  73    5   48   21  300    9   35   35    7    2  181   22  136    4   49    6 \n37.6 \n  10 \n\n#Also tally soil types for each mule deer sampled\nobs.tbl2 &lt;- table(soils.locs[soils.locs])\nobs.tbl2\n\n\n   0  8.7  9.7 11.5 11.7 12.1 12.6   13 13.1 16.5 18.5   21   40 \n  71   31  670  237   96    1    8   89    2  236   12   30   31 \n\n\n10. Converts the counts to proportions\n\nobs &lt;- obs.tbl/sum(obs.tbl)\nobs\n\n\n       10.3        11.5        11.7        12.4        12.6        14.5 \n0.077412513 0.005302227 0.050901379 0.022269353 0.318133616 0.009544008 \n       16.5        19.8        20.7        21.1        21.5        24.7 \n0.037115589 0.037115589 0.007423118 0.002120891 0.191940615 0.023329799 \n       25.8          26        26.1        26.7        37.6 \n0.144220573 0.004241782 0.051961824 0.006362672 0.010604454 \n\nobs2 &lt;- obs.tbl2/sum(obs.tbl2)\nobs2\n\n\n          0         8.7         9.7        11.5        11.7        12.1 \n0.046895641 0.020475561 0.442536328 0.156538970 0.063408190 0.000660502 \n       12.6          13        13.1        16.5        18.5          21 \n0.005284016 0.058784676 0.001321004 0.155878468 0.007926024 0.019815059 \n         40 \n0.020475561"
  },
  {
    "objectID": "RasterScript.html",
    "href": "RasterScript.html",
    "title": "\n7  Manipulate Raster Data Layer\n",
    "section": "",
    "text": "1. Open the script “RasterScript.R”\n2. First we need to load the packages needed for the exercise\n\n#install.packages(c(\"adehabitatHR\",\"maptools\",\"raster\", \"rgdal\"))\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tigris)\n#options(tigris_use_cache = TRUE)\n#tigris_cache_dir(\"/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-Applied-Spatial-Ecology/data\")\n#'\n#install.packages(\"remotes\")\n# remotes::install_github(repo = \"r-lib/devtools\",\n#                               dependencies = TRUE,\n#                               upgrade = TRUE)\n# devtools::install_github(repo = \"r-lib/devtools\",\n#                            dependencies = TRUE,\n#                            upgrade = TRUE)\n#devtools::install_github(\"ropensci/FedData\")\n#remotes::install_github(\"ropensci/FedData\")\nlibrary(FedData)\nsessionInfo() #Search for and confirm FedData_4.0.0 was loaded\n\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] FedData_4.0.1 tigris_2.0.3  dplyr_1.1.3   sf_1.0-14     terra_1.7-46 \n\nloaded via a namespace (and not attached):\n [1] jsonlite_1.8.7     compiler_4.3.2     tidyselect_1.2.0   Rcpp_1.0.11       \n [5] stringr_1.5.0      uuid_1.1-0         yaml_2.3.7         fastmap_1.1.1     \n [9] readr_2.1.4        R6_2.5.1           generics_0.1.3     classInt_0.4-10   \n[13] knitr_1.42         htmlwidgets_1.6.2  tibble_3.2.1       units_0.8-4       \n[17] DBI_1.1.3          tzdb_0.4.0         pillar_1.9.0       rlang_1.1.1       \n[21] utf8_1.2.3         stringi_1.7.12     xfun_0.39          cli_3.6.1         \n[25] magrittr_2.0.3     class_7.3-22       digest_0.6.31      grid_4.3.2        \n[29] rstudioapi_0.14    hms_1.1.3          rappdirs_0.3.3     lifecycle_1.0.3   \n[33] vctrs_0.6.3        KernSmooth_2.23-22 proxy_0.4-27       evaluate_0.21     \n[37] glue_1.6.2         codetools_0.2-19   fansi_1.0.4        e1071_1.7-13      \n[41] rmarkdown_2.21     httr_1.4.7         tools_4.3.2        pkgconfig_2.0.3   \n[45] htmltools_0.5.5   \n\n#library(adehabitatHR)\n#library(maptools)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise\n\nll.crs=st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n#CRS of shapefile layers\n#crs &lt;- CRS(\"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 \n#  +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0\")\n#CRS of raster layers\n#crs2 &lt;- CRS(\"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 \n#  +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\")\n\n4. We will use the tigris package to downloaded statewide layers for state and county outlines\n\nst &lt;- tigris::states() %&gt;%\n  dplyr::filter(GEOID &lt; \"60\") %&gt;% \n  tigris::shift_geometry()\n\nRetrieving data for the year 2021\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |======================================================================| 100%\n\n#GEOID's above 60 are territories and islands, etc. So I'm removing them for scaling.\nplot(st_geometry(st))\n\n\n\nst &lt;- st_transform(st, albers.crs)\nSC.outline &lt;- subset(st, st$NAME == \"South Carolina\")\n\nSCcounties &lt;- counties(\"South Carolina\", cb = TRUE)\n\nRetrieving data for the year 2021\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |======================================================================| 100%\n\nSCcounties &lt;- st_transform(SCcounties, albers.crs)\n\n5. Now let’s add some shapefiles to our raster specific to the military base of interest\n\n#Load county shapefile\ncounty&lt;- subset(SCcounties, SCcounties$NAME == \"Beaufort\")\n\n#Load airport runway shapefile\nrun&lt;-st_read(\"data/RunwayAlbers.shp\")\n\nReading layer `RunwayAlbers' from data source \n  `/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-of-Applied-Spatial-Ecology/data/RunwayAlbers.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1420912 ymin: 1158247 xmax: 1423610 ymax: 1162103\nProjected CRS: NAD_1983_Albers\n\n#Load aircraft flight pattern shapefile\npath&lt;-st_read(\"data/FlightImage.shp\")\n\nReading layer `FlightImage' from data source \n  `/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-of-Applied-Spatial-Ecology/data/FlightImage.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 232 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 1407742 ymin: 1144853 xmax: 1436609 ymax: 1175174\nProjected CRS: NAD_1983_Albers\n\n#Load aircraft flight pattern shapefile\nroad &lt;- st_read(\"data/CountyRoadAlbers.shp\")\n\nReading layer `CountyRoadAlbers' from data source \n  `/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-of-Applied-Spatial-Ecology/data/CountyRoadAlbers.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 17117 features and 15 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 1398896 ymin: 1113776 xmax: 1450561 ymax: 1182944\nProjected CRS: NAD_1983_Albers\n\n\n\nUsing county outline, we can then download National Land Cover data for any year of interest using the FedData package\n\n\n#new.outline &lt;- sf::as_Spatial(county)#need to do this line due to error with dplyr package\nSC.outline &lt;- subset(st, st$NAME == \"South Carolina\")\nSCnlcd &lt;- get_nlcd(template=SC.outline, year = 2019, label = 'SC',force.redo = T)\n#Write raster if needed\n#writeRaster(SCnlcd, filename = \"SCnlcd2019.tif\", filetype = \"GTiff\", datatype = 'INT4U',overwrite=TRUE)\n  \nplot(SCnlcd)\nplot(st_geometry(SCcounties), add=T, lwd=3)\n\n#Import raster saved above just for demonstration\n#SCnlcd &lt;-rast(\"SCnlcd2019.tif\")\nplot(SCnlcd)\nplot(st_geometry(SCcounties), add=T, lwd=3)\n\n7. Plot out all the shapefiles overlayed on each other with and without the raster.\n\nplot(st_geometry(county))\nplot(st_geometry(road), add=T)\nplot(st_geometry(run), col=\"red\",add=T)\nplot(st_geometry(path), col=\"blue\",add=T)\n\n\n\n\n. Let’s reclassify NLCD layer to get fewer land cover categories to make it easier to work with the raster.\n\n# reclassify the values into 7 groups all values between 0 and 20 equal 1, etc. while removing water (11)\nm &lt;- c(0, 19, NA, 20, 39, 1, 40, 50, 2, 51, 68, 3, 69,79, 4, 80, 88, 5, 89, 99, 6)\nrclmat &lt;- matrix(m, ncol=3, byrow=TRUE)\nrc &lt;- classify(SCnlcd, rclmat)\nplot(rc)\n\n#Clip the raster within the county polygon for a zoomed in view then plot\nclip &lt;- crop(rc, county)\nplot(clip)\nplot(st_geometry(county),add=T,lwd=2)\nplot(st_geometry(run),add=T,lwd=2, col=\"red\")\nplot(st_geometry(path),add=T,lty=\"62\", col=\"blue\")\nplot(st_geometry(road),add=T,lty=\"22\", col=\"yellow\")\n\n. We can load some vulture locations to extract landcover that each location occurs in that will be considered “used” habitat in resource selection analysis.\n\n#Import bird 49 locations to R\nbv49 &lt;-read.csv(\"data/Bird49.csv\", header=T)#How many bird locations?\n\nutm.crs&lt;-\"+proj=utm +zone=17 +ellps=WGS84\" #NOTE: If using a mac, we need to remove the N in zone 17 prior to running line below\n#Include geometry\nbvspdf &lt;- st_as_sf(bv49, coords = c(\"x\", \"y\"), crs = utm.crs)\n\nplot(st_geometry(bvspdf), col=\"red\")\n#NOTE: Locations must be assigned to the UTM coordinate system prior to projection to Albers \n#so won't overlay on veg layer at this point because veg is in Albers\nbv49Albers &lt;- st_transform(bvspdf, albers.crs)\n#plot(clip)\nplot(st_geometry(bv49Albers), col=\"blue\",add=T)\n\n\n\n\n10. Determine which of those points lie within a cell that contains data by using the extract function. The extract function will extract covariate information from the raster for xy coordinates at for each location.\n\nbvspdf.df &lt;- data.frame(st_coordinates(bv49Albers))\nbvspdf2 &lt;- st_as_sf(bvspdf.df, coords= c(\"X\", \"Y\"), crs = albers.crs)\nbvspdf.sv &lt;- vect(bvspdf2)#Need SpatVector for extract function in terra\nveg.survey&lt;-terra::extract(clip, bvspdf.sv)\nveg.survey&lt;-subset(bv49Albers,!is.na(veg.survey$Class))\nplot(st_geometry(veg.survey), col=\"black\")\n\n11. We can also create some random points within the extent of the area to be considered as “available” habitat.\n\n##First we need to create a box across the study site to generate a random sample of locations\ne.box &lt;- as.polygons(clip,extent=TRUE)\ne.box &lt;- st_as_sf(e.box)\nSample.points &lt;- st_sample(e.box, 1000, type=\"random\")\n\nplot(clip)\nplot(st_geometry(Sample.points),add=T)\n\n#Determine which of those points lie within a cell that contains data by using the extract function again.\nSample.points.sv &lt;- vect(Sample.points)#Need SpatVector class for extract function in terra\nsamp.survey&lt;-terra::extract(clip, Sample.points.sv)\nsamp.survey&lt;-subset(samp.survey,!is.na(samp.survey$Class))\n\n12. New code addition below (Update 1/7/2021) to use mapview package to treat plotting data layers as done with GIS software. This package enables user to zoom in and out and scroll around layer after calling a plot function \n\nlibrary(mapview)\nmapview(clip)\nmapview(county) + mapview(bv49Albers, color=\"yellow\")\nmapview(run) + bv49Albers + road"
  },
  {
    "objectID": "GridScripts.html",
    "href": "GridScripts.html",
    "title": "\n8  Creating a Hexagonal Polygon Grid Over a Study Area\n",
    "section": "",
    "text": "1. Open the script “GridScripts.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(terra)\nlibrary(sf)\nlibrary(tigris)\nlibrary(FedData)\nlibrary(ggplot2)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs=st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. We will use the tigris package to downloaded statewide layers for state and county outlines\n\nst &lt;- tigris::states() %&gt;%\n  dplyr::filter(GEOID &lt; \"60\") %&gt;% #GEOID's above 60 are territories and islands, etc. So I'm removing them for scaling.\n  tigris::shift_geometry()\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nplot(st_geometry(st))\n\n\n\nst &lt;- st_transform(st, utm.crs)\nCO.outline &lt;- subset(st, st$NAME == \"Colorado\")\n\nCOcounties &lt;- counties(\"Colorado\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |======================================================================| 100%\n\nCOcounties &lt;- st_transform(COcounties, utm.crs)\nplot(st_geometry(COcounties))\n\n\n\n#Import the location from earlier exercise\nmuleys &lt;-read.csv(\"data/muleysexample.csv\", header=T)\n\n#Remove outlier locations\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous plot\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\ndeer.spdf &lt;-st_transform(deer.spdf, crs=utm.crs)\n\n5. Now lets extract counties within the extent of the mule deer locations\n\nint &lt;- st_intersection(COcounties,deer.spdf)\nclipped &lt;- COcounties[int,]\nplot(st_geometry(clipped))\nplot(st_geometry(deer.spdf), add=T)\n\n\n\n\n6. We also can create a hexagonal grid across the study site\n\ngrid_spacing &lt;- 1500\nHexPols &lt;- st_make_grid(clipped, square = F, cellsize = c(grid_spacing, grid_spacing)) %&gt;% # the grid, covering bounding box\n  st_intersection(clipped) \n  \nplot(st_geometry(clipped))\nplot(st_geometry(HexPols),add=T)\n\n\n\n\n7. Create this hexagonal grid across our study site by zooming into deer locations\n\n#Import the study site zoomed in shapefile\nstudy.zoom&lt;-st_read(\"data/MDzoom.shp\")\n\nReading layer `MDzoom' from data source \n  `/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-of-Applied-Spatial-Ecology/data/MDzoom.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -109.2433 ymin: 37.56257 xmax: -108.6488 ymax: 37.95425\nGeodetic CRS:  WGS 84\n\nstudy.zoom &lt;- st_transform(study.zoom, crs=utm.crs)\n\n#Create new hexagonal grid\ngrid_spacing &lt;- 1500\nHexPols &lt;- st_make_grid(study.zoom, square = F, cellsize = c(grid_spacing, grid_spacing)) %&gt;% # the grid, covering bounding box\n  st_intersection(study.zoom) %&gt;%\n    cbind(data.frame(ID = sprintf(paste(\"GID%0\",nchar(length(.)),\"d\",sep=\"\"), 1:length(.)))) %&gt;%\n    st_sf()\nplot(st_geometry(study.zoom))\nplot(st_geometry(HexPols),add=T)\n\n\n\n#Now add labels to each hexagon for unique ID\nhex.centroids &lt;- sf::st_point_on_surface(HexPols)\nhex_coords &lt;- as.data.frame(sf::st_coordinates(hex.centroids))\nhex_coords$NAME &lt;- HexPols$ID\nggplot() +\n  geom_sf(data = HexPols) +\n  geom_text(data = hex_coords, aes(X, Y, label = NAME), colour =\"black\")\n\n\n\n\n8. We can intersect the mule deer locations with the polygon shapefile (i.e., county) they occurred in Hexagon ID if needed\n\no = st_intersection(deer.spdf,COcounties)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n9. As an alternative to importing a polygon that we created in ArcMap above, we can create a polygon in R using the coordinates of the boundary box of the area of interest. In our case here, the bounding box will be the mule deer locations.\n\nbb &lt;- st_bbox(deer.spdf) %&gt;% st_as_sfc()\n#bb &lt;- cbind(x=c(-108.83966,-108.83966,-108.9834,-108.9834, -108.83966), \n#  y=c(37.8142, 37.86562,37.86562,37.8142,37.8142))\nplot(st_geometry(bb))\nplot(st_geometry(deer.spdf),col=\"red\",add=T)\n\n\n\n\n10. Now make practical use of the new bounding box we created by clipping a larger raster dataset. A smaller raster dataset runs analyses faster, provides a zoomed in view of mule deer locations and vegetation, and is just easier to work with.\n\n#Load vegetation raster layer textfile clipped in ArcMap \nveg &lt;-raster(\"extentnlcd2.txt\")\nplot(veg)\nclass(veg)\n\n\n#Clip using the raster imported with \"raster\" package\nbbclip &lt;- st_crop(veg, bb)\nveg\n\n\n#WON'T WORK because projections are not the same, WHY?\n#Let's check projections of layers we are working with now.\nst_crs(MDclip)\nst_crs(deer.spdf)\nst_crs(SP)\nst_crs(veg)\n\n11. We need to have all layers in same projection so project the deer.spdf to Albers and then clip vegetation layer with new polygon we created in the Albers projection.\n\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nbb.albers &lt;- st_bbox(deer.albers) %&gt;% st_as_sfc()\n#Check to see all our layers are now in Albers projection\nst_crs(veg)\nst_crs(deer.albers)\nst_crs(bb.albers)\n\n#Clip using the raster imported with \"raster\" package \nbbclip &lt;- st_crop(veg, AlbersSP)\nplot(bbclip)\nplot(st_geometry(deer.albers), col=\"red\", add=T)\nplot(st_geometry(bb.albers), lwd=5, add=T)"
  },
  {
    "objectID": "GridSystem2Script.html",
    "href": "GridSystem2Script.html",
    "title": "\n9  Creating a Square Polygon Grid Over a Study Area\n",
    "section": "",
    "text": "1. Open the script “GridSystem2Script.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(sf)\nlibrary(terra)\nlibrary(adehabitatMA)\nlibrary(FedData)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. We need to have all layers in same projection so import, create, and remove outliers for mule deer locations then project all to the Albers projection as we did previously.\n\n#Import the location from earlier exercise\nmuleys &lt;-read.csv(\"data/muleysexample.csv\", header=T)\n\n#Remove outlier locations\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous plot\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\ndeer.spdf &lt;-st_transform(deer.spdf, crs=utm.crs)\n\n#Project deer.spdf to Albers as in previous exercise\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers,axes=T))\n\n\n\n\n5. Create points for x and y from the bounding box of all mule deer locations with 1500 m spacing between each point.\n\nbb &lt;- st_bbox(deer.albers) %&gt;% st_as_sfc()\n#bb &lt;- cbind(x=c(-108.83966,-108.83966,-108.9834,-108.9834, -108.83966), \n#  y=c(37.8142, 37.86562,37.86562,37.8142,37.8142))\nplot(st_geometry(bb))\nplot(st_geometry(deer.albers),col=\"red\",add=T)\n\n\n\n\n6. Create a grid of all pairs of coordinates (as a data.frame) using the “expand grid” function and then make it a gridded object.\n\ngrid_spacing &lt;- 500\nGridPols &lt;- st_make_grid(bb, square = T, cellsize = c(grid_spacing, grid_spacing)) %&gt;% # the grid, covering bounding box\n  st_intersection(bb) %&gt;%\n    cbind(data.frame(ID = sprintf(paste(\"GID%0\",nchar(length(.)),\"d\",sep=\"\"), 1:length(.)))) %&gt;%\n    st_sf()\nplot(st_geometry(bb))\nplot(st_geometry(GridPols),add=T)\n\n\n\n\n7. Similar to the hexagonal grid, identify the cell ID that contains each mule deer location.\n\no = st_intersection(deer.albers,GridPols)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nhead(o)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1120488 ymin: 1718097 xmax: -1120110 ymax: 1718916\nProjected CRS: NAD83 / Conus Albers\n         DateTimeAnimalID id  Serial            Acq_Time              Acq_ST\n197 D82011.10.20 12:00:00 D8 647578A 2011.10.20 12:00:40 2011.10.20 12:00:00\n230 D82011.10.24 15:00:00 D8 647578A 2011.10.24 15:00:48 2011.10.24 15:00:00\n231 D82011.10.24 18:00:00 D8 647578A 2011.10.24 18:01:58 2011.10.24 18:00:00\n232 D82011.10.24 21:00:00 D8 647578A 2011.10.24 21:00:49 2011.10.24 21:00:00\n333 D82011.11.06 18:00:00 D8 647578A 2011.11.06 18:01:04 2011.11.06 18:00:00\n328 D82011.11.06 03:00:00 D8 647578A 2011.11.06 03:00:52 2011.11.06 03:00:00\n             GPSFixTime      GPSFixAtt UTM_Zone       Y      X GPS_Alt\n197 2011.10.20 12:00:40 Succeeded (3D)      12S 4187300 685838    2176\n230 2011.10.24 15:00:48 Succeeded (3D)      12S 4187827 686114    2237\n231 2011.10.24 18:01:58 Succeeded (3D)      12S 4187807 686078    2217\n232 2011.10.24 21:00:49 Succeeded (3D)      12S 4187812 686107    2233\n333 2011.11.06 18:01:04 Succeeded (3D)      12S 4187805 686032    2222\n328 2011.11.06 03:00:52 Succeeded (3D)      12S 4188096 685684    2173\n    GPS_Speed GPSHeading GPSHorizEr GPS_PD    GPS_SB GPS_SC GPS_NT Pre_Data\n197      0.05        0.7      18.46    3.4   4227808      6     40       No\n230      0.07        0.7      24.00    2.8 134299840      5     48       No\n231      0.04        0.7     108.00    4.7      2314      4    118       No\n232      0.06      348.0      24.00    3.8  16779290      5     49       No\n333      0.03        0.7      45.00    3.3     66314      5     64       No\n328      0.02        0.7      48.00    3.8   1318948      5     52       No\n    Error     ID                 geometry\n197    NA GID015 POINT (-1120464 1718097)\n230    NA GID016 POINT (-1120110 1718579)\n231    NA GID016 POINT (-1120149 1718566)\n232    NA GID016 POINT (-1120119 1718566)\n333    NA GID016 POINT (-1120193 1718571)\n328    NA GID040 POINT (-1120488 1718916)\n\n\n8. If we get some NA errors because our grid does not encompass all mule deer locations then we can expand the grid size extending beyond our locations.\n\n# Create vectors of the x and y points using boundary box created around deer locations\nbb1 &lt;- st_bbox(GridPols)\n     \nincrement = 2000\nminx=(min(bb1$xmin)-(increment))\nmaxx=(max(bb1$xmax)+(increment))\nminy=(min(bb1$ymin)-(increment))\nmaxy=(max(bb1$ymax)+(increment))\n  # Create vectors of the x and y points using mean size of deer home range of X square kilometers\nx=seq(from=minx,to=maxx,by=increment)\ny=seq(from=miny,to=maxy,by=increment)\n# Create a grid of all pairs of coordinates (as a data.frame) \nxy=expand.grid(x=x,y=y)\ngrid.pts&lt;-st_as_sf(xy, coords = c(\"x\",\"y\"),crs = albers.crs)\nplot(st_geometry(grid.pts))\n\n\n\nGridPols2 &lt;- st_make_grid(grid.pts, square=T,cellsize = grid_spacing) %&gt;%\n   cbind(data.frame(ID = sprintf(paste(\"GID%0\",nchar(length(.)),\"d\",sep=\"\"), 1:length(.)))) %&gt;%\n   st_sf()\n\nplot(st_geometry(GridPols2))\nplot(st_geometry(GridPols),add=T, col=\"red\")\nplot(st_geometry(deer.albers),add=T, col=\"blue\")\n\n\n\n\n\nThen identify the cell ID that contains each mule deer location from the new expanded grid\n\n\n##BE SURE TO RUN CODE FROM XY CREATION THROUGH NEW2 AGAIN THEN LOOK AT DATA!!\no2 = st_intersection(deer.albers,GridPols2)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n10. Now we can load a vegetation raster layer using the FedData package to summarize vegetation categories within each polygon grid cell.\n\nnlcd &lt;- get_nlcd(template=GridPols2, year = 2019, label = 'nlcd',force.redo = T)\nplot(nlcd)\n\n\n\n\n11. Clip the raster within the extent of the newly created grid\n\nbbclip &lt;- crop(nlcd, GridPols2)\nplot(bbclip)\nplot(st_geometry(deer.albers),add=T,col=\"red\")\nplot(st_geometry(GridPols2), add=T)\n\n\n\n#Cell size of raster layer\nxres(bbclip)\n\n[1] 30\n\n#Create histogram of vegetation categories in bbclip\nhist(bbclip)\n\n\n\n#Calculate cell size in square meters\nii &lt;- st_area(GridPols2)#requires adehabitatMA package\nii[1]\n\n250000 [m^2]\n\n\n\nWe can extract the vegetation characteristics within each polygon of the grid and then tabulate area of each vegetation category within each polygon by extracting vegetation within each polygon by ID then appending the results back to the extracted table by running it twice but with different names. Summarizing the vegetation characteristics in each cell will be used in future resource selection analysis or disease epidemiology.\n\n\ntable = extract(bbclip,GridPols2)\nstr(table[1])\n\n'data.frame':   177489 obs. of  1 variable:\n $ ID: num  1 1 1 1 1 1 1 1 1 1 ...\n\narea = extract(bbclip,GridPols2)\ncombine=lapply(area,table)\ncombine\n\n$ID\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 \n 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n272 272 256 272 272 256 272 272 256 272 272 256 289 272 289 289 272 289 289 272 \n 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 \n 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n272 289 289 272 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 \n 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 272 256 272 272 \n101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 \n121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n272 256 272 272 256 272 272 256 289 272 289 289 272 289 289 272 289 289 272 289 \n141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 \n161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 \n181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n289 289 272 289 289 272 289 289 272 289 289 272 272 256 272 272 256 272 272 256 \n201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 \n221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n256 272 272 256 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 \n241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 272 289 289 \n261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 \n281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n289 272 289 289 272 289 289 272 272 256 272 272 256 272 272 256 272 272 256 272 \n301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 \n321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 \n341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n289 289 272 289 289 272 289 289 272 289 289 272 289 272 289 289 272 289 289 272 \n361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 \n381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n272 289 289 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 \n401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 289 272 289 289 \n421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 \n441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 \n289 272 289 289 272 289 289 272 289 272 289 289 272 289 289 272 289 289 272 289 \n461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 \n289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 \n481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 \n272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 \n501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 \n272 272 256 272 272 256 272 272 256 272 272 256 289 272 289 289 272 289 289 272 \n521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 \n289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 \n541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 \n272 289 289 272 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 \n561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 \n272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 272 256 272 272 \n581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 \n256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 272 256 272 \n601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 \n272 256 272 272 256 272 272 256 289 272 289 289 272 289 289 272 289 289 272 289 \n621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 \n289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 289 289 272 \n\n$Class\n\n                  Open Water           Perennial Ice/Snow \n                           3                            0 \n       Developed, Open Space     Developed, Low Intensity \n                        2258                         1452 \n Developed, Medium Intensity     Developed High Intensity \n                           8                            0 \nBarren Land (Rock/Sand/Clay)             Deciduous Forest \n                           0                         2781 \n            Evergreen Forest                 Mixed Forest \n                       33224                            9 \n                 Dwarf Scrub                  Shrub/Scrub \n                           0                        89482 \n        Grassland/Herbaceous             Sedge/Herbaceous \n                         127                            0 \n                     Lichens                         Moss \n                           0                            0 \n                 Pasture/Hay             Cultivated Crops \n                         214                        47213 \n              Woody Wetlands Emergent Herbaceous Wetlands \n                         495                          223"
  },
  {
    "objectID": "BufferScript.html",
    "href": "BufferScript.html",
    "title": "\n10  Creating Buffers\n",
    "section": "",
    "text": "For this exercise, we will again be working with the Colorado mule deer locations and rasters from earlier sections (1.3, 1.7). Creating buffers around locations of animals, plots, or some other variable may be necessary to determine what occurs around the locations. Often times,in resource selection studies, we may want to generate buffers that can be considered used habitat within the buffer as opposed to simply counting only the habitat that the location is found. Lets begin with loading the proper packages and mule deer locations from previous exercise. Because we are dealing with the raster layer projected in Albers, we will need to project our mule deer locations as we did above.\n1. Open the script “BufferScript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(sf)\nlibrary(terra)\nlibrary(exactextractr)\nlibrary(FedData)\nlibrary(zoo)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. We will import a dataset then subset by a single deer to make processing time faster\n\n#Import the location from earlier exercise\nmuleys &lt;-read.csv(\"data/muleysexample.csv\", header=T)\n\n#Remove outlier locations\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous plot\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\n#Project deer.spdf to Albers as in previous exercise\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers,axes=T))\n\n\n\n#Let us subset data so there are fewer locations to work with\nmuley8 &lt;- subset(deer.albers, id==\"D8\")\n\n5. For mule deer 8 locations, we will create a bounding box.\n\nbb &lt;- st_bbox(muley8) %&gt;% st_as_sfc()\n#bb &lt;- cbind(x=c(-108.83966,-108.83966,-108.9834,-108.9834, -108.83966), \n#  y=c(37.8142, 37.86562,37.86562,37.8142,37.8142))\nplot(st_geometry(bb))\nplot(st_geometry(deer.albers),col=\"red\",add=T)\n\n\n\n\n. But let us create a new bounding box that encompass mule deer 8 locations but also extends beyond the periphery of the outermost locations. Then clip the large vegetation raster again so it is within the newly created bounding box polygon\n\n# Create vectors of the x and y points using boundary box created around deer locations\nbb1 &lt;- st_bbox(muley8)\n     \nincrement = 2000\nminx=(min(bb1$xmin)-(increment))\nmaxx=(max(bb1$xmax)+(increment))\nminy=(min(bb1$ymin)-(increment))\nmaxy=(max(bb1$ymax)+(increment))\n  # Create vectors of the x and y points using mean size of deer home range of X square kilometers\nx=seq(from=minx,to=maxx,by=increment)\ny=seq(from=miny,to=maxy,by=increment)\n# Create a grid of all pairs of coordinates (as a data.frame) \nxy=expand.grid(x=x,y=y)\ngrid.pts&lt;-st_as_sf(xy, coords = c(\"x\",\"y\"),crs = albers.crs)\nplot(st_geometry(grid.pts))\n\n\n\ngrid_spacing &lt;- 500\ngrid &lt;- st_make_grid(grid.pts, square=T,cellsize = grid_spacing) %&gt;%\n   cbind(data.frame(ID = sprintf(paste(\"GID%0\",nchar(length(.)),\"d\",sep=\"\"), 1:length(.)))) %&gt;%\n   st_sf()\n\nplot(st_geometry(grid))\nplot(st_geometry(muley8),add=T, col=\"blue\")\n\n\n\n\n7. Now we can load use FedData package with new bounding box to get Land Cover categories for use later.\n\nnlcd &lt;- get_nlcd(template=grid, year = 2019, label = 'nlcd',force.redo = T)\nplot(nlcd)\n\n\n\n\n8. To conduct some analyses, let us create 100 m buffered circles around all the locations and extract Land Cover that occurs in each buffered circle. Most efforts will want percent habitat or area of each habitat defined individually for each location (i.e., within each buffered circle).\n\nsettbuff=st_buffer(muley8,500) %&gt;% st_as_sfc()\nplot(nlcd)\nplot(st_geometry(settbuff), add=T, lty=2)\n\n\n\next &lt;- exact_extract(nlcd, st_as_sf(settbuff))\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\nhead(ext[[1]])#Here we can see the percent of each category in buffer IDs 1-6 for category 52\n\n  value coverage_fraction\n1    52       0.006735825\n2    52       0.170379668\n3    52       0.352209508\n4    52       0.472016335\n5    52       0.531102777\n6    52       0.529586434\n\net=lapply(ext,table)\n\nprop &lt;- list()\nfor(i in 1:length(ext)[1] ){\n  prop[[i]] &lt;- round((margin.table(et[[i]],1)/margin.table(et[[i]])),digits = 6)\n}\nprop\n\n[[1]]\nvalue\n      41       42       52       82 \n0.061637 0.058448 0.602550 0.277365 \n\n[[2]]\nvalue\n      41       42       52 \n0.014878 0.020191 0.964931 \n\n[[3]]\nvalue\n      41       42       52 \n0.032979 0.075532 0.891489 \n\n[[4]]\nvalue\n      21       41       42       52 \n0.002130 0.047923 0.102236 0.847710 \n\n[[5]]\nvalue\n      41       42       52 \n0.067308 0.198718 0.733974 \n\n[[6]]\nvalue\n      21       22       41       42       52       82 \n0.004255 0.028723 0.043617 0.050000 0.714894 0.158511 \n\n[[7]]\nvalue\n      21       22       41       42       52       82 \n0.005319 0.028723 0.043617 0.050000 0.714894 0.157447 \n\n[[8]]\nvalue\n      21       41       42       52       82 \n0.008493 0.049894 0.070064 0.604034 0.267516 \n\n[[9]]\nvalue\n      21       41       42       52       82 \n0.008493 0.049894 0.070064 0.604034 0.267516 \n\n[[10]]\nvalue\n      21       41       42       52       82 \n0.008502 0.049947 0.070138 0.603613 0.267800 \n\n[[11]]\nvalue\n      21       41       42       52       82 \n0.006403 0.050160 0.071505 0.606190 0.265742 \n\n[[12]]\nvalue\n      21       41       42       52       82 \n0.004251 0.046759 0.069075 0.612115 0.267800 \n\n[[13]]\nvalue\n      21       41       42       52       82 \n0.004269 0.045891 0.068303 0.614728 0.266809 \n\n[[14]]\nvalue\n      21       41       42       52       82 \n0.004269 0.045891 0.068303 0.612593 0.268943 \n\n[[15]]\nvalue\n      21       41       42       52       82 \n0.003185 0.046709 0.069002 0.613588 0.267516 \n\n[[16]]\nvalue\n      41       42       52       82 \n0.056144 0.063559 0.618644 0.261653 \n\n[[17]]\nvalue\n      41       42       52       82 \n0.059574 0.059574 0.595745 0.285106 \n\n[[18]]\nvalue\n      21       41       42       52       82 \n0.002139 0.049198 0.064171 0.621390 0.263102 \n\n[[19]]\nvalue\n      21       41       42       52       82 \n0.006383 0.047872 0.071277 0.604255 0.270213 \n\n[[20]]\nvalue\n      21       41       42       52       82 \n0.004255 0.045745 0.069149 0.611702 0.269149 \n\n[[21]]\nvalue\n      21       41       42       52       82 \n0.004255 0.046809 0.068085 0.612766 0.268085 \n\n[[22]]\nvalue\n      21       41       42       52       82 \n0.002121 0.048780 0.066808 0.620361 0.261930 \n\n[[23]]\nvalue\n      21       22       41       42       52       82 \n0.004260 0.027689 0.031949 0.041534 0.747604 0.146965 \n\n[[24]]\nvalue\n      21       22       41       42       52       82 \n0.004255 0.027660 0.032979 0.041489 0.743617 0.150000 \n\n[[25]]\nvalue\n      21       41       42       52       82 \n0.017021 0.054255 0.060638 0.630851 0.237234 \n\n[[26]]\nvalue\n      21       22       41       42       52       82 \n0.025424 0.016949 0.044492 0.056144 0.634534 0.222458 \n\n[[27]]\nvalue\n      21       22       41       42       52       82 \n0.014862 0.010616 0.044586 0.058386 0.647558 0.223992 \n\n[[28]]\nvalue\n      21       22       41       42       52       82 \n0.012766 0.006383 0.044681 0.059574 0.652128 0.224468 \n\n[[29]]\nvalue\n      21       22       41       42       52       82 \n0.012752 0.006376 0.044633 0.059511 0.654623 0.222104 \n\n[[30]]\nvalue\n      21       22       41       42       52       82 \n0.012793 0.008529 0.044776 0.058635 0.659915 0.215352 \n\n[[31]]\nvalue\n      21       22       41       42       52       82 \n0.002125 0.009564 0.020191 0.030818 0.756642 0.180659 \n\n[[32]]\nvalue\n      22       41       42       52       82 \n0.004255 0.045745 0.064894 0.762766 0.122340 \n\n[[33]]\nvalue\n      21       22       41       42       52       82 \n0.004269 0.026681 0.008538 0.029883 0.687300 0.243330 \n\n[[34]]\nvalue\n      22       41       42       52       82 \n0.008538 0.042689 0.048026 0.770544 0.130203 \n\n[[35]]\nvalue\n      21       41       42       52       82 \n0.004255 0.046809 0.069149 0.612766 0.267021 \n\n[[36]]\nvalue\n      21       41       42       52       82 \n0.004274 0.045940 0.068376 0.611111 0.270299 \n\n[[37]]\nvalue\n      21       41       42       52       82 \n0.004251 0.046759 0.071201 0.609989 0.267800 \n\n[[38]]\nvalue\n      21       41       42       52       82 \n0.004283 0.046039 0.068522 0.612420 0.268737 \n\n[[39]]\nvalue\n      21       22       41       42       52       82 \n0.004264 0.027719 0.024520 0.038380 0.732409 0.172708 \n\n[[40]]\nvalue\n      21       22       41       42       52       82 \n0.013800 0.031847 0.043524 0.049894 0.676221 0.184713 \n\n[[41]]\nvalue\n      21       22       41       42       52       82 \n0.035294 0.022460 0.044920 0.058824 0.638503 0.200000 \n\n[[42]]\nvalue\n      21       22       41       42       52       82 \n0.042463 0.023355 0.041401 0.063694 0.639066 0.190021 \n\n[[43]]\nvalue\n      21       41       42       52       82 \n0.011740 0.046958 0.059765 0.649947 0.231590 \n\n[[44]]\nvalue\n      21       41       42       52       82 \n0.011727 0.046908 0.059701 0.650320 0.231343 \n\n[[45]]\nvalue\n      21       41       42       52       82 \n0.011702 0.046809 0.059574 0.646809 0.235106 \n\n[[46]]\nvalue\n      21       41       42       52       82 \n0.011715 0.046858 0.060703 0.647497 0.233227 \n\n[[47]]\nvalue\n      21       22       41       42       52       82 \n0.004269 0.027748 0.020277 0.037353 0.715048 0.195304 \n\n[[48]]\nvalue\n      21       22       41       42       52       82 \n0.004251 0.026567 0.015940 0.034006 0.705632 0.213603 \n\n[[49]]\nvalue\n      21       22       41       42       52       82 \n0.004242 0.027572 0.033934 0.041357 0.743372 0.149523 \n\n[[50]]\nvalue\n      21       22       41       42       52       82 \n0.004246 0.026539 0.033970 0.041401 0.746285 0.147558 \n\n[[51]]\nvalue\n      21       22       41       42       52       82 \n0.004233 0.025397 0.044444 0.052910 0.710053 0.162963 \n\n[[52]]\nvalue\n      21       22       41       42       52       82 \n0.005336 0.029883 0.043757 0.050160 0.713981 0.156884 \n\n[[53]]\nvalue\n      21       22       41       42       52       82 \n0.005342 0.029915 0.043803 0.050214 0.714744 0.155983 \n\n[[54]]\nvalue\n      21       22       41       42       52       82 \n0.005308 0.028662 0.043524 0.049894 0.714437 0.158174 \n\n[[55]]\nvalue\n      21       41       42       52       82 \n0.021322 0.063966 0.065032 0.615139 0.234542 \n\n[[56]]\nvalue\n      21       41       42       52       82 \n0.016949 0.059322 0.064619 0.617585 0.241525 \n\n[[57]]\nvalue\n      41       42       52       82 \n0.056503 0.062900 0.626866 0.253731 \n\n[[58]]\nvalue\n      21       41       42       52       82 \n0.004260 0.045793 0.069223 0.612354 0.268371 \n\n[[59]]\nvalue\n      21       41       42       52       82 \n0.008466 0.049735 0.068783 0.607407 0.265608 \n\n[[60]]\nvalue\n      21       41       42       52       82 \n0.007431 0.049894 0.069002 0.608280 0.265393 \n\n[[61]]\nvalue\n      21       41       42       52       82 \n0.007455 0.050053 0.070288 0.608094 0.264111 \n\n[[62]]\nvalue\n      21       41       42       52       82 \n0.006403 0.049093 0.072572 0.605123 0.266809 \n\n[[63]]\nvalue\n      21       22       42       52       82 \n0.002132 0.021322 0.025586 0.656716 0.294243 \n\n[[64]]\nvalue\n      21       22       41       42       52       82 \n0.001065 0.008520 0.011715 0.027689 0.732694 0.218317 \n\n[[65]]\nvalue\n      21       22       41       42       52       82 \n0.002134 0.010672 0.036286 0.040555 0.767343 0.143010 \n\n[[66]]\nvalue\n      22       41       42       52       82 \n0.004260 0.045793 0.062833 0.764643 0.122471 \n\n[[67]]\nvalue\n      21       41       42       52       82 \n0.004255 0.046809 0.070213 0.610638 0.268085 \n\n[[68]]\nvalue\n      21       41       42       52       82 \n0.004269 0.045891 0.068303 0.612593 0.268943 \n\n[[69]]\nvalue\n      21       41       42       52       82 \n0.004255 0.045745 0.069149 0.612766 0.268085 \n\n[[70]]\nvalue\n      21       41       42       52       82 \n0.004269 0.045891 0.068303 0.612593 0.268943 \n\n[[71]]\nvalue\n      21       22       41       42       52       82 \n0.004260 0.027689 0.020234 0.037274 0.715655 0.194888 \n\n[[72]]\nvalue\n      21       22       41       42       52       82 \n0.004274 0.025641 0.006410 0.027778 0.675214 0.260684 \n\n[[73]]\nvalue\n      21       22       41       42       52       82 \n0.001063 0.008502 0.040383 0.041445 0.769394 0.139214 \n\n[[74]]\nvalue\n      21       22       41       42       52       90 \n0.038217 0.022293 0.008493 0.233546 0.695329 0.002123 \n\n[[75]]\nvalue\n      21       41       42       52       82 \n0.004251 0.052072 0.072264 0.616366 0.255048 \n\n[[76]]\nvalue\n      21       41       42       52       82 \n0.004269 0.045891 0.068303 0.612593 0.268943 \n\n[[77]]\nvalue\n      21       41       42       52       82 \n0.004274 0.045940 0.068376 0.613248 0.268162 \n\n[[78]]\nvalue\n      41       42       52       82 \n0.048832 0.078556 0.633758 0.238854 \n\n[[79]]\nvalue\n      41       42       52       82 \n0.028693 0.089267 0.879915 0.002125 \n\n[[80]]\nvalue\n      21       41       42       52       82 \n0.004260 0.031949 0.071353 0.864750 0.027689 \n\n[[81]]\nvalue\n      21       41       42       52       82 \n0.004251 0.035069 0.040383 0.912859 0.007439 \n\n[[82]]\nvalue\n      41       42       52 \n0.025451 0.009544 0.965005 \n\n[[83]]\nvalue\n      41       42       52 \n0.025451 0.003181 0.971368 \n\n[[84]]\nvalue\n      41       42       52 \n0.037116 0.069989 0.892895 \n\n[[85]]\nvalue\n      41       42       52 \n0.025641 0.043803 0.930556 \n\n[[86]]\nvalue\n      41       42       52 \n0.026483 0.049788 0.923729 \n\n[[87]]\nvalue\n      21       41       42       52       82 \n0.004274 0.040598 0.035256 0.898504 0.021368 \n\n[[88]]\nvalue\n      21       41       42       52       82 \n0.004264 0.040512 0.035181 0.898721 0.021322 \n\n[[89]]\nvalue\n      21       41       42       52       82 \n0.004260 0.040469 0.035144 0.898829 0.021299 \n\n[[90]]\nvalue\n      21       22       41       42       52       82 \n0.016985 0.005308 0.010616 0.011677 0.715499 0.239915 \n\n[[91]]\nvalue\n      41       42       52 \n0.041401 0.063694 0.894904 \n\n[[92]]\nvalue\n      41       42       52 \n0.041622 0.058698 0.899680 \n\n[[93]]\nvalue\n      41       42       52 \n0.041489 0.059574 0.898936 \n\n[[94]]\nvalue\n      41       42       52 \n0.042599 0.059638 0.897764 \n\n[[95]]\nvalue\n      41       42       52 \n0.040297 0.081654 0.878049 \n\n[[96]]\nvalue\n      41       52 \n0.013889 0.986111 \n\n[[97]]\nvalue\n      42       52 \n0.007447 0.992553 \n\n[[98]]\nvalue\n      41       42       52 \n0.031847 0.043524 0.924628 \n\n[[99]]\nvalue\n      41       42       52 \n0.043571 0.051010 0.905420 \n\n[[100]]\nvalue\n      41       42       52 \n0.025451 0.046660 0.927890 \n\n[[101]]\nvalue\n      41       42       52 \n0.025397 0.046561 0.928042 \n\n[[102]]\nvalue\n      41       42       52 \n0.025641 0.048077 0.926282 \n\n[[103]]\nvalue\n      21       41       42       52       82 \n0.004233 0.041270 0.058201 0.853968 0.042328 \n\n[[104]]\nvalue\n      21       41       42       52       82 \n0.004264 0.041578 0.059701 0.857143 0.037313 \n\n[[105]]\nvalue\n      41       42       52       82 \n0.032874 0.089077 0.820785 0.057264 \n\n[[106]]\nvalue\n      41       42       52       82 \n0.024442 0.058448 0.718385 0.198725 \n\n[[107]]\nvalue\n      21       22       41       42       52       82       90 \n0.051010 0.024442 0.046759 0.279490 0.460149 0.134963 0.003188 \n\n[[108]]\nvalue\n      21       22       41       42       52       82       90 \n0.051173 0.025586 0.040512 0.278252 0.490405 0.110874 0.003198 \n\n[[109]]\nvalue\n      21       22       41       42       52       82       90 \n0.052239 0.024520 0.044776 0.281450 0.466951 0.126866 0.003198 \n\n[[110]]\nvalue\n      21       22       41       42       52       82       90 \n0.050107 0.026652 0.050107 0.266525 0.466951 0.136461 0.003198 \n\n[[111]]\nvalue\n      21       22       41       42       52       82       90 \n0.044728 0.033014 0.022364 0.152290 0.593184 0.151225 0.003195 \n\n[[112]]\nvalue\n      21       22       41       42       52       82       90 \n0.047821 0.034006 0.026567 0.158342 0.580234 0.149841 0.003188 \n\n[[113]]\nvalue\n      21       22       41       42       52       82       90 \n0.045503 0.033862 0.025397 0.129101 0.607407 0.155556 0.003175 \n\n[[114]]\nvalue\n      21       22       41       42       52       82 \n0.041357 0.023330 0.040297 0.061506 0.639449 0.194062 \n\n[[115]]\nvalue\n      21       41       42       52       82 \n0.007447 0.050000 0.070213 0.607447 0.264894 \n\n[[116]]\nvalue\n      21       41       42       52       82 \n0.006390 0.050053 0.071353 0.607029 0.265176 \n\n[[117]]\nvalue\n      21       41       42       52       82 \n0.007447 0.050000 0.069149 0.607447 0.265957 \n\n[[118]]\nvalue\n      21       41       42       52       82 \n0.002137 0.049145 0.067308 0.616453 0.264957 \n\n[[119]]\nvalue\n      21       41       42       52       82 \n0.004255 0.048936 0.064894 0.618085 0.263830 \n\n[[120]]\nvalue\n      21       22       41       42       52       82 \n0.004246 0.026539 0.007431 0.027601 0.671975 0.262208 \n\n[[121]]\nvalue\n      21       22       41       42       52       82 \n0.004274 0.024573 0.001068 0.022436 0.632479 0.315171 \n\n[[122]]\nvalue\n      22       41       42       52       82 \n0.004237 0.046610 0.066737 0.741525 0.140890 \n\n[[123]]\nvalue\n      21       41       42       52       82 \n0.001064 0.051064 0.071277 0.638298 0.238298 \n\n[[124]]\nvalue\n      41       42       52       82 \n0.050107 0.076759 0.637527 0.235608 \n\n[[125]]\nvalue\n      21       41       42       52       82 \n0.004255 0.046809 0.068085 0.613830 0.267021 \n\n[[126]]\nvalue\n      41       42       52       82 \n0.058511 0.058511 0.617021 0.265957 \n\n[[127]]\nvalue\n      41       42       52       82 \n0.043757 0.041622 0.637140 0.277481 \n\n[[128]]\nvalue\n      41       42       52       82 \n0.036017 0.037076 0.661017 0.265890 \n\n[[129]]\nvalue\n      41       42       52       82 \n0.034043 0.035106 0.663830 0.267021 \n\n[[130]]\nvalue\n      41       42       52       82 \n0.034043 0.038298 0.661702 0.265957 \n\n[[131]]\nvalue\n      41       42       52       82 \n0.053305 0.065032 0.628998 0.252665 \n\n[[132]]\nvalue\n      41       42       52       82 \n0.050955 0.072187 0.652866 0.223992 \n\n[[133]]\nvalue\n      21       22       41       42       52       82       90 \n0.050214 0.027778 0.036325 0.266026 0.519231 0.097222 0.003205 \n\n[[134]]\nvalue\n      41       42       52       82 \n0.061966 0.055556 0.586538 0.295940 \n\n[[135]]\nvalue\n      41       42       52       82 \n0.032944 0.032944 0.673751 0.260361 \n\n[[136]]\nvalue\n      41       42       52       82 \n0.029630 0.030688 0.713228 0.226455 \n\n[[137]]\nvalue\n      21       41       42       52       82 \n0.004246 0.032909 0.031847 0.925690 0.005308 \n\n[[138]]\nvalue\n      41       42       52 \n0.025397 0.045503 0.929101 \n\n[[139]]\nvalue\n      41       42       52 \n0.025641 0.048077 0.926282 \n\n[[140]]\nvalue\n      41       42       52 \n0.030720 0.036017 0.933263 \n\n[[141]]\nvalue\n      41       42       52 \n0.003178 0.005297 0.991525 \n\n[[142]]\nvalue\n      21       41       42       52 \n0.004255 0.025532 0.012766 0.957447 \n\n[[143]]\nvalue\n      21       41       42       52 \n0.004251 0.027630 0.019129 0.948990 \n\n[[144]]\nvalue\n      21       41       42       52       82 \n0.004260 0.030884 0.029819 0.926518 0.008520 \n\n[[145]]\nvalue\n      21       41       42       52       82 \n0.004246 0.023355 0.021231 0.899151 0.052017 \n\n[[146]]\nvalue\n      41       42       52 \n0.026567 0.048884 0.924548 \n\n[[147]]\nvalue\n      41       42       52 \n0.025505 0.048884 0.925611 \n\n[[148]]\nvalue\n      41       42       52 \n0.025505 0.041445 0.933050 \n\n[[149]]\nvalue\n      41       42       52 \n0.025505 0.023379 0.951116 \n\n[[150]]\nvalue\n      21       41       42       52       82 \n0.004255 0.031915 0.071277 0.863830 0.028723 \n\n[[151]]\nvalue\n      41       42       52       82 \n0.040555 0.069370 0.808965 0.081110 \n\n[[152]]\nvalue\n      41       42       52       82 \n0.034115 0.094883 0.833689 0.037313 \n\n[[153]]\nvalue\n      41       42       52       82 \n0.042418 0.063627 0.801697 0.092259 \n\n[[154]]\nvalue\n      41       42       52 \n0.043617 0.051064 0.905319 \n\n[[155]]\nvalue\n      41       42       52 \n0.043663 0.051118 0.905218 \n\n[[156]]\nvalue\n      41       42       52 \n0.025586 0.045842 0.928571 \n\n[[157]]\nvalue\n      41       42       52 \n0.035106 0.042553 0.922340 \n\n[[158]]\nvalue\n      21       22       41       42       52       82 \n0.025505 0.007439 0.005313 0.011690 0.720510 0.229543 \n\n[[159]]\nvalue\n      21       22       41       42       52       82 \n0.014846 0.005302 0.010604 0.010604 0.731707 0.226935 \n\n[[160]]\nvalue\n      21       22       41       42       52       82 \n0.013815 0.004251 0.010627 0.018066 0.683316 0.269926 \n\n[[161]]\nvalue\n      41       42       52       82 \n0.045599 0.046660 0.689290 0.218452 \n\n[[162]]\nvalue\n      21       41       42       52       82 \n0.002125 0.048884 0.065887 0.620616 0.262487 \n\n[[163]]\nvalue\n      21       41       42       52       82 \n0.001060 0.049841 0.065748 0.620361 0.262990 \n\n[[164]]\nvalue\n      21       41       42       52       82 \n0.002137 0.049145 0.067308 0.618590 0.262821 \n\n[[165]]\nvalue\n      21       41       42       52       82 \n0.002128 0.048936 0.065957 0.615957 0.267021 \n\n[[166]]\nvalue\n      22       41       42       52       82 \n0.004246 0.030786 0.029724 0.760085 0.175159 \n\n[[167]]\nvalue\n      21       22       41       42       52       82 \n0.011715 0.004260 0.010650 0.018104 0.692226 0.263046 \n\n[[168]]\nvalue\n      41       42       52       82 \n0.037194 0.046759 0.681190 0.234857 \n\n[[169]]\nvalue\n      41       42       52       82 \n0.039488 0.039488 0.725720 0.195304 \n\n[[170]]\nvalue\n      21       41       42       52       82 \n0.002125 0.027630 0.077577 0.891605 0.001063 \n\n[[171]]\nvalue\n      41       42       52 \n0.027601 0.070064 0.902335 \n\n[[172]]\nvalue\n      41       42       52 \n0.027630 0.068013 0.904357 \n\n[[173]]\nvalue\n      41       42       52 \n0.050214 0.082265 0.867521 \n\n[[174]]\nvalue\n      41       42       52       82 \n0.028785 0.045842 0.760128 0.165245 \n\n[[175]]\nvalue\n      41       42       52       82 \n0.039278 0.045648 0.803609 0.111465 \n\n[[176]]\nvalue\n      41       42       52 \n0.042463 0.082803 0.874735 \n\n[[177]]\nvalue\n      41       42       52 \n0.038380 0.077825 0.883795 \n\n[[178]]\nvalue\n      21       41       42       52 \n0.003178 0.051907 0.094280 0.850636 \n\n[[179]]\nvalue\n      21       41       42       52 \n0.001063 0.081828 0.113709 0.803401 \n\n[[180]]\nvalue\n      41       42       52 \n0.011702 0.065957 0.922340 \n\n[[181]]\nvalue\n      41       42       52 \n0.026567 0.061637 0.911796 \n\n[[182]]\nvalue\n      41       42       52 \n0.033049 0.072495 0.894456 \n\n[[183]]\nvalue\n      41       42       52       82 \n0.046709 0.055202 0.799363 0.098726 \n\n[[184]]\nvalue\n      21       41       42       52       82 \n0.004242 0.033934 0.065748 0.873807 0.022269 \n\n[[185]]\nvalue\n      41       42       52 \n0.026539 0.049894 0.923567 \n\n[[186]]\nvalue\n      41       42       52 \n0.028785 0.050107 0.921109 \n\n[[187]]\nvalue\n      41       42       52 \n0.028693 0.051010 0.920298 \n\n[[188]]\nvalue\n      21       41       42       52 \n0.002123 0.027601 0.058386 0.911890 \n\n[[189]]\nvalue\n      41       42       52       82 \n0.033049 0.046908 0.737740 0.182303 \n\n[[190]]\nvalue\n      41       42       52       82 \n0.025559 0.057508 0.692226 0.224707 \n\n[[191]]\nvalue\n      41       42       52       82 \n0.028662 0.071125 0.799363 0.100849 \n\n[[192]]\nvalue\n      41       42       52       82 \n0.034006 0.061637 0.659936 0.244421 \n\n[[193]]\nvalue\n      41       42       52 \n0.043803 0.052350 0.903846 \n\n[[194]]\nvalue\n      41       42       52 \n0.043710 0.051173 0.905117 \n\n[[195]]\nvalue\n      41       42       52 \n0.043524 0.049894 0.906582 \n\n[[196]]\nvalue\n      41       42       52 \n0.029819 0.068158 0.902023 \n\n[[197]]\nvalue\n      41       42       52       82 \n0.072495 0.069296 0.627932 0.230277 \n\n[[198]]\nvalue\n      21       41       42       52       82 \n0.012712 0.095339 0.108051 0.547669 0.236229 \n\n[[199]]\nvalue\n      21       41       42       52       82 \n0.007447 0.050000 0.071277 0.607447 0.263830 \n\n[[200]]\nvalue\n      21       41       42       52       82 \n0.011702 0.048936 0.064894 0.626596 0.247872 \n\n[[201]]\nvalue\n      21       41       42       52       82 \n0.007439 0.049947 0.069075 0.608927 0.264612 \n\n[[202]]\nvalue\n      21       41       42       52       82 \n0.008520 0.051118 0.069223 0.611289 0.259851 \n\n[[203]]\nvalue\n      21       41       42       52       82 \n0.001067 0.051227 0.070438 0.637140 0.240128 \n\n[[204]]\nvalue\n      21       22       41       42       52       82 \n0.021277 0.014894 0.044681 0.056383 0.644681 0.218085 \n\n[[205]]\nvalue\n      21       22       41       42       52       82       90 \n0.030983 0.038462 0.006410 0.075855 0.600427 0.244658 0.003205 \n\n[[206]]\nvalue\n      21       22       41       42       52       82       90 \n0.018085 0.039362 0.006383 0.041489 0.637234 0.254255 0.003191 \n\n[[207]]\nvalue\n      21       22       41       42       52       82       90 \n0.054429 0.024546 0.052295 0.251868 0.418356 0.195304 0.003202 \n\n[[208]]\nvalue\n      21       22       41       42       52       82       90 \n0.047923 0.031949 0.027689 0.173589 0.568690 0.146965 0.003195 \n\n[[209]]\nvalue\n      21       22       41       42       52       82       90 \n0.049947 0.027630 0.047821 0.242295 0.506908 0.122210 0.003188 \n\n[[210]]\nvalue\n      21       22       41       42       52       82       90 \n0.050160 0.025614 0.034152 0.272145 0.524013 0.090715 0.003202 \n\n[[211]]\nvalue\n      21       22       41       42       52       82       90 \n0.049947 0.030818 0.038257 0.212540 0.546227 0.119022 0.003188 \n\n[[212]]\nvalue\n      21       41       42       52       82 \n0.008502 0.051010 0.068013 0.606801 0.265675 \n\n[[213]]\nvalue\n      21       22       41       42       52       82       90 \n0.055556 0.024573 0.054487 0.244658 0.415598 0.201923 0.003205 \n\n[[214]]\nvalue\n      21       22       41       42       52       82 \n0.045793 0.018104 0.095847 0.165069 0.446219 0.228967 \n\n[[215]]\nvalue\n      21       22       41       42       52       82 \n0.041578 0.010661 0.083156 0.233475 0.280384 0.350746 \n\n[[216]]\nvalue\n      21       41       42       52       82 \n0.016967 0.078473 0.076352 0.584305 0.243902 \n\n[[217]]\nvalue\n      41       42       52 \n0.025614 0.046958 0.927428 \n\n[[218]]\nvalue\n      41       42       52 \n0.025586 0.046908 0.927505 \n\n[[219]]\nvalue\n      41       42       52 \n0.026455 0.051852 0.921693 \n\n[[220]]\nvalue\n      41       42       52 \n0.036247 0.044776 0.918977 \n\n[[221]]\nvalue\n      21       41       42       52 \n0.003191 0.041489 0.084043 0.871277 \n\n[[222]]\nvalue\n      41       42       52       82 \n0.036325 0.098291 0.839744 0.025641 \n\n[[223]]\nvalue\n      41       42       52       82 \n0.035106 0.098936 0.839362 0.026596 \n\n[[224]]\nvalue\n      41       42       52       82 \n0.035219 0.042689 0.661686 0.260406 \n\n[[225]]\nvalue\n      41       42       52 \n0.032944 0.029756 0.937301 \n\n[[226]]\nvalue\n      41       42       52 \n0.006403 0.072572 0.921025 \n\n[[227]]\nvalue\n      41       42       52 \n0.005319 0.063830 0.930851 \n\n[[228]]\nvalue\n      41       42       52 \n0.006403 0.065101 0.928495 \n\n[[229]]\nvalue\n      41       42       52 \n0.006383 0.100000 0.893617 \n\n[[230]]\nvalue\n      41       42       52 \n0.058698 0.184632 0.756670 \n\n[[231]]\nvalue\n      41       42       52 \n0.006363 0.078473 0.915164 \n\n[[232]]\nvalue\n      21       41       42       52 \n0.001066 0.072495 0.105544 0.820896 \n\n[[233]]\nvalue\n      21       41       42       52 \n0.001067 0.049093 0.090715 0.859125 \n\n[[234]]\nvalue\n      21       41       42       52 \n0.001067 0.068303 0.105656 0.824973 \n\n[[235]]\nvalue\n      41       42       52 \n0.012725 0.081654 0.905620 \n\n[[236]]\nvalue\n      41       42       52 \n0.006403 0.068303 0.925293 \n\n[[237]]\nvalue\n      41       42       52 \n0.006390 0.069223 0.924388 \n\n[[238]]\nvalue\n      41       42       52 \n0.006390 0.077742 0.915868 \n\n[[239]]\nvalue\n      41       42       52 \n0.001064 0.054255 0.944681 \n\n[[240]]\nvalue\n      42       52 \n0.003198 0.996802 \n\n[[241]]\nvalue\n      41       42       52 \n0.015957 0.015957 0.968085 \n\n[[242]]\nvalue\n      41       42       52 \n0.025586 0.059701 0.914712 \n\n[[243]]\nvalue\n52 \n 1 \n\n[[244]]\nvalue\n      21       42       52 \n0.001067 0.007471 0.991462 \n\n[[245]]\nvalue\n      41       42       52       82 \n0.045599 0.063627 0.728526 0.162248 \n\n[[246]]\nvalue\n      41       42       52       82 \n0.037037 0.043386 0.653968 0.265608 \n\n[[247]]\nvalue\n      41       42       52       82 \n0.031915 0.035106 0.676596 0.256383 \n\n[[248]]\nvalue\n      41       42       52       82 \n0.037353 0.039488 0.651014 0.272145 \n\n[[249]]\nvalue\n      21       41       42       52 \n0.001062 0.048832 0.104034 0.846072 \n\n[[250]]\nvalue\n      21       41       42       52 \n0.001063 0.088204 0.113709 0.797024 \n\n[[251]]\nvalue\n      21       41       42       52 \n0.001064 0.097872 0.122340 0.778723 \n\n[[252]]\nvalue\n      21       41       42       52 \n0.001067 0.057631 0.097118 0.844184 \n\n[[253]]\nvalue\n      21       41       42       52 \n0.002125 0.044633 0.088204 0.865037 \n\n[[254]]\nvalue\n      21       41       42       52 \n0.003185 0.042463 0.117834 0.836518 \n\n[[255]]\nvalue\n      21       41       42       52 \n0.002125 0.044633 0.107333 0.845909 \n\n[[256]]\nvalue\n      21       41       42       52 \n0.003188 0.052072 0.115834 0.828905 \n\n[[257]]\nvalue\n      21       41       42       52 \n0.002125 0.046759 0.087141 0.863974 \n\n[[258]]\nvalue\n      21       41       42       52 \n0.001066 0.099147 0.120469 0.779318 \n\n[[259]]\nvalue\n      21       41       42       52 \n0.001064 0.084043 0.111702 0.803191 \n\n[[260]]\nvalue\n      41       42       52 \n0.032909 0.066879 0.900212 \n\n[[261]]\nvalue\n      41       42       52       82 \n0.031813 0.090138 0.828208 0.049841 \n\n[[262]]\nvalue\n      41       42       52       82 \n0.034043 0.070213 0.791489 0.104255 \n\n[[263]]\nvalue\n      41       42       52       82 \n0.031949 0.050053 0.726305 0.191693 \n\n[[264]]\nvalue\n      41       42       52       82 \n0.044872 0.048077 0.664530 0.242521 \n\n[[265]]\nvalue\n      41       42       52       82 \n0.031915 0.054255 0.701064 0.212766 \n\n[[266]]\nvalue\n      41       42       52       82 \n0.031949 0.054313 0.709265 0.204473 \n\n[[267]]\nvalue\n      41       42       52       82 \n0.031915 0.054255 0.701064 0.212766 \n\n[[268]]\nvalue\n      41       42       52       82 \n0.040426 0.050000 0.684043 0.225532 \n\n[[269]]\nvalue\n      41       42       52       82 \n0.035144 0.046858 0.725240 0.192758 \n\n[[270]]\nvalue\n      41       42       52       82 \n0.034115 0.051173 0.723881 0.190832 \n\n[[271]]\nvalue\n      41       42       52       82 \n0.041667 0.045940 0.660256 0.252137 \n\n[[272]]\nvalue\n      41       42       52       82 \n0.027601 0.085987 0.875796 0.010616 \n\n[[273]]\nvalue\n      41       42       52 \n0.037155 0.070064 0.892781 \n\n[[274]]\nvalue\n      41       42       52 \n0.035219 0.080043 0.884739 \n\n[[275]]\nvalue\n      41       42       52 \n0.035256 0.079060 0.885684 \n\n[[276]]\nvalue\n      21       41       42       52 \n0.001066 0.031983 0.028785 0.938166 \n\n[[277]]\nvalue\n      21       41       42       52 \n0.002128 0.032979 0.072340 0.892553 \n\n[[278]]\nvalue\n      21       41       42       52 \n0.002128 0.032979 0.074468 0.890426 \n\n[[279]]\nvalue\n      21       41       42       52 \n0.003205 0.040598 0.088675 0.867521 \n\n[[280]]\nvalue\n      41       42       52 \n0.038503 0.051337 0.910160 \n\n[[281]]\nvalue\n      21       41       42       52 \n0.001067 0.033084 0.058698 0.907150 \n\n[[282]]\nvalue\n      41       42       52 \n0.030688 0.050794 0.918519 \n\n[[283]]\nvalue\n      41       42       52 \n0.029947 0.051337 0.918717 \n\n[[284]]\nvalue\n      41       42       52 \n0.026652 0.035181 0.938166 \n\n[[285]]\nvalue\n      41       42       52       82 \n0.034958 0.097458 0.848517 0.019068 \n\n[[286]]\nvalue\n      41       42       52       82 \n0.033049 0.091684 0.827292 0.047974 \n\n[[287]]\nvalue\n      41       42       52 \n0.040297 0.088017 0.871686 \n\n[[288]]\nvalue\n      41       42       52 \n0.036247 0.035181 0.928571 \n\n[[289]]\nvalue\n      21       41       42       52 \n0.001067 0.076841 0.105656 0.816435 \n\n[[290]]\nvalue\n      42       52 \n0.021277 0.978723 \n\n[[291]]\nvalue\n      42       52 \n0.021254 0.978746 \n\n[[292]]\nvalue\n      21       41       42       52 \n0.001063 0.096706 0.121148 0.781084 \n\n[[293]]\nvalue\n      21       41       42       52 \n0.001064 0.064894 0.105319 0.828723 \n\n[[294]]\nvalue\n      21       41       42       52 \n0.001063 0.063762 0.106270 0.828905 \n\n[[295]]\nvalue\n      21       41       42       52 \n0.001065 0.054313 0.096912 0.847710 \n\n[[296]]\nvalue\n      41       42       52 \n0.031881 0.051010 0.917109 \n\n[[297]]\nvalue\n      21       41       42       52 \n0.002130 0.036209 0.075612 0.886049 \n\n[[298]]\nvalue\n      21       41       42       52 \n0.002132 0.028785 0.082090 0.886994 \n\n[[299]]\nvalue\n      21       41       42       52 \n0.002130 0.028754 0.082002 0.887114 \n\n[[300]]\nvalue\n      21       41       42       52 \n0.003188 0.031881 0.081828 0.883103 \n\n[[301]]\nvalue\n      21       41       42       52 \n0.003188 0.038257 0.104145 0.854410 \n\n[[302]]\nvalue\n      21       41       42       52 \n0.003188 0.021254 0.103082 0.872476 \n\n[[303]]\nvalue\n      21       41       42       52 \n0.001063 0.063762 0.088204 0.846971 \n\n[[304]]\nvalue\n      41       42       52 \n0.042463 0.078556 0.878981 \n\n[[305]]\nvalue\n      41       42       52 \n0.033049 0.075693 0.891258 \n\n[[306]]\nvalue\n      41       42       52 \n0.032979 0.080851 0.886170 \n\n[[307]]\nvalue\n      41       42       52 \n0.032909 0.080679 0.886412 \n\n[[308]]\nvalue\n      41       42       52       82 \n0.036093 0.084926 0.877919 0.001062 \n\n[[309]]\nvalue\n      41       42       52       82 \n0.033049 0.050107 0.764392 0.152452 \n\n[[310]]\nvalue\n      41       42       52       82 \n0.032979 0.059574 0.784043 0.123404 \n\n[[311]]\nvalue\n      41       42       52       82 \n0.036170 0.085106 0.827660 0.051064 \n\n[[312]]\nvalue\n      41       42       52 \n0.035219 0.083244 0.881537 \n\n[[313]]\nvalue\n      41       42       52 \n0.040212 0.074074 0.885714 \n\n[[314]]\nvalue\n      41       42       52 \n0.042418 0.075292 0.882291 \n\n[[315]]\nvalue\n      21       41       42       52 \n0.002125 0.036132 0.082891 0.878852 \n\n[[316]]\nvalue\n      41       42       52 \n0.026567 0.023379 0.950053 \n\n[[317]]\nvalue\n      41       42       52 \n0.025559 0.004260 0.970181 \n\n[[318]]\nvalue\n      41       42       52 \n0.025614 0.005336 0.969050 \n\n[[319]]\nvalue\n      41       42       52 \n0.025586 0.012793 0.961620 \n\n[[320]]\nvalue\n      41       42       52       82 \n0.036286 0.049093 0.757737 0.156884 \n\n[[321]]\nvalue\n      41       42       52 \n0.026624 0.057508 0.915868 \n\n[[322]]\nvalue\n      41       42       52 \n0.025424 0.046610 0.927966 \n\n[[323]]\nvalue\n      41       42       52 \n0.025451 0.045599 0.928950 \n\n[[324]]\nvalue\n      41       42       52 \n0.035069 0.081828 0.883103 \n\n[[325]]\nvalue\n      41       42       52 \n0.037313 0.077825 0.884861 \n\n[[326]]\nvalue\n      41       42       52 \n0.039278 0.081741 0.878981 \n\n[[327]]\nvalue\n      41       42       52 \n0.040555 0.082177 0.877268 \n\n[[328]]\nvalue\n      41       42       52       82 \n0.027719 0.086354 0.880597 0.005330 \n\n[[329]]\nvalue\n      21       41       42       52 \n0.001060 0.038176 0.065748 0.895016 \n\n[[330]]\nvalue\n      21       41       42       52 \n0.003191 0.028723 0.100000 0.868085 \n\n[[331]]\nvalue\n      21       41       42       52 \n0.003198 0.027719 0.086354 0.882729 \n\n[[332]]\nvalue\n      41       42       52 \n0.030818 0.034006 0.935175 \n\n[[333]]\nvalue\n      41       42       52 \n0.028662 0.018047 0.953291 \n\n[[334]]\nvalue\n      41       42       52 \n0.029724 0.013800 0.956476 \n\n[[335]]\nvalue\n      41       42       52 \n0.029630 0.014815 0.955556 \n\n[[336]]\nvalue\n      41       42       52 \n0.025478 0.059448 0.915074 \n\n[[337]]\nvalue\n      41       42       52 \n0.018008 0.024364 0.957627 \n\n[[338]]\nvalue\n      41       42       52 \n0.025614 0.048026 0.926361 \n\n[[339]]\nvalue\n      41       42       52 \n0.011640 0.006349 0.982011 \n\n[[340]]\nvalue\n      41       42       52 \n0.011727 0.004264 0.984009 \n\n[[341]]\nvalue\n      41       42       52 \n0.022293 0.002123 0.975584 \n\n[[342]]\nvalue\n      41       42       52 \n0.020148 0.002121 0.977731 \n\n[[343]]\nvalue\n      41       42       52 \n0.025505 0.001063 0.973433 \n\n[[344]]\nvalue\n      41       42       52 \n0.025424 0.009534 0.965042 \n\n[[345]]\nvalue\n      41       42       52 \n0.003191 0.002128 0.994681 \n\n[[346]]\nvalue\n      41       42       52 \n0.025451 0.045599 0.928950 \n\n[[347]]\nvalue\n      41       42       52 \n0.025424 0.046610 0.927966 \n\n[[348]]\nvalue\n      41       42       52       82 \n0.037353 0.092850 0.866596 0.003202 \n\n[[349]]\nvalue\n      41       42       52       82 \n0.038298 0.063830 0.791489 0.106383 \n\n[[350]]\nvalue\n      41       42       52       82 \n0.031847 0.052017 0.719745 0.196391 \n\n[[351]]\nvalue\n      41       42       52       82 \n0.031847 0.054140 0.709130 0.204883 \n\n[[352]]\nvalue\n      41       42       52       82 \n0.026567 0.058448 0.695005 0.219979 \n\n[[353]]\nvalue\n      41       42       52 \n0.044633 0.081828 0.873539 \n\n[[354]]\nvalue\n      41       42       52 \n0.036132 0.075452 0.888417 \n\n[[355]]\nvalue\n      41       42       52       82 \n0.035144 0.078807 0.883919 0.002130 \n\n[[356]]\nvalue\n      41       42       52       82 \n0.032909 0.067941 0.670913 0.228238 \n\n[[357]]\nvalue\n      41       42       52       82 \n0.048988 0.087327 0.654952 0.208733 \n\n[[358]]\nvalue\n      41       42       52       82 \n0.028632 0.071050 0.799576 0.100742 \n\n[[359]]\nvalue\n      41       42       52       82 \n0.031746 0.053968 0.706878 0.207407 \n\n[[360]]\nvalue\n      41       42       52       82 \n0.040426 0.052128 0.774468 0.132979 \n\n[[361]]\nvalue\n      21       41       42       52 \n0.001064 0.037234 0.085106 0.876596 \n\n[[362]]\nvalue\n      41       42       52 \n0.022269 0.081654 0.896076 \n\n[[363]]\nvalue\n      41       42       52 \n0.022364 0.083067 0.894569 \n\n[[364]]\nvalue\n      42       52 \n0.007439 0.992561 \n\n[[365]]\nvalue\n      21       41       42       52       82 \n0.004251 0.035069 0.065887 0.855473 0.039320 \n\n[[366]]\nvalue\n      21       41       42       52       82 \n0.004269 0.038420 0.066169 0.851654 0.039488 \n\n[[367]]\nvalue\n     41      42      52      82 \n0.03095 0.08111 0.84952 0.03842 \n\n[[368]]\nvalue\n      41       42       52       82 \n0.030786 0.083864 0.825902 0.059448 \n\n[[369]]\nvalue\n      41       42       52 \n0.043850 0.082353 0.873797 \n\n[[370]]\nvalue\n      41       42       52 \n0.044824 0.081110 0.874066 \n\n[[371]]\nvalue\n      41       42       52 \n0.043571 0.081828 0.874601 \n\n[[372]]\nvalue\n      41       42       52       82 \n0.030851 0.089362 0.828723 0.051064 \n\n[[373]]\nvalue\n      41       42       52       82 \n0.039278 0.049894 0.772824 0.138004 \n\n[[374]]\nvalue\n      41       42       52       82 \n0.039278 0.049894 0.771762 0.139066 \n\n[[375]]\nvalue\n      41       42       52       82 \n0.041401 0.049894 0.781316 0.127389 \n\n[[376]]\nvalue\n      41       42       52       82 \n0.038298 0.078723 0.801064 0.081915 \n\n[[377]]\nvalue\n      41       42       52 \n0.037116 0.079533 0.883351 \n\n[[378]]\nvalue\n      41       42       52 \n0.025451 0.046660 0.927890 \n\n[[379]]\nvalue\n      41       42       52 \n0.028693 0.045696 0.925611 \n\n[[380]]\nvalue\n      41       42       52       82 \n0.027778 0.083333 0.883547 0.005342 \n\n[[381]]\nvalue\n      41       42       52       82 \n0.035256 0.073718 0.798077 0.092949 \n\n[[382]]\nvalue\n      41       42       52       82 \n0.031949 0.051118 0.776358 0.140575 \n\n[[383]]\nvalue\n      41       42       52       82 \n0.019149 0.063830 0.811702 0.105319 \n\n[[384]]\nvalue\n      41       42       52       82 \n0.025586 0.043710 0.756930 0.173774 \n\n[[385]]\nvalue\n      21       41       42       52 \n0.003181 0.027572 0.083775 0.885472 \n\n[[386]]\nvalue\n      21       41       42       52 \n0.003188 0.037194 0.080765 0.878852 \n\n[[387]]\nvalue\n      41       42       52       82 \n0.028571 0.091005 0.877249 0.003175 \n\n[[388]]\nvalue\n      41       42       52       82 \n0.028846 0.047009 0.761752 0.162393 \n\n[[389]]\nvalue\n      41       42       52       82 \n0.028754 0.048988 0.735889 0.186368 \n\n[[390]]\nvalue\n      41       42       52       82 \n0.032874 0.046660 0.758218 0.162248 \n\n[[391]]\nvalue\n      41       42       52       82 \n0.039278 0.049894 0.770701 0.140127 \n\n[[392]]\nvalue\n      41       42       52 \n0.032874 0.079533 0.887593 \n\n[[393]]\nvalue\n      41       42       52 \n0.032979 0.078723 0.888298 \n\n[[394]]\nvalue\n      41       42       52 \n0.030818 0.012752 0.956429 \n\n[[395]]\nvalue\n      21       41       42       52 \n0.004264 0.017058 0.021322 0.957356 \n\n[[396]]\nvalue\n      21       41       42       52       82 \n0.001064 0.041489 0.043617 0.820213 0.093617 \n\n[[397]]\nvalue\n      41       42       52       82 \n0.037076 0.068856 0.792373 0.101695 \n\n[[398]]\nvalue\n      41       42       52       82 \n0.026567 0.058448 0.687566 0.227418 \n\n[[399]]\nvalue\n      41       42       52       82 \n0.031915 0.054255 0.709574 0.204255 \n\n[[400]]\nvalue\n      41       42       52       82 \n0.032909 0.054140 0.713376 0.199575 \n\n[[401]]\nvalue\n      41       42       52       82 \n0.031949 0.054313 0.698616 0.215122 \n\n[[402]]\nvalue\n      41       42       52       82 \n0.031949 0.054313 0.698616 0.215122 \n\n[[403]]\nvalue\n      41       42       52       82 \n0.068158 0.066028 0.636848 0.228967 \n\n[[404]]\nvalue\n      41       42       52       82 \n0.059574 0.050000 0.652128 0.238298 \n\n[[405]]\nvalue\n      21       41       42       52       82 \n0.004251 0.094580 0.100956 0.558980 0.241233 \n\n[[406]]\nvalue\n      41       42       52       82 \n0.073639 0.069370 0.630736 0.226254 \n\n[[407]]\nvalue\n      41       42       52       82 \n0.023404 0.055319 0.697872 0.223404 \n\n[[408]]\nvalue\n      41       42       52       82 \n0.052072 0.046759 0.642933 0.258236 \n\n[[409]]\nvalue\n      41       42       52 \n0.025586 0.038380 0.936034 \n\n[[410]]\nvalue\n      21       41       42       52 \n0.001063 0.034006 0.062699 0.902232 \n\n[[411]]\nvalue\n      41       42       52 \n0.035106 0.077660 0.887234 \n\n[[412]]\nvalue\n      41       42       52 \n0.025478 0.015924 0.958599 \n\n[[413]]\nvalue\n      41       42       52 \n0.038257 0.077577 0.884166 \n\n[[414]]\nvalue\n      41       42       52 \n0.040383 0.081828 0.877790 \n\n[[415]]\nvalue\n      21       41       42       52       82 \n0.004242 0.029692 0.071050 0.868505 0.026511 \n\n[[416]]\nvalue\n      21       41       42       52 \n0.003202 0.029883 0.093917 0.872999 \n\n[[417]]\nvalue\n      21       41       42       52 \n0.003209 0.047059 0.103743 0.845989 \n\n[[418]]\nvalue\n      21       41       42       52 \n0.003188 0.046759 0.103082 0.846971 \n\n[[419]]\nvalue\n      21       41       42       52 \n0.003188 0.037194 0.105207 0.854410 \n\n[[420]]\nvalue\n      41       42       52 \n0.048884 0.087141 0.863974 \n\n[[421]]\nvalue\n      41       42       52 \n0.044681 0.078723 0.876596 \n\n[[422]]\nvalue\n      21       41       42       52 \n0.001066 0.040512 0.085288 0.873134 \n\n[[423]]\nvalue\n      21       41       42       52 \n0.001066 0.052239 0.086354 0.860341 \n\n[[424]]\nvalue\n      21       41       42       52 \n0.003205 0.038462 0.100427 0.857906 \n\n[[425]]\nvalue\n      21       41       42       52 \n0.002121 0.027572 0.082715 0.887593 \n\n[[426]]\nvalue\n      21       41       42       52 \n0.002125 0.031881 0.069075 0.896918 \n\n[[427]]\nvalue\n      41       42       52 \n0.036286 0.037353 0.926361 \n\n[[428]]\nvalue\n      41       42       52 \n0.025532 0.024468 0.950000 \n\n[[429]]\nvalue\n      41       42       52 \n0.006397 0.006397 0.987207 \n\n[[430]]\nvalue\n     41      52 \n0.00213 0.99787 \n\n[[431]]\nvalue\n      41       42       52 \n0.038176 0.074231 0.887593 \n\n[[432]]\nvalue\n      21       41       42       52 \n0.002121 0.034995 0.077413 0.885472 \n\n[[433]]\nvalue\n      21       41       42       52 \n0.002134 0.035219 0.073639 0.889007 \n\n[[434]]\nvalue\n      21       41       42       52 \n0.002134 0.046958 0.089648 0.861259 \n\n[[435]]\nvalue\n      41       42       52 \n0.040426 0.088298 0.871277 \n\n[[436]]\nvalue\n      21       41       42       52 \n0.003202 0.026681 0.118463 0.851654 \n\n[[437]]\nvalue\n      21       41       42       52 \n0.003191 0.025532 0.117021 0.854255 \n\n[[438]]\nvalue\n      21       41       42       52 \n0.002123 0.037155 0.113588 0.847134 \n\n[[439]]\nvalue\n      21       41       42       52 \n0.003185 0.024416 0.118896 0.853503 \n\n[[440]]\nvalue\n      21       41       42       52 \n0.003195 0.037274 0.104366 0.855165 \n\n[[441]]\nvalue\n      21       41       42       52 \n0.003195 0.038339 0.108626 0.849840 \n\n[[442]]\nvalue\n      21       41       42       52 \n0.003191 0.039362 0.109574 0.847872 \n\n[[443]]\nvalue\n      21       41       42       52 \n0.003185 0.042463 0.110403 0.843949 \n\n[[444]]\nvalue\n      21       41       42       52 \n0.003188 0.005313 0.107333 0.884166 \n\n[[445]]\nvalue\n      41       42       52 \n0.041401 0.081741 0.876858 \n\n[[446]]\nvalue\n      41       42       52 \n0.046908 0.084222 0.868870 \n\n[[447]]\nvalue\n      41       42       52 \n0.041578 0.083156 0.875267 \n\n[[448]]\nvalue\n      21       41       42       52 \n0.003181 0.053022 0.102863 0.840933 \n\n[[449]]\nvalue\n      21       41       42       52 \n0.001064 0.086170 0.111702 0.801064 \n\n[[450]]\nvalue\n      21       41       42       52 \n0.001059 0.076271 0.104873 0.817797 \n\n[[451]]\nvalue\n      41       42       52 \n0.035181 0.071429 0.893390 \n\n[[452]]\nvalue\n      21       41       42       52 \n0.003209 0.017112 0.101604 0.878075 \n\n[[453]]\nvalue\n      21       41       42       52 \n0.002125 0.048884 0.103082 0.845909 \n\n[[454]]\nvalue\n      21       41       42       52 \n0.003191 0.038298 0.104255 0.854255 \n\n[[455]]\nvalue\n      21       41       42       52 \n0.003202 0.038420 0.104589 0.853789 \n\n[[456]]\nvalue\n      21       41       42       52 \n0.003188 0.041445 0.089267 0.866100 \n\n[[457]]\nvalue\n      41       42       52 \n0.025451 0.022269 0.952280 \n\n[[458]]\nvalue\n      41       42       52 \n0.035106 0.032979 0.931915 \n\n[[459]]\nvalue\n      41       42       52 \n0.039446 0.044776 0.915778 \n\n[[460]]\nvalue\n      41       42       52 \n0.028785 0.038380 0.932836 \n\n[[461]]\nvalue\n      21       41       42       52 \n0.003202 0.006403 0.104589 0.885806 \n\n[[462]]\nvalue\n      21       41       42       52 \n0.003188 0.031881 0.107333 0.857598 \n\n[[463]]\nvalue\n      21       41       42       52 \n0.003202 0.027748 0.109925 0.859125 \n\n[[464]]\nvalue\n      21       41       42       52 \n0.003205 0.034188 0.107906 0.854701 \n\n[[465]]\nvalue\n      21       41       42       52 \n0.003195 0.034079 0.107561 0.855165 \n\n[[466]]\nvalue\n      21       41       42       52 \n0.003178 0.036017 0.106992 0.853814 \n\n[[467]]\nvalue\n      21       41       42       52 \n0.003198 0.033049 0.107676 0.856077 \n\n[[468]]\nvalue\n      21       41       42       52 \n0.003191 0.010638 0.103191 0.882979 \n\n[[469]]\nvalue\n      21       41       42       52 \n0.003178 0.043432 0.106992 0.846398 \n\n[[470]]\nvalue\n      21       41       42       52 \n0.003185 0.038217 0.104034 0.854565 \n\n[[471]]\nvalue\n      21       41       42       52 \n0.003188 0.042508 0.115834 0.838470 \n\n[[472]]\nvalue\n      21       41       42       52 \n0.003198 0.035181 0.102345 0.859275 \n\n[[473]]\nvalue\n      21       41       42       52 \n0.003202 0.038420 0.102455 0.855923 \n\n[[474]]\nvalue\n      21       41       42       52 \n0.002121 0.045599 0.091198 0.861082 \n\n[[475]]\nvalue\n      21       41       42       52 \n0.003198 0.038380 0.081023 0.877399 \n\n[[476]]\nvalue\n      21       41       42       52 \n0.003202 0.041622 0.114194 0.840982 \n\n[[477]]\nvalue\n      21       41       42       52 \n0.002130 0.040469 0.105431 0.851970 \n\n[[478]]\nvalue\n      21       41       42       52 \n0.002132 0.040512 0.108742 0.848614 \n\n[[479]]\nvalue\n      21       41       42       52 \n0.001066 0.052239 0.103412 0.843284 \n\n[[480]]\nvalue\n      41       42       52 \n0.035106 0.079787 0.885106 \n\n[[481]]\nvalue\n      41       42       52 \n0.034995 0.078473 0.886532 \n\n[[482]]\nvalue\n      41       42       52       82 \n0.066950 0.061637 0.641870 0.229543 \n\n[[483]]\nvalue\n      41       42       52       82 \n0.062633 0.059448 0.638004 0.239915 \n\n[[484]]\nvalue\n      41       42       52       82 \n0.063694 0.052017 0.626327 0.257962 \n\n[[485]]\nvalue\n      41       42       52       82 \n0.064034 0.054429 0.628602 0.252935 \n\n[[486]]\nvalue\n      21       22       41       42       52       82 \n0.035032 0.004246 0.097665 0.139066 0.331210 0.392781 \n\n[[487]]\nvalue\n      21       22       41       42       52       82       90 \n0.042553 0.034043 0.012766 0.146809 0.588298 0.172340 0.003191 \n\n[[488]]\nvalue\n      21       41       42       52       82 \n0.011702 0.046809 0.060638 0.647872 0.232979 \n\n[[489]]\nvalue\n      21       41       42       52       82 \n0.011740 0.052295 0.067236 0.612593 0.256137 \n\n[[490]]\nvalue\n      21       22       41       42       52       82       90 \n0.054140 0.027601 0.049894 0.211253 0.505308 0.148620 0.003185 \n\n[[491]]\nvalue\n      21       22       41       42       52       82       90 \n0.052128 0.026596 0.052128 0.203191 0.497872 0.164894 0.003191 \n\n[[492]]\nvalue\n      21       22       41       42       52       82 \n0.039362 0.010638 0.089362 0.208511 0.303191 0.348936 \n\n[[493]]\nvalue\n      21       41       42       52       82 \n0.021368 0.092949 0.098291 0.551282 0.236111 \n\n[[494]]\nvalue\n      21       41       42       52       82 \n0.010604 0.049841 0.072110 0.602333 0.265111 \n\n[[495]]\nvalue\n      41       42       52       82 \n0.060638 0.058511 0.596809 0.284043 \n\n[[496]]\nvalue\n      41       42       52       82 \n0.041357 0.049841 0.681866 0.226935 \n\n[[497]]\nvalue\n      41       42       52       82 \n0.042508 0.046759 0.653560 0.257173 \n\n[[498]]\nvalue\n      41       42       52       82 \n0.070513 0.069444 0.631410 0.228632 \n\n[[499]]\nvalue\n      41       42       52       82 \n0.057325 0.050955 0.657113 0.234607 \n\n[[500]]\nvalue\n      41       42       52       82 \n0.050955 0.046709 0.657113 0.245223 \n\n[[501]]\nvalue\n      41       42       52       82 \n0.060574 0.051010 0.603613 0.284803 \n\n[[502]]\nvalue\n      41       42       52       82 \n0.057143 0.048677 0.606349 0.287831 \n\n[[503]]\nvalue\n      41       42       52       82 \n0.045793 0.044728 0.670927 0.238552 \n\n[[504]]\nvalue\n      41       42       52       82 \n0.042553 0.050000 0.679787 0.227660 \n\n[[505]]\nvalue\n      41       42       52       82 \n0.032051 0.052350 0.698718 0.216880 \n\n[[506]]\nvalue\n      41       42       52       82 \n0.032909 0.044586 0.683652 0.238854 \n\n[[507]]\nvalue\n      41       42       52       82 \n0.035181 0.037313 0.749467 0.178038 \n\n[[508]]\nvalue\n      21       41       42       52       82 \n0.003191 0.037234 0.028723 0.822340 0.108511 \n\n[[509]]\nvalue\n      21       41       42       52       82 \n0.004251 0.023379 0.012752 0.952179 0.007439 \n\n[[510]]\nvalue\n      21       41       42       52       82 \n0.004264 0.031983 0.019190 0.939232 0.005330 \n\n[[511]]\nvalue\n      21       41       42       52 \n0.001064 0.019149 0.035106 0.944681 \n\n[[512]]\nvalue\n      21       41       42       52 \n0.001063 0.018066 0.007439 0.973433 \n\n[[513]]\nvalue\n      21       41       42       52 \n0.001065 0.005325 0.010650 0.982961 \n\n[[514]]\nvalue\n      41       42       52 \n0.025532 0.001064 0.973404 \n\n[[515]]\nvalue\n      41       42       52 \n0.025586 0.012793 0.961620 \n\n[[516]]\nvalue\n      41       42       52 \n0.039320 0.077577 0.883103 \n\n[[517]]\nvalue\n      41       42       52 \n0.039404 0.076677 0.883919 \n\n[[518]]\nvalue\n      41       42       52 \n0.039320 0.076514 0.884166 \n\n[[519]]\nvalue\n      41       42       52 \n0.032944 0.077577 0.889479 \n\n[[520]]\nvalue\n      41       42       52 \n0.018104 0.006390 0.975506 \n\n[[521]]\nvalue\n      41       42       52 \n0.002132 0.003198 0.994670 \n\n[[522]]\nvalue\n      41       42       52 \n0.025505 0.010627 0.963868 \n\n[[523]]\nvalue\n      41       42       52 \n0.026567 0.022317 0.951116 \n\n[[524]]\nvalue\n      41       42       52       82 \n0.029756 0.094580 0.853348 0.022317 \n\n[[525]]\nvalue\n      41       42       52       82 \n0.033084 0.094984 0.834578 0.037353 \n\n[[526]]\nvalue\n      41       42       52       82 \n0.031847 0.090234 0.831210 0.046709 \n\n[[527]]\nvalue\n      21       41       42       52 \n0.003181 0.039236 0.094380 0.863203 \n\n[[528]]\nvalue\n      21       41       42       52 \n0.001064 0.102128 0.121277 0.775532 \n\n[[529]]\nvalue\n      21       41       42       52 \n0.001063 0.095643 0.123273 0.780021 \n\n[[530]]\nvalue\n      41       42       52 \n0.064687 0.237540 0.697773 \n\n[[531]]\nvalue\n      41       42       52 \n0.084043 0.167021 0.748936 \n\n[[532]]\nvalue\n      21       41       42       52 \n0.002132 0.042644 0.109808 0.845416 \n\n[[533]]\nvalue\n      41       42       52 \n0.042508 0.082891 0.874601 \n\n[[534]]\nvalue\n      41       42       52 \n0.039362 0.076596 0.884043 \n\n[[535]]\nvalue\n      41       42       52 \n0.025559 0.033014 0.941427 \n\n[[536]]\nvalue\n      41       42       52 \n0.022269 0.013786 0.963945 \n\n[[537]]\nvalue\n      41       42       52 \n0.016949 0.011653 0.971398 \n\n[[538]]\nvalue\n      41       42       52 \n0.026624 0.084132 0.889244 \n\n[[539]]\nvalue\n      21       41       42       52 \n0.001060 0.065748 0.111347 0.821845 \n\n[[540]]\nvalue\n      21       41       42       52 \n0.001063 0.075452 0.108395 0.815090 \n\n[[541]]\nvalue\n      21       41       42       52 \n0.001068 0.068376 0.103632 0.826923 \n\n[[542]]\nvalue\n      21       41       42       52 \n0.003188 0.043571 0.093518 0.859724 \n\n[[543]]\nvalue\n      21       41       42       52 \n0.002123 0.038217 0.090234 0.869427 \n\n[[544]]\nvalue\n      21       41       42       52 \n0.003198 0.076759 0.118337 0.801706 \n\n[[545]]\nvalue\n      21       41       42       52 \n0.003188 0.042508 0.096706 0.857598 \n\n[[546]]\nvalue\n      21       41       42       52 \n0.003191 0.027660 0.088298 0.880851 \n\n[[547]]\nvalue\n      41       42       52 \n0.039278 0.079618 0.881104 \n\n[[548]]\nvalue\n      41       42       52 \n0.039320 0.077577 0.883103 \n\n[[549]]\nvalue\n      41       42       52 \n0.037155 0.079618 0.883227 \n\n[[550]]\nvalue\n      21       41       42       52 \n0.001058 0.034921 0.085714 0.878307 \n\n[[551]]\nvalue\n      41       42       52 \n0.032909 0.080679 0.886412 \n\n[[552]]\nvalue\n      21       41       42       52 \n0.002128 0.028723 0.084043 0.885106 \n\n[[553]]\nvalue\n      21       41       42       52 \n0.002134 0.035219 0.085379 0.877268 \n\n[[554]]\nvalue\n      41       42       52 \n0.051962 0.079533 0.868505 \n\n[[555]]\nvalue\n      41       42       52 \n0.039404 0.078807 0.881789 \n\n[[556]]\nvalue\n      21       41       42       52 \n0.001066 0.083156 0.116205 0.799574 \n\n[[557]]\nvalue\n      21       41       42       52 \n0.003195 0.077742 0.118211 0.800852 \n\n[[558]]\nvalue\n      21       41       42       52 \n0.002137 0.049145 0.107906 0.840812 \n\n[[559]]\nvalue\n      21       41       42       52 \n0.002130 0.059638 0.096912 0.841321 \n\n[[560]]\nvalue\n      21       41       42       52 \n0.002128 0.079787 0.114894 0.803191 \n\n[[561]]\nvalue\n      41       42       52 \n0.029819 0.006390 0.963791 \n\n[[562]]\nvalue\n      41       52 \n0.019068 0.980932 \n\n[[563]]\nvalue\n      21       41       42       52 \n0.001060 0.029692 0.042418 0.926829 \n\n[[564]]\nvalue\n      21       41       42       52 \n0.004260 0.027689 0.035144 0.932907 \n\n[[565]]\nvalue\n      21       41       42       52 \n0.004269 0.027748 0.034152 0.933831 \n\n[[566]]\nvalue\n      21       41       42       52       82 \n0.004255 0.034043 0.031915 0.920213 0.009574 \n\n[[567]]\nvalue\n      41       42       52       82 \n0.037313 0.069296 0.792111 0.101279 \n\n[[568]]\nvalue\n      41       42       52       82 \n0.036170 0.070213 0.826596 0.067021 \n\n[[569]]\nvalue\n      41       42       52       82 \n0.022340 0.071277 0.832979 0.073404 \n\n[[570]]\nvalue\n      41       42       52       82 \n0.034043 0.090426 0.830851 0.044681 \n\n[[571]]\nvalue\n      41       42       52       82 \n0.028815 0.085379 0.813234 0.072572 \n\n[[572]]\nvalue\n      41       42       52       82 \n0.035106 0.098936 0.837234 0.028723 \n\n[[573]]\nvalue\n      41       42       52       82 \n0.034995 0.098621 0.837752 0.028632 \n\n[[574]]\nvalue\n      41       42       52 \n0.041445 0.081828 0.876727 \n\n[[575]]\nvalue\n      41       42       52 \n0.039320 0.076514 0.884166 \n\n[[576]]\nvalue\n      41       42       52 \n0.010661 0.005330 0.984009 \n\n[[577]]\nvalue\n      41       42       52 \n0.025424 0.004237 0.970339 \n\n[[578]]\nvalue\n      41       42       52 \n0.025505 0.011690 0.962806 \n\n[[579]]\nvalue\n      41       42       52 \n0.044824 0.082177 0.872999 \n\n[[580]]\nvalue\n      41       42       52 \n0.044824 0.082177 0.872999 \n\n[[581]]\nvalue\n      41       42       52 \n0.037274 0.079872 0.882854 \n\n[[582]]\nvalue\n      41       42       52 \n0.042373 0.077331 0.880297 \n\n[[583]]\nvalue\n      21       41       42       52 \n0.003185 0.040340 0.098726 0.857749 \n\n[[584]]\nvalue\n      21       41       42       52 \n0.003195 0.062833 0.132055 0.801917 \n\n[[585]]\nvalue\n      21       41       42       52 \n0.003198 0.067164 0.128998 0.800640 \n\n[[586]]\nvalue\n      21       41       42       52 \n0.002121 0.050901 0.106045 0.840933 \n\n[[587]]\nvalue\n      21       41       42       52 \n0.003191 0.039362 0.109574 0.847872 \n\n[[588]]\nvalue\n      21       41       42       52 \n0.003188 0.040383 0.122210 0.834219 \n\n[[589]]\nvalue\n      21       41       42       52 \n0.002134 0.049093 0.102455 0.846318 \n\n[[590]]\nvalue\n      21       41       42       52 \n0.001066 0.060768 0.083156 0.855011 \n\n[[591]]\nvalue\n      41       42       52 \n0.043571 0.078640 0.877790 \n\n[[592]]\nvalue\n      21       41       42       52 \n0.003188 0.034006 0.099894 0.862912 \n\n[[593]]\nvalue\n      21       41       42       52 \n0.003198 0.037313 0.102345 0.857143 \n\n[[594]]\nvalue\n      21       41       42       52 \n0.001067 0.039488 0.066169 0.893276 \n\n[[595]]\nvalue\n      41       42       52 \n0.043524 0.080679 0.875796 \n\n[[596]]\nvalue\n      41       42       52 \n0.045891 0.082177 0.871932 \n\n[[597]]\nvalue\n      41       42       52 \n0.039278 0.079618 0.881104 \n\n[[598]]\nvalue\n      41       42       52 \n0.042553 0.082979 0.874468 \n\n[[599]]\nvalue\n      41       42       52 \n0.032874 0.043478 0.923648 \n\n[[600]]\nvalue\n      21       41       42       52 \n0.001064 0.098936 0.096809 0.803191 \n\n[[601]]\nvalue\n      21       41       42       52 \n0.002128 0.097872 0.096809 0.803191 \n\n[[602]]\nvalue\n      21       41       42       52 \n0.003191 0.081915 0.106383 0.808511 \n\n[[603]]\nvalue\n      21       41       42       52 \n0.003191 0.074468 0.100000 0.822340 \n\n[[604]]\nvalue\n      21       41       42       52 \n0.003188 0.072264 0.104145 0.820404 \n\n[[605]]\nvalue\n      21       41       42       52 \n0.003195 0.059638 0.106496 0.830671 \n\n[[606]]\nvalue\n      21       41       42       52 \n0.003191 0.088298 0.114894 0.793617 \n\n[[607]]\nvalue\n      21       41       42       52 \n0.001067 0.104589 0.127001 0.767343 \n\n[[608]]\nvalue\n      41       42       52 \n0.008466 0.386243 0.605291 \n\n[[609]]\nvalue\n      41       42       52 \n0.008511 0.546809 0.444681 \n\n[[610]]\nvalue\n      41       42       52 \n0.002130 0.613419 0.384452 \n\n[[611]]\nvalue\n      42       52 \n0.592316 0.407684 \n\n[[612]]\nvalue\n      41       42       52 \n0.017076 0.519744 0.463180 \n\n[[613]]\nvalue\n      41       42       52 \n0.005297 0.474576 0.520127 \n\n[[614]]\nvalue\n      41       42       52 \n0.008475 0.441737 0.549788 \n\n[[615]]\nvalue\n      41       42       52 \n0.035069 0.397450 0.567481 \n\n[[616]]\nvalue\n      21       41       42       52 \n0.001062 0.085987 0.131635 0.781316 \n\n[[617]]\nvalue\n      21       41       42       52 \n0.001067 0.104589 0.121665 0.772679 \n\n[[618]]\nvalue\n      41       42       52 \n0.039278 0.077495 0.883227 \n\n[[619]]\nvalue\n      41       42       52 \n0.039195 0.077331 0.883475 \n\n[[620]]\nvalue\n      41       42       52 \n0.035106 0.077660 0.887234 \n\n[[621]]\nvalue\n      41       42       52 \n0.032909 0.080679 0.886412 \n\n[[622]]\nvalue\n      21       41       42       52 \n0.003191 0.028723 0.101064 0.867021 \n\n[[623]]\nvalue\n      21       41       42       52 \n0.001064 0.047872 0.087234 0.863830 \n\n[[624]]\nvalue\n      41       42       52 \n0.006376 0.075452 0.918172 \n\n[[625]]\nvalue\n      41       42       52 \n0.005308 0.058386 0.936306 \n\n[[626]]\nvalue\n      41       42       52 \n0.017003 0.160468 0.822529 \n\n[[627]]\nvalue\n      41       42       52 \n0.011690 0.167906 0.820404 \n\n[[628]]\nvalue\n      41       42       52 \n0.009564 0.163656 0.826780 \n\n[[629]]\nvalue\n      41       42       52 \n0.015924 0.195329 0.788747 \n\n[[630]]\nvalue\n      41       42       52 \n0.037313 0.223881 0.738806 \n\n[[631]]\nvalue\n      41       42       52 \n0.053305 0.194030 0.752665 \n\n[[632]]\nvalue\n      41       42       52 \n0.049093 0.140875 0.810032 \n\n[[633]]\nvalue\n      21       41       42       52 \n0.002128 0.078723 0.119149 0.800000 \n\n[[634]]\nvalue\n      21       41       42       52 \n0.001066 0.102345 0.118337 0.778252 \n\n[[635]]\nvalue\n      21       41       42       52 \n0.001066 0.082090 0.135394 0.781450 \n\n[[636]]\nvalue\n      41       42       52 \n0.017094 0.134615 0.848291 \n\n[[637]]\nvalue\n      41       42       52 \n0.008484 0.125133 0.866384 \n\n[[638]]\nvalue\n      41       42       52 \n0.007447 0.108511 0.884043 \n\n[[639]]\nvalue\n      21       41       42       52 \n0.001062 0.052017 0.123142 0.823779 \n\n[[640]]\nvalue\n      21       41       42       52 \n0.003205 0.057692 0.136752 0.802350 \n\n[[641]]\nvalue\n      21       41       42       52 \n0.001063 0.057386 0.089267 0.852285 \n\n[[642]]\nvalue\n      41       42       52       82 \n0.021254 0.064825 0.837407 0.076514 \n\n[[643]]\nvalue\n      41       42       52       82 \n0.034152 0.049093 0.738527 0.178228 \n\n[[644]]\nvalue\n      41       42       52       82 \n0.033120 0.050214 0.727564 0.189103 \n\n[[645]]\nvalue\n      21       41       42       52       82 \n0.002134 0.037353 0.037353 0.813234 0.109925 \n\n[[646]]\nvalue\n      21       41       42       52       82 \n0.003185 0.039278 0.039278 0.830149 0.088110 \n\n[[647]]\nvalue\n      41       42       52       82 \n0.050955 0.043524 0.619958 0.285563 \n\n[[648]]\nvalue\n      41       42       52 \n0.027689 0.060703 0.911608 \n\n[[649]]\nvalue\n      41       42       52 \n0.031746 0.053968 0.914286 \n\n[[650]]\nvalue\n      41       52 \n0.022293 0.977707 \n\n[[651]]\nvalue\n      41       42       52 \n0.009574 0.091489 0.898936 \n\n[[652]]\nvalue\n     41      42      52 \n0.01174 0.25080 0.73746 \n\n[[653]]\nvalue\n      41       42       52 \n0.013874 0.304162 0.681964 \n\n[[654]]\nvalue\n      41       42       52 \n0.013874 0.293490 0.692636 \n\n[[655]]\nvalue\n      41       42       52 \n0.019108 0.360934 0.619958 \n\n[[656]]\nvalue\n      41       42       52 \n0.009595 0.410448 0.579957 \n\n[[657]]\nvalue\n      41       42       52 \n0.009554 0.409766 0.580679 \n\n[[658]]\nvalue\n      41       42       52 \n0.019129 0.496281 0.484591 \n\n[[659]]\nvalue\n      41       42       52 \n0.045696 0.582359 0.371945 \n\n[[660]]\nvalue\n      41       42       52 \n0.044397 0.582452 0.373150 \n\n[[661]]\nvalue\n      41       42       52 \n0.045940 0.600427 0.353632 \n\n[[662]]\nvalue\n      41       42       52 \n0.003188 0.071201 0.925611 \n\n[[663]]\nvalue\n      41       42       52 \n0.002125 0.071201 0.926674 \n\n[[664]]\nvalue\n      41       42       52 \n0.003205 0.070513 0.926282 \n\n[[665]]\nvalue\n      41       42       52 \n0.002123 0.063694 0.934183 \n\n[[666]]\nvalue\n      41       42       52 \n0.002137 0.066239 0.931624 \n\n[[667]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[668]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[669]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[670]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[671]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[672]]\nvalue\n      41       42       52 \n0.002134 0.072572 0.925293 \n\n[[673]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[674]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[675]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[676]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[677]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[678]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[679]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[680]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[681]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[682]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[683]]\nvalue\n      41       42       52 \n0.002132 0.068230 0.929638 \n\n[[684]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[685]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[686]]\nvalue\n      41       42       52 \n0.002132 0.068230 0.929638 \n\n[[687]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[688]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[689]]\nvalue\n      41       42       52 \n0.003198 0.066098 0.930704 \n\n[[690]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[691]]\nvalue\n      41       42       52 \n0.002121 0.068929 0.928950 \n\n[[692]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[693]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[694]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[695]]\nvalue\n      41       42       52 \n0.003195 0.069223 0.927583 \n\n[[696]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[697]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[698]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[699]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[700]]\nvalue\n      41       42       52 \n0.002125 0.068013 0.929862 \n\n[[701]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[702]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[703]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[704]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[705]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[706]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[707]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[708]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[709]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[710]]\nvalue\n      41       42       52 \n0.003202 0.067236 0.929562 \n\n[[711]]\nvalue\n      41       42       52 \n0.002130 0.067093 0.930777 \n\n[[712]]\nvalue\n      41       42       52 \n0.003195 0.069223 0.927583 \n\n[[713]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[714]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[715]]\nvalue\n      41       42       52 \n0.002130 0.067093 0.930777 \n\n[[716]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[717]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[718]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[719]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[720]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[721]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[722]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[723]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[724]]\nvalue\n      41       42       52 \n0.002121 0.067869 0.930011 \n\n[[725]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[726]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[727]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[728]]\nvalue\n      41       42       52 \n0.003181 0.068929 0.927890 \n\n[[729]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[730]]\nvalue\n      41       42       52 \n0.002132 0.068230 0.929638 \n\n[[731]]\nvalue\n      41       42       52 \n0.002134 0.068303 0.929562 \n\n[[732]]\nvalue\n      41       42       52 \n0.002123 0.067941 0.929936 \n\n[[733]]\nvalue\n      41       42       52 \n0.003198 0.069296 0.927505 \n\n[[734]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[735]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[736]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[737]]\nvalue\n      41       42       52 \n0.003181 0.068929 0.927890 \n\n[[738]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[739]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[740]]\nvalue\n      41       42       52 \n0.002121 0.068929 0.928950 \n\n[[741]]\nvalue\n      41       42       52 \n0.003188 0.068013 0.928799 \n\n[[742]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[743]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[744]]\nvalue\n      41       42       52 \n0.002125 0.068013 0.929862 \n\n[[745]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[746]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[747]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[748]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[749]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[750]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[751]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[752]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[753]]\nvalue\n      41       42       52 \n0.003191 0.069149 0.927660 \n\n[[754]]\nvalue\n      41       42       52 \n0.002123 0.067941 0.929936 \n\n[[755]]\nvalue\n      41       42       52 \n0.002125 0.070138 0.927736 \n\n[[756]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[757]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[758]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[759]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[760]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[761]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[762]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[763]]\nvalue\n      41       42       52 \n0.002130 0.069223 0.928647 \n\n[[764]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[765]]\nvalue\n      41       42       52 \n0.003171 0.068710 0.928118 \n\n[[766]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[767]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[768]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[769]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[770]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[771]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[772]]\nvalue\n      41       42       52 \n0.002125 0.068013 0.929862 \n\n[[773]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[774]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[775]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[776]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[777]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[778]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[779]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[780]]\nvalue\n      41       42       52 \n0.002121 0.068929 0.928950 \n\n[[781]]\nvalue\n      41       42       52 \n0.003188 0.068013 0.928799 \n\n[[782]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[783]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[784]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[785]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[786]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[787]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[788]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[789]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[790]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[791]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[792]]\nvalue\n      41       42       52 \n0.002121 0.068929 0.928950 \n\n[[793]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[794]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[795]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[796]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[797]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[798]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[799]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[800]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[801]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[802]]\nvalue\n      41       42       52 \n0.003191 0.069149 0.927660 \n\n[[803]]\nvalue\n      41       42       52 \n0.002121 0.068929 0.928950 \n\n[[804]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[805]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[806]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[807]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[808]]\nvalue\n      41       42       52 \n0.003171 0.068710 0.928118 \n\n[[809]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[810]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[811]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[812]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[813]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[814]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[815]]\nvalue\n      41       42       52 \n0.002121 0.068929 0.928950 \n\n[[816]]\nvalue\n      41       42       52 \n0.002125 0.068013 0.929862 \n\n[[817]]\nvalue\n      41       42       52 \n0.003188 0.069075 0.927736 \n\n[[818]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[819]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[820]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[821]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[822]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[823]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[824]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[825]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[826]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[827]]\nvalue\n      41       42       52 \n0.002123 0.070064 0.927813 \n\n[[828]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[829]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[830]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[831]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[832]]\nvalue\n      41       42       52 \n0.002123 0.067941 0.929936 \n\n[[833]]\nvalue\n      41       42       52 \n0.003171 0.068710 0.928118 \n\n[[834]]\nvalue\n      41       42       52 \n0.002132 0.066098 0.931770 \n\n[[835]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[836]]\nvalue\n      41       42       52 \n0.002134 0.066169 0.931697 \n\n[[837]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[838]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[839]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[840]]\nvalue\n      41       42       52 \n0.003198 0.069296 0.927505 \n\n[[841]]\nvalue\n      41       42       52 \n0.002121 0.068929 0.928950 \n\n[[842]]\nvalue\n      41       42       52 \n0.003188 0.068013 0.928799 \n\n[[843]]\nvalue\n      41       42       52 \n0.002125 0.070138 0.927736 \n\n[[844]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[845]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[846]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[847]]\nvalue\n      41       42       52 \n0.002134 0.070438 0.927428 \n\n[[848]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[849]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[850]]\nvalue\n      41       42       52 \n0.002132 0.068230 0.929638 \n\n[[851]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[852]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[853]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[854]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[855]]\nvalue\n      41       42       52 \n0.002132 0.068230 0.929638 \n\n[[856]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[857]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[858]]\nvalue\n      41       42       52 \n0.002121 0.068929 0.928950 \n\n[[859]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[860]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[861]]\nvalue\n      41       42       52 \n0.002125 0.068013 0.929862 \n\n[[862]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[863]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[864]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[865]]\nvalue\n      41       42       52 \n0.003202 0.067236 0.929562 \n\n[[866]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[867]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[868]]\nvalue\n      41       42       52 \n0.002121 0.068929 0.928950 \n\n[[869]]\nvalue\n      41       42       52 \n0.003198 0.070362 0.926439 \n\n[[870]]\nvalue\n      41       42       52 \n0.002134 0.068303 0.929562 \n\n[[871]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[872]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[873]]\nvalue\n      41       42       52 \n0.003185 0.067941 0.928875 \n\n[[874]]\nvalue\n      41       42       52 \n0.003198 0.069296 0.927505 \n\n[[875]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[876]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[877]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[878]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[879]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[880]]\nvalue\n      41       42       52 \n0.002130 0.069223 0.928647 \n\n[[881]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[882]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[883]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[884]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[885]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[886]]\nvalue\n      41       42       52 \n0.002125 0.068013 0.929862 \n\n[[887]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[888]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[889]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[890]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[891]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[892]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[893]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[894]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[895]]\nvalue\n      41       42       52 \n0.003195 0.070288 0.926518 \n\n[[896]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[897]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[898]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[899]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[900]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[901]]\nvalue\n      41       42       52 \n0.002130 0.067093 0.930777 \n\n[[902]]\nvalue\n      41       42       52 \n0.002134 0.067236 0.930630 \n\n[[903]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[904]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[905]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[906]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[907]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[908]]\nvalue\n      41       42       52 \n0.003191 0.069149 0.927660 \n\n[[909]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[910]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[911]]\nvalue\n      41       42       52 \n0.002132 0.068230 0.929638 \n\n[[912]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[913]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[914]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[915]]\nvalue\n      41       42       52 \n0.003188 0.068013 0.928799 \n\n[[916]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[917]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[918]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[919]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[920]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[921]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[922]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[923]]\nvalue\n      41       42       52 \n0.003185 0.067941 0.928875 \n\n[[924]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[925]]\nvalue\n      41       42       52 \n0.002132 0.067164 0.930704 \n\n[[926]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[927]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[928]]\nvalue\n      41       42       52 \n0.003188 0.068013 0.928799 \n\n[[929]]\nvalue\n      41       42       52 \n0.003195 0.068158 0.928647 \n\n[[930]]\nvalue\n      41       42       52 \n0.002132 0.068230 0.929638 \n\n[[931]]\nvalue\n      41       42       52 \n0.003188 0.068013 0.928799 \n\n[[932]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[933]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\n[[934]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[935]]\nvalue\n      41       42       52 \n0.003188 0.069075 0.927736 \n\n[[936]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[937]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[938]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[939]]\nvalue\n      41       42       52 \n0.003198 0.068230 0.928571 \n\n[[940]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[941]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[942]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[943]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[944]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[945]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[946]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[947]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[948]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[949]]\nvalue\n      41       42       52 \n0.002132 0.068230 0.929638 \n\n[[950]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[951]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[952]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[953]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[954]]\nvalue\n      41       42       52 \n0.002130 0.068158 0.929712 \n\n[[955]]\nvalue\n      41       42       52 \n0.002128 0.068085 0.929787 \n\n[[956]]\nvalue\n      41       42       52 \n0.003198 0.068230 0.928571 \n\n[[957]]\nvalue\n      41       42       52 \n0.003178 0.068856 0.927966 \n\n[[958]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[959]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[960]]\nvalue\n      41       42       52 \n0.002128 0.069149 0.928723 \n\n[[961]]\nvalue\n      41       42       52 \n0.002121 0.068929 0.928950 \n\n[[962]]\nvalue\n      41       42       52 \n0.002125 0.069075 0.928799 \n\n[[963]]\nvalue\n      41       42       52 \n0.002123 0.069002 0.928875 \n\n[[964]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[965]]\nvalue\n      41       42       52 \n0.003191 0.070213 0.926596 \n\n[[966]]\nvalue\n      41       42       52 \n0.003185 0.069002 0.927813 \n\n[[967]]\nvalue\n      41       42       52 \n0.003175 0.068783 0.928042 \n\n[[968]]\nvalue\n      41       42       52 \n0.003191 0.068085 0.928723 \n\nM &lt;- coredata(do.call(cbind, lapply(prop, zoo)))\ncolnames(M) &lt;- NULL\n#Transpose matrix so land cover become separate columns of data\nmatrix &lt;- t(M)\n#Now convert the matrix to a data frame so it is easier to manipulate\ndfland &lt;- as.data.frame(matrix)\n\n#Assing column names to land cover\ncolnames(dfland) &lt;- c(\"21\",\"22\",\"41\",\"42\",\"52\",\"71\",\"82\")\nhead(dfland)\n\n        21       22       41       42       52       71 82\n1 0.061637 0.058448 0.602550 0.277365       NA       NA NA\n2 0.014878 0.020191 0.964931       NA       NA       NA NA\n3 0.032979 0.075532 0.891489       NA       NA       NA NA\n4 0.002130 0.047923 0.102236 0.847710       NA       NA NA\n5 0.067308 0.198718 0.733974       NA       NA       NA NA\n6 0.004255 0.028723 0.043617 0.050000 0.714894 0.158511 NA\n\n#Cell size of raster layer\nres(nlcd)\n\n[1] 30 30\n\n# 30^2\n# 900*37\n# (900*37)/1000000"
  },
  {
    "objectID": "MidAtlantic.html",
    "href": "MidAtlantic.html",
    "title": "\n11  Mid-Atlantic layers using online sources\n",
    "section": "",
    "text": "1. Open the script “MidAtlantic.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(sf)\nlibrary(terra)\nlibrary(FedData)\nsessionInfo()#be sure version 3.0 is installed\n\nR version 4.3.2 (2023-10-31)\nPlatform: x86_64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] FedData_4.0.1 terra_1.7-46  sf_1.0-14    \n\nloaded via a namespace (and not attached):\n [1] jsonlite_1.8.7     dplyr_1.1.3        compiler_4.3.2     tidyselect_1.2.0  \n [5] Rcpp_1.0.11        yaml_2.3.7         fastmap_1.1.1      readr_2.1.4       \n [9] R6_2.5.1           generics_0.1.3     classInt_0.4-10    knitr_1.42        \n[13] htmlwidgets_1.6.2  tibble_3.2.1       units_0.8-4        DBI_1.1.3         \n[17] tzdb_0.4.0         pillar_1.9.0       rlang_1.1.1        utf8_1.2.3        \n[21] xfun_0.39          cli_3.6.1          magrittr_2.0.3     class_7.3-22      \n[25] digest_0.6.31      grid_4.3.2         rstudioapi_0.14    hms_1.1.3         \n[29] lifecycle_1.0.3    vctrs_0.6.3        KernSmooth_2.23-22 proxy_0.4-27      \n[33] evaluate_0.21      glue_1.6.2         codetools_0.2-19   fansi_1.0.4       \n[37] e1071_1.7-13       rmarkdown_2.21     tools_4.3.2        pkgconfig_2.0.3   \n[41] htmltools_0.5.5   \n\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise\n\nll.crs=st_crs(4269)\nutm.crs &lt;- st_crs(9001) \nalbers.crs &lt;- st_crs(5070) #CRS of shapefile layers #crs &lt;- CRS(\"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0  #  +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0\") #CRS of raster layers #crs2 &lt;- CRS(\"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0  #  +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\") \n\n4. We will use the tigris package to downloaded statewide layers for state and county outlines\n\nst &lt;- tigris::states() %&gt;%   dplyr::filter(GEOID &lt; \"60\") %&gt;%    tigris::shift_geometry() #GEOID's above 60 are territories and islands, etc. So I'm removing them for scaling \n\nRetrieving data for the year 2021\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |======================================================================| 100%\n\nplot(st_geometry(st)) \n\n\n\nst &lt;- st_transform(st, albers.crs) \nPA.outline &lt;- subset(st, st$NAME == \"Pennsylvania\")  \n#PAcounties &lt;- counties(\"Pennsylvania\", cb = TRUE) \n#PAcounties &lt;- st_transform(PAcounties, albers.crs)\n#plot(st_geometry(PAcounties))\n#We will also get the outline for Maryland to learn to combine rasters\nMD.outline &lt;- subset(st, st$NAME == \"Maryland\")\n\n5. Here we are going to explore a new method to download NLCD or Digital Elevation Data from online within a polygon from your study site.\n\nPAnlcd &lt;- get_nlcd(template=PA.outline, label = 'Pennsylvania',force.redo = T)\nMDnlcd &lt;- get_nlcd(template=MD.outline, label = 'Maryland',force.redo = T)\nnlcd &lt;- mosaic(PAnlcd, MDnlcd, fun=min)\nplot(nlcd)\n\n\n\n#writeRaster(nlcd,\"mosaic_nlcd.tif\",format=\"GTiff\",datatype = 'INT4U',overwrite=T)\n\n#nlcd &lt;- raster(\"mosaic_nlcd.tif\")\n# PAstate &lt;- spTransform(PAstate, CRS=proj4string(PAnlcd))\n# OHstate &lt;- spTransform(OHstate, CRS=proj4string(OHnlcd))\n# plot(PAstate, add=T);plot(OHstate, add=T)\n\nPADEM &lt;- get_ned(template=PA.outline, label = 'PAdem',force.redo = T)\n\nArea of interest includes 28 NED tiles.\n\n\n(Down)Loading NED tile for 40N and 75W.\n\n\n(Down)Loading NED tile for 41N and 75W.\n\n\n(Down)Loading NED tile for 42N and 75W.\n\n\n(Down)Loading NED tile for 43N and 75W.\n\n\n(Down)Loading NED tile for 40N and 76W.\n\n\n(Down)Loading NED tile for 41N and 76W.\n\n\n(Down)Loading NED tile for 42N and 76W.\n\n\n(Down)Loading NED tile for 43N and 76W.\n\n\n(Down)Loading NED tile for 40N and 77W.\n\n\n(Down)Loading NED tile for 41N and 77W.\n\n\n(Down)Loading NED tile for 42N and 77W.\n\n\n(Down)Loading NED tile for 43N and 77W.\n\n\n(Down)Loading NED tile for 40N and 78W.\n\n\n(Down)Loading NED tile for 41N and 78W.\n\n\n(Down)Loading NED tile for 42N and 78W.\n\n\n(Down)Loading NED tile for 43N and 78W.\n\n\n(Down)Loading NED tile for 40N and 79W.\n\n\n(Down)Loading NED tile for 41N and 79W.\n\n\n(Down)Loading NED tile for 42N and 79W.\n\n\n(Down)Loading NED tile for 43N and 79W.\n\n\n(Down)Loading NED tile for 40N and 80W.\n\n\n(Down)Loading NED tile for 41N and 80W.\n\n\n(Down)Loading NED tile for 42N and 80W.\n\n\n(Down)Loading NED tile for 43N and 80W.\n\n\n(Down)Loading NED tile for 40N and 81W.\n\n\n(Down)Loading NED tile for 41N and 81W.\n\n\n(Down)Loading NED tile for 42N and 81W.\n\n\n(Down)Loading NED tile for 43N and 81W.\n\n\nMosaicking NED tiles.\n\nPADEM.proj &lt;- project(PADEM,PAnlcd,method='near')\n#writeRaster(PADEM.proj,\"PA_elev.tif\",format=\"GTiff\",datatype = 'INT4U',overwrite=T)\n\n#Or read in raster from above\n#PADEM.proj &lt;- raster(\"PA_elev.tif\")\nplot(PADEM.proj)\nplot(st_geometry(PA.outline), add = TRUE)\n\n\n\n\n\nDMA2 &lt;- readOGR(dsn=\".\",layer=\"PGC_CWDManagementUnits2020\",verbose=FALSE)\nDMA2.proj &lt;- spTransform(DMA2, CRS=proj4string(PAnlcd))\nDMA_DAYMET &lt;- get_daymet(template=DMA2.proj,label = 'PAdaymet',elements = c('prcp','tmin','tmax'),\n                     years = 2010:2011)\nplot(DMA_DAYMET$tmin$X2011.05.23)\nplot(DMA_DAYMET$tmin$X2011.06.23)\nplot(DMA_DAYMET$tmin$X2011.10.23)\nplot(DMA_DAYMET$tmin$X2012.05.23)\nplot(DMA_DAYMET$prcp$X2011.05.23)\n\nGHCN.prcp &lt;- get_ghcn_daily(template=DMA2, label='GHCNprecip', elements=c('prcp'))\nplot(DMA2)\nplot(GHCN.prcp$spatial, pch=1)\nlegend('topright', pch=1, legend='GHCN Precipitation Records')\n\n\nDMAsoil &lt;- get_ssurgo(template=PAstate, label = 'DMA_SSURGO',force.redo = T) \n\n\nNHD &lt;- get_nhd(template=PAstate, label='PAnhd')%&gt;%\n  plot_nhd(template = PAstate)\nget_nhd(template=PAstate, label='PAnhd')%&gt;%\n  plot_nhd(template = PAstate)\nplot(DMA2.proj)\nplot(NHD$NHDFlowline, add=T)\nplot(NHD$NHDLine, add=T)\nplot(NHD$NHDArea, col='black', add=T)\nplot(NHD$NHDWaterbody, col='black', add=T)"
  },
  {
    "objectID": "ReadFilesScript.html#cleaning-raw-climate-data",
    "href": "ReadFilesScript.html#cleaning-raw-climate-data",
    "title": "\n12  Climate Data Interpretation\n",
    "section": "\n12.1 Cleaning Raw Climate Data",
    "text": "12.1 Cleaning Raw Climate Data\n1. Open the script “Read_FilesScript.Rmd” and run code directly from the script\n2.. No packages are needed for this exercise, these are base R functions\n5. For each csv file, save as Excel Worsheet 1997-2003 if importing to NCSS or keep in csv or txt if using R.\n6. Take out all non-Jan months for every year\n7. Files will need to meet the following criteria but will be addressed in Exercise 2.4: Each weather station must have records for at least 10 of the 11 Januaries. Each weather station must have at least 95% of daily records for those Januaries. This means at least 325 days for 11 seasons and 295 days for 10 seasons.\nSnow Depth (SNWD) 66 stations Maximum temp (TMAX) 69 stations Minimum temp (TMIN) 68 stations\n8. The code that follows should have all files in the same folder but not the R script or any R files or code will not run. The code below brings in each text file and summarizes the data for each weather station as instructed in the code.\n\n#setwd(\"/Users/wdw12/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Walter-Datasets/Manual of Applied Spatial Ecology/Exercise_2.3_CleaningRawData\")\nsetwd(\"/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual of Applied Spatial Ecology/data/Exercise_2.3_CleaningRawData\")\n# Vector of files names in working directory\nfiles &lt;- list.files(pattern = \".txt\")\n\n# Total number of files in working directory (for loop below)\nn.files &lt;- length(files)\n\n# Container to hold text files\nfiles.list &lt;- list()\n\n#Populate the container files.list with climate data sets\nfiles.list &lt;- lapply(files, read.table, header =T, sep=\"\\t\") \n\n#Set up matrix for weather station summary data\nm1 &lt;- matrix(NA,ncol=8,nrow=n.files)\n\n#Loop for running through all weather station files\nfor(i in 1:n.files){\n      \n    # Assign elevation\n        m1[i,1] &lt;- files.list[[i]][1,10]\n\n    #Assign Lat\n        m1[i,2] &lt;- files.list[[i]][1,11]\n\n    #Assign Long\n        m1[i,3] &lt;- files.list[[i]][1,12]\n\n    #Calculate mean snow depth\n        SNWD_mm &lt;- mean(files.list[[i]][,7],na.rm=T)\n\n    #Convert snow depth mean to inches\n    SNWD_in &lt;- SNWD_mm/25.4\n\n    #Assign snow depth\n    m1[i,4] &lt;- SNWD_in\n\n    #Calculate mean maximum temp\n        TMAX_C &lt;- mean(files.list[[i]][,8],na.rm=T)\n\n    #Convert max temp to F\n    TMAX_F &lt;- TMAX_C*0.18 + 32\n    \n    #Assign max temp\n    m1[i,5] &lt;- TMAX_F\n\n    #Calculate mean minimum temp\n    TMIN_C &lt;- mean(files.list[[i]][,9],na.rm=T)\n\n    #Convert min temp to F\n    TMIN_F &lt;- TMIN_C*0.18 + 32\n\n    #Assign min temp\n    m1[i,6] &lt;- TMIN_F\n\n    #Reassign GHCN number\n    GHCN &lt;- toString(files.list[[i]][1,1])\n\n    #Assign Station Name\n    m1[i,7] &lt;- GHCN\n\n    #Reassign Station Name\n    SN &lt;- toString(files.list[[i]][1,2])\n\n    #Assign Station Name\n    m1[i,8] &lt;- SN\n}\n\ncolnames(m1) &lt;- c(\"Elevation\",\"Lat\",\"Long\",\"SNWD\",\"TMAX\",\"TMIN\",\"GHCN\",\"Station\")\n#write.csv(m1,paste(\".\",\"\\\\output.csv\",sep=\"\"))\n\n#Removes quotation marks in output table\nm1 &lt;-noquote(m1)\nm1[1:5,]\n\n\n#setwd(\"/Users/wdw12/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Walter-Datasets/Manual of Applied Spatial Ecology\")\n#setwd(\"/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Walter-Datasets/Manual of Applied Spatial Ecology/Exercise_2.3_CleaningRawData\")"
  },
  {
    "objectID": "Weather-Station-Code_Snowfall.html",
    "href": "Weather-Station-Code_Snowfall.html",
    "title": "\n13  Using Data in R\n",
    "section": "",
    "text": "This code is designed to process data downloaded from Climate Date Online. http://www.ncdc.noaa.gov/cdo-web/ This version looks at weather stations that provide snowfall data in and around Pennsylvania. The code pulls out the desired data from the downloaded aggregate weather-station file and calculates the mean annual snowfall per weather station for 12/1/94 - 3/31/05. The data is then exported to a text file for interpolation in ArcGIS. - Bill Kanapaux, PA Cooperative Fish & Wildlife Research Unit\nLast modified: Jan. 13, 2014\n1. Open the script “Weather-Station-Code_Snowfall.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatHR)\nlibrary(gstat)\nlibrary(plyr)\nlibrary(sf)\nlibrary(tigris)\nlibrary(tmap)\nlibrary(terra)\nlibrary(sfheaders)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs=st_crs(4269) \nutm.crs &lt;- st_crs(9001) \nalbers.crs &lt;- st_crs(5070)\n\n4. This code is designed to process data downloaded from NOAA Date. This version looks at weather stations that provide snowfall data in and around Pennsylvania. The code pulls out the desired data from the downloaded aggregate weather-station file and calculates the mean annual snowfall per weather station for 12/1/94 - 3/31/05. The data is then exported to a text file for interpolation in R or ArcGIS. First, we read weather station data - note the use of stringsAsFactors=FALSE and na.strings=‘-9999’\n\nWS &lt;-read.table(\"data/Weather_Station_Data-Sep_01Dec1994-31March2005.txt\", \n  stringsAsFactors=FALSE, na.strings='-9999',header=T)\n\n\n#Check data\ndim(WS)\nhead(WS)\nsummary (WS)\n\n\n#Reformat DATE and create Year Month Day columns from NewDate column ###\nWS$NewDate &lt;- as.Date(as.character(WS$DATE), format(\"%Y%m%d\"))\nWS$Year = as.numeric(format(WS$NewDate, format = \"%Y\"))\nWS$Month = as.numeric(format(WS$NewDate, format = \"%m\"))\nWS$Day = as.numeric(format(WS$NewDate, format = \"%d\"))\n\n5. Make a subset of WS that includes only the months of Dec-March with further manipulation of the data for desired output of project objectives.\n\nWinter &lt;- WS[WS$Month %in% c(1,2,3,12), ]\n\n#For December, add 1 to Year so that Year matches Jan-March in that season ###\nWinter &lt;- within(Winter, Year[Month==12] &lt;- Year[Month==12] +1)\n\n\n#Check subset, including random row to make sure only selected months included ###\ndim(Winter)\nhead(Winter)\nWinter[699,]\n\n\n#Create a matrix of unique STATION values (GHCND ) with Lat/Long values for later reference.\n### Data contains some multiple versions of individual GHCND coordinates. Only want 1 set per.\nPulledCoords &lt;- Winter[!duplicated(Winter[,1]),]\nCoordChart &lt;- ddply(PulledCoords, c('STATION'), function(x) c(Lat=x$LATITUDE, Long=x$LONGITUDE))\n\n#Get the number of snowfall records for each STATION for each year and name it RecordTotal.\n#Note that NA is omitted from the length count\nWinterRecords &lt;- ddply(Winter, .(STATION,Year), summarize, RecordTotal = length(na.omit(SNOW)))\n\n#Get the total amount of snowfall per STATION per year and name it YearlySnow\nYearlySnow &lt;- ddply(Winter, .(STATION,Year), summarize, Snow = sum(SNOW, na.rm=TRUE))\n\n#Combine WinterRecords and YearlySnow into one matrix\nAllWinters &lt;- cbind(WinterRecords,YearlySnow)\nAllWinters &lt;- AllWinters[,-4:-5]\n\n#Only include years that have more than 75% of days recorded\nWinterDays &lt;- 121\nFullWinters &lt;- AllWinters[AllWinters$RecordTotal/WinterDays &gt; 0.75, ]\n\n#Get the number of years with more than 75% of days recorded for each STATION\nWinterYears &lt;- ddply(FullWinters, c('STATION'), function(x) c(TotalYears=length(x$Year)))\n\n#Get the total amount of snow for each station for all years\nTotalWinterSnow &lt;- ddply(FullWinters, c('STATION'), function(x) c(TotalWinterSnow=sum(x$Snow)))\n\n#Combine WinterYears and TotalWinterSnow into one matrix\nSnowCalc &lt;- cbind(WinterYears,TotalWinterSnow)\nSnowCalc &lt;- SnowCalc[,-3]\n\n#Get rid of the stations that don't have at least 10 years recorded at &gt;75% of days ###\nComplete.Records &lt;- SnowCalc[SnowCalc$TotalYears &gt; 9, ]\n\n#Calculate average annual snowfall and round to nearest mm\nComplete.Records$MeanAnnualSnowfall &lt;- Complete.Records$TotalWinterSnow/Complete.Records$TotalYears\nComplete.Records$MeanAnnualSnowfall &lt;- round (Complete.Records$MeanAnnualSnowfall, digits = 0)\n\n#Convert SnowDepth from mm to cm\nComplete.Records$MeanAnnualSnowfall &lt;- Complete.Records$MeanAnnualSnowfall/10\nhead(Complete.Records)\n\n            STATION TotalYears TotalWinterSnow MeanAnnualSnowfall\n1 GHCND:USC00072730         11            3493               31.8\n3 GHCND:USC00079605         10            4967               49.7\n4 GHCND:USC00181530         11            7232               65.7\n5 GHCND:USC00181750         11            4828               43.9\n7 GHCND:USC00182282         11            7496               68.1\n8 GHCND:USC00182336         11            8420               76.5\n\n#Add a column to CoordChart showing whether each row matches  a STATION in Complete.Records\n#Use \"NA\" for value if no match, then delete rows with \"NA\" value. \n#Number of rows in CoordChart should now equal number of rows in Complete.Records\nCoordChart$match &lt;- match(CoordChart$STATION, Complete.Records$STATION, nomatch=NA)\nCoordChart &lt;- na.omit(CoordChart)\n\n#Combine Complete.Records and CoordChart. Make sure each STATION matches in row\n#Delete any rows that don't match. Shouldn't be any. If number of rows in Final.Values \n#is less than number of rows in CoordChart, there is a problem (but note that # of cols does change).\nFinal.Values &lt;- cbind(Complete.Records,CoordChart)\nFinal.Values$match2 &lt;- match(Final.Values[  ,1], Final.Values[ ,5], nomatch=NA)\nFinal.Values &lt;- na.omit(Final.Values)\ndim(Final.Values)\n\n[1] 144   9\n\ndim(CoordChart)\n\n[1] 144   4\n\n#Take out unnecessary rows (2nd STATION, match, and match2) and round MeanSnow to 2 decimal places\nFinal.Values[,5] &lt;- Final.Values[,8] &lt;- Final.Values[,9] &lt;- NULL\n\n6. Make data frame to get rid of lists (in R) so can export to text file to use to load weather station points into ArcGIS and skip to Section 2.5.\n\nFinal.Values &lt;- as.data.frame(lapply(Final.Values,unlist))\n\n\nwrite.table(Final.Values, \"MeanSnowData_95-05.txt\", sep=\"\\t\", row.names=F)\n\n7. Alternatively we can conduct interpolation directly in R using the steps below\n\nmerge &lt;-st_as_sf(Final.Values, coords = c(\"Long\",\"Lat\"),crs = ll.crs)\nmerge &lt;- st_transform(merge, albers.crs)\nPAcounties &lt;- counties(\"Pennsylvania\", cb = TRUE) \n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\nPAcounties &lt;- st_transform(PAcounties, albers.crs)\nplot(st_geometry(PAcounties))\nplot(st_geometry(merge),add=T)\n\n\n\n#Import a county layer for study site and check projections\n#counties&lt;-st_read(\"data/PaCounty2019_05.shp\")\n#st_crs(counties)\n\n\n#Copy and Paste State Plane projection into code for StatePlane below\n#Project Weather Stations and Counties to State Plane\n#StatePlane &lt;- CRS(\"+proj=lcc +lat_0=39.3333333333333 +lon_0=-77.75 +lat_1=40.9666666666667 #+lat_2=39.9333333333333 +x_0=600000 +y_0=0 +ellps=GRS80 +units=m +no_defs +type=crs\")\n\nstations &lt;- merge\nstations$MAS &lt;- stations$MeanAnnualSnowfall\n\n\npabox &lt;- st_bbox(PAcounties)\n#Plot out the MAS across the study region\ntmap.bubble &lt;- tm_shape(stations, bbox=pabox) + tm_sf(size = \"MAS\")\ntmap.bubble\n\n\n#Create a grid onto which we will interpolate:\nbb &lt;- st_bbox(PAcounties) %&gt;% st_as_sfc()\ngrid_spacing &lt;- 5000\ngrid &lt;- st_make_grid(bb, square = T, cellsize = c(grid_spacing, grid_spacing)) %&gt;% # the grid, covering bounding box\n  st_intersection(bb) %&gt;%\n    cbind(data.frame(ID = sprintf(paste(\"GID%0\",nchar(length(.)),\"d\",sep=\"\"), 1:length(.)))) %&gt;%\n    st_sf()\n#Convert grid to a raster to use later\nrgrid &lt;- rast(grid, res=5000)#, type=\"xyz\",crs = albers.crs,digits=6,extent=NULL)\nplot(st_geometry(bb))\nplot(st_geometry(grid),add=T)\n\n\n\n#Plot Weather Stations over counties\nplot(st_geometry(PAcounties))\nplot(st_geometry(stations), add=T, pch=16, col=\"red\")\n\n\n\n\nnearest neighbor interpolate grid over sample points directly in R with gstat package\n\nstations2 &lt;- sf_to_df(stations, fill=TRUE)\ngOK &lt;- gstat(formula=MAS~1, data=stations2, locations=~x+y, nmax=10, set=list(idp=0))\nx &lt;- interpolate(rgrid, gOK,debug.level=0)\nclass(x)\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\nplot(x,1)\nplot(st_geometry(stations), add=T, pch=16, col=\"red\")\nplot(st_geometry(PAcounties), add=T)\n\n\n\n\nnmax = maximum number of points used is 10\nidp = inverse distance power is zero so that all 10 neighbors are equally weighted"
  },
  {
    "objectID": "NetCDF_Script.html",
    "href": "NetCDF_Script.html",
    "title": "\n14  Importing Dynamically Downscaled Global Climate Data\n",
    "section": "",
    "text": "This exercise will provide some code for manipulating climate change data from the Regional Climate Downscaling by copy the link into your browser: http://regclim.coas.oregonstate.edu/data-access/index.html or just select the link here: Regional Climate Downscaling . IMPORTANT: For each climate projection, must change name in first command and file name in last command.\n1. Open the script “NetCDF_Script.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(ncdf4)\n\n3. Open netCDF and setting verbose=true provides details about the data in the netcdf file including the varid. You need to know the varid to select the variable you want to extract/summarize. Note: the dimensions x, y, time also get a varid so you will need to subtract 3 from the varid of interest to get the correct one.\n\ndat &lt;- nc_open(\"data/Monthly_AvgMinTemp_1995-99_MPI.nc\", write=TRUE,  readunlim=TRUE, verbose=FALSE)  \n\n#Read data to load all the data from the downloaded variable into the tmin object\ntmin &lt;- dat$var[[1]]\n\n#####################################\n# The following illustrates how to read the data \n#####################################\nprint(paste(tmin$name)) #in this case the 'field name' is TAMIN\n\n[1] \"TAMIN\"\n\n# Grab data for TAMIN variable and place in object df1\ndf1 &lt;- ncvar_get(dat, tmin)\n\n#head(df1, n = 10L)# head(x, n = 6L, ...); head returns the first data  entries, x is the object, \n#n sets the number of entries displayed. tail returns  the last of the data entries         \n# Dimensions of df1 (x, y, time)\ndim(df1)\n\n[1] 37 22 49\n\n# Dimensions can also be examined one at a time\ndim(df1)[1]     # number of x grids (37)\n\n[1] 37\n\ndim(df1)[2]     # number of y grids (22)\n\n[1] 22\n\ndim(df1)[3]     # number of months in file (49)\n\n[1] 49\n\n#NOTE: FILE INCLUDES MONTHS OTHER THAN JANUARY (Jans are 1,13,25,37,49)\n\n# Check first element\ndf1[1,1,1]\n\n[1] -2.499621\n\n\n\n# Check first January for all x,y\ndf1[,,1]\n\n\n#Create a new matrix which is monthly averages for each grid cell. Make the new  matrix the \n#same size (i.e. same number of rows and columns as there are in the dataframe df1\nsum1 &lt;- array(data=NA, c(dim(df1)[1],dim(df1)[2] ))\ndim(sum1)\n\n[1] 37 22\n\n# Create January mean TAMIN for each x-y coordinate\nfor(i in 1:dim(df1)[1]){ # loop over x-coords\n    for(j in 1:dim(df1)[2]){ # loop over y-coords\n        sum1[i, j] &lt;- (df1[i,j,1]+df1[i,j,13]+df1[i,j,25]+df1[i,j,37]+df1[i,j,49])/5\n    }\n}\n\n###########################################################\n###########################################################\n# Create netcdf file from sum1 (contains matrix of new data)\n###########################################################\n# Get x and y coordinates from original \"dat\" ncdf file\nx  = ncvar_get(nc=dat,varid=\"x\")   \ny  = ncvar_get(nc=dat,varid=\"y\")  \n\n# Check dimensions\nlength(x)\n\n[1] 37\n\nlength(y)\n\n[1] 22\n\ndim(sum1)\n\n[1] 37 22\n\n## define the netcdf coordinate variables - note that these are coming from the dat\n#file with actual values\ndim1 = ncdim_def( \"X\",\"meters\", as.double(x))\ndim2 = ncdim_def( \"Y\",\"meters\", as.double(y))\n\n## define the EMPTY (climate) netcdf variable and define names that will be used in the \n#var.def.ncdf function\n# Define climate variable names\n    new.name &lt;- 'mintemp'\n# Define units of measurement for variable\n    units &lt;- 'degreesC'\n# Define long name for variable\n    long.name &lt;- 'Jan average min temperature'\n\nvarz = ncvar_def(new.name,units, list(dim1,dim2), -1, \n          longname=long.name)\n\n# associate the netcdf variable with a netcdf file   \n# put the variable into the file, and close\n\nnc.ex = nc_create( \"MPI1999-95.nc\", varz )\nncvar_put(nc.ex, varz, sum1)\nnc_close(nc.ex)"
  },
  {
    "objectID": "DMAdeerMovebank.html#importing-datasets-from-a-web-source",
    "href": "DMAdeerMovebank.html#importing-datasets-from-a-web-source",
    "title": "\n15  Movement Methods\n",
    "section": "\n15.1 Importing datasets from a web source",
    "text": "15.1 Importing datasets from a web source\nMovebank.org is a cloud-based repository for relocation data from GPS-collared or VHF-collared animals. It provides a storage facility in the cloud that can serve as a backup for your data or a transfer portal to share data among colleagues or interested researchers. Similar to any email account, each user has a Movebank account that has a login and password to gain access to your data. Administration privileges can be given to anyone with an account for viewing and downloading data.\n1. Open the script “DMAdeerMovebank.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(move)\nlibrary(RCurl)\nlibrary(circular)\n\n3. Next we are going to the Movebank home page and explore what it has to offer. Need to create an account or select a dataset that does not require permission to use.\n\nlogin &lt;- movebankLogin(username=\"wdwalter\", password=\"XXXXX\")\n\n\ndeer &lt;- getMovebankData(study=\"DMA White-tailed Deer 2018 Pennsylvania USA\",\n                        login=login, moveObject=TRUE)\nn.indiv(deer)\nn.locs(deer)\n#Plot the first deer in the stack\nplot(deer[[1]])\n\n#Now we will select a single deer to explore more\ndeer1 &lt;- deer[['X20212_20242F']]\nplot(deer1)\n\n#Select and plot locations of the initial 2 deer in your list\ndeer2 &lt;- deer[[c(1,2)]]\nplot(deer2)\n#Determine names of initial 2 deer selected above\nnamesIndiv(deer2)"
  },
  {
    "objectID": "MovementScript.html",
    "href": "MovementScript.html",
    "title": "\n16  Movement Trajectories\n",
    "section": "",
    "text": "We will start with simply creating trajectories between successive locations. As stated previously, there are 2 types of trajectories but their are also 2 forms of Type II trajectories if we have time recorded. Depending on the duration between locations we can have uniform time lag between successive relocations termed regular trajectories and non-uniform time lag that results in irregular trajectories. We will begin this section with simply creating irregular trajectories from relocation data because, even though we set up a time schedule to collection locations at uniform times, climate, habitat, and satellites do not always permit such schedules of data collection.\n1. Open the script “MovementScript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatLT)\nlibrary(chron)\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. We are again going to be using more of the mule deer dataset than from the earlier exercises\n\nmuleys &lt;-read.csv(\"data/DCmuleysedited.csv\", header=T)\n\n5. Check for duplicate locations in dataset. The reason for this is very important and will be apparent shortly.\n\n#Check for duplicate locations in dataset\nsummary(duplicated(muleys))\n\n   Mode   FALSE \nlogical    9752 \n\n\n6. For trajectories of type II (time recorded), the conversion of the date to the format POSIX needs to be done to get proper digits of date into R.\n\nda &lt;- as.POSIXct(strptime(muleys$GPSFixTime,format=\"%Y.%m.%d %H:%M:%S\"))\nmuleys$da &lt;- da\n\ntimediff &lt;- diff(muleys$da)*60\nmuleys &lt;-muleys[-1,]\nmuleys$timediff &lt;-as.numeric(abs(timediff)) \n\n#Remove outlier locations\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous plot\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\n#Project deer.spdf to Albers as in previous exercise\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers),axes=T)\n\n\n\n\n6. Now create an object of class “ltraj” by animal using the ID field and display by each individual (i.e., ltraj[1])\n\nltraj &lt;- as.ltraj(st_coordinates(deer.albers),deer.albers$da,id=deer.albers$id)\nhead(ltraj[1])#Describes the trajectory\n\n\n*********** List of class ltraj ***********\n\nType of the traject: Type II (time recorded)\n* Time zone unspecified: dates printed in user time zone *\nIrregular traject. Variable time lag between two locs\n\nCharacteristics of the bursts:\n   id burst nb.reloc NAs          date.begin            date.end\n1 D12   D12      101   0 2011-10-12 03:00:48 2011-10-24 21:00:48\n\n\n infolocs provided. The following variables are available:\n[1] \"pkey\"\n\nplot(ltraj)#plot all trajectories created\n\n\n\n#Plot each trajectory separately\nplot(ltraj[1])\n\n\n\nplot(ltraj[2])\n\n\n\nplot(ltraj[3])\n\n\n\nplot(ltraj[4])\n\n\n\nplot(ltraj[5])\n\n\n\nplot(ltraj[6])\n\n\n\n\n7. Create a histogram of time lag (i.e., interval) and distance between successive locations for each deer. This is a nice way to inspect the time lag between locations as you don’t want to include a location if too much time has passed since the previous and it also shows why a trajectory is irregular.\n\nhist(ltraj[1], \"dt\", freq = TRUE)\nhist(ltraj[1], \"dist\", freq = TRUE)\nhist(ltraj[2], \"dt\", freq = TRUE)\nhist(ltraj[2], \"dist\", freq = TRUE)\nhist(ltraj[3], \"dt\", freq = TRUE)\nhist(ltraj[3], \"dist\", freq = TRUE)\nhist(ltraj[4], \"dt\", freq = TRUE)\nhist(ltraj[4], \"dist\", freq = TRUE)\nhist(ltraj[5], \"dt\", freq = TRUE)\nhist(ltraj[5], \"dist\", freq = TRUE)\nhist(ltraj[6], \"dt\", freq = TRUE)\nhist(ltraj[6], \"dist\", freq = TRUE)"
  },
  {
    "objectID": "DistanceUniqueBurst.html",
    "href": "DistanceUniqueBurst.html",
    "title": "\n17  Distance Between Locations\n",
    "section": "",
    "text": "Determining the distance between locations or between locations and respective habitat types can serve a variety of purposes. Several resource selection procedures require a description of the daily movement distance of an animal to determine the habitat available to an animal or when generating random locations around known locations. We will start here with a method to determine the average distance moved by mule deer in Colorado in a study to determine methods to alleviate depradation on sunflowers that have become a high commodity crop in the area.\n1. Open the script “DistanceUniqueBurst.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatLT)\nlibrary(chron)\nlibrary(class)\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Code to read in dataset then subset for an individual animal\n\nmuleys &lt;-read.csv(\"data/DCmuleysedited.csv\", header=T)\n#Code to select an individual animal\nmuley15 &lt;- subset(muleys, id==\"D15\")\ntable(muley15$id)\n\n\n D15 \n2589 \n\n# #Sort data to address error in code and then look at first 20 records of data to confirm\n# muley15 &lt;- muley15[order(muley15$GPSFixTime),]\n# #Run code to display the first 20 records to look at what sorting did to data\n\n5. Prepare data to create trajectories using the ltraj command in Adehabitat LT\n\n######################################################\n## Example of a trajectory of type II (time recorded) with conversion of the date to the \n#format POSIX that nNeeds to be done to get proper digits of date into R then POSIXct uses\n#library(chron)\nda &lt;- as.character(muley15$GPSFixTime)\nda &lt;- as.POSIXct(strptime(muley15$GPSFixTime,format=\"%Y.%m.%d %H:%M:%S\"))\nhead(da)\n\n[1] \"2011-10-12 00:02:03 EDT\" \"2011-10-15 09:00:48 EDT\"\n[3] \"2011-11-05 00:00:48 EDT\" \"2011-12-06 06:00:52 EST\"\n[5] \"2011-12-16 21:00:49 EST\" \"2011-12-20 21:00:49 EST\"\n\n#Attach da to muley15\nmuley15$da &lt;- da\n\ntimediff &lt;- diff(muley15$da)\nmuley15 &lt;-muley15[-1,]\nmuley15$timediff &lt;-as.numeric(abs(timediff)) \n\n#Remove outlier locations\ncoords &lt;- st_as_sf(muley15, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-110.0,xmax=-106.5,ymin=36.8,ymax=39.0)#Visually identified based on previous plot\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\n#Project deer.spdf to Albers as in previous exercise\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers),axes=T)\n\n\n\n\n6. Create an object of class “ltraj” for muley15 dataset\n\nltraj &lt;- as.ltraj(st_coordinates(deer.albers),deer.albers$da,id=deer.albers$id)\nplot(ltraj)\n\n\n\nltraj\n\n\n*********** List of class ltraj ***********\n\nType of the traject: Type II (time recorded)\n* Time zone unspecified: dates printed in user time zone *\nIrregular traject. Variable time lag between two locs\n\nCharacteristics of the bursts:\n   id burst nb.reloc NAs          date.begin            date.end\n1 D15   D15     2588   0 2011-10-12 03:00:52 2012-08-31 09:00:51\n\n\n infolocs provided. The following variables are available:\n[1] \"pkey\"\n\n#Now let's look at time differences between locations before moving forward\nsummary(muley15$timediff)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   1.998    2.998    3.000    8.874    3.002 7589.999 \n\n\n7. Need to create separate “bursts” for each trajectory based on the number of locations collected each day. In our case it was 8 (i.e., locations collected every 3 hours during a 24-hour period).\n\n#We want to study the trajectory of the day at the scale of the day. We define one trajectory \n#per day. The trajectory should begin at 2200 hours so the following function returns TRUE if\n#the date is time between 06H00 and 23H00 (i.e. results in 7-8 locations/day bursts)\nfoo &lt;- function(date) {\nda &lt;- as.POSIXlt(date)\nho &lt;- da$hour + da$min\nreturn(ho&gt;15.9&ho&lt;23.9)\n}\ndeer &lt;- cutltraj(ltraj, \"foo(date)\", nextr = TRUE)\n\nWarning in cutltraj(ltraj, \"foo(date)\", nextr = TRUE): At least 3 relocations are needed for a burst\n 345 relocations have been deleted\n\n#Notice that the above code will remove 328 relocations that fall\n#outside of your time criteria\n#Warning message:\n#In cutltraj(ltraj, \"foo(date)\", nextr = TRUE) :\n#  At least 3 relocations are needed for a burst\n# 328 relocations have been deleted\nhead(deer)\n\n\n*********** List of class ltraj ***********\n\nType of the traject: Type II (time recorded)\n* Time zone unspecified: dates printed in user time zone *\nIrregular traject. Variable time lag between two locs\n\nCharacteristics of the bursts:\n   id   burst nb.reloc NAs          date.begin            date.end\n1 D15 D15.001        6   0 2011-10-12 03:00:52 2011-10-12 18:00:52\n2 D15 D15.003        7   0 2011-10-13 00:00:35 2011-10-13 18:00:35\n3 D15 D15.005        7   0 2011-10-14 00:00:42 2011-10-14 18:00:42\n4 D15 D15.007        7   0 2011-10-15 00:00:35 2011-10-15 18:00:45\n5 D15 D15.009        7   0 2011-10-16 00:00:39 2011-10-16 18:00:49\n6 D15 D15.011        6   0 2011-10-17 00:01:07 2011-10-17 15:01:03\n\n\n infolocs provided. The following variables are available:\n[1] \"pkey\"\n\n\n8. Code to change ltraj to a data.frame to summarize distance between locations for each daily burst\n\ndfdeer &lt;- ld(deer)\nhead(dfdeer)\n\n#Code to get mean distance moved for each burst\ndfdeer &lt;- subset(dfdeer, !is.na(dfdeer$dist))#remove NAs from last location of a burst\nmean_dist &lt;- do.call(data.frame, aggregate(dfdeer$dist, by=list(dfdeer$burst), \n    function(x) c(mean = mean(x), sd = sd(x), n=abs(length(x)))))\nhead(mean_dist)\n#Write.table gives csv output of Summary \n#write.table(mean_dist, file = \"Distance.csv\", sep =\",\", row.names = TRUE, \n#  col.names = TRUE, qmethod =\"double\")"
  },
  {
    "objectID": "FPTscript.html",
    "href": "FPTscript.html",
    "title": "\n18  First Passage Time (FPT)\n",
    "section": "",
    "text": "The first passage time (FPT) is a parameter often used to describe the scale at which patterns occur in a trajectory. For a given scale r, it is defined as the time required by the animals to pass through a circle of radius r. The mean first passage time scales proportionately to the square of the radius of the circle for an uncorrelated random walk (Johnson et al. 1992). Johnson et al. (1992) used this property to differentiate facilitated diffusion and impeded diffusion, according to the value of the coefficient of the linear regression log(FPT) = a * log(radius) + b. Under the hypothesis of a random walk, a should be equal to 2 (higher for impeded diffusion, and lower for facilitated diffusion). Note however, that the value of a converges to 2 only for large values of radius. Another use of the FPT was proposed that, instead of computing the mean of FPT, use the variance of the log(FPT). This variance should be high for scales at which patterns occur in the trajectory (e.g. area restricted search; Fauchald and Tverra 2003). This method is often used to determine the scale at which an animal searches for food.\nThe value fpt computes the FPT for each relocation and each radius, and for each animal. This function returns an object of class “fipati” (i.e., a list with one component per animal). Each component is a data frame with each column corresponding to a value of radii and each row corresponding to a relocation. An object of class fipati has an attribute named “radii” corresponding to the argument radii of the function fpt. meanfpt and varlogfpt return a data frame giving respectively the mean FPT and the variance of the log(FPT) for each animal (rows) and rach radius (column). These objects also have an attribute “radii”.\n1. Open the script “FPTscript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatLT)\nlibrary(chron)\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Load in our mule deer dataset from previous exercises\n\nmuleys &lt;-read.csv(\"data/DCmuleysedited.csv\", header=T)\n\n#Code to look at number of relocations per animal\ntable(muleys$id)\n\n\n D12  D15  D16  D19   D4   D6   D8 \n 120 2589 2157 1156 1304 1455  971 \n\n#Remove outlier locations\nnewmuleys &lt;-subset(muleys, muleys$Long &gt; -110.50 & muleys$Lat &gt; 37.3 & muleys$Long &lt; -107)\nmuleys &lt;- newmuleys\n\n#Conversion of the date to the format POSIX as in previous exercise\nda &lt;- as.character(muleys$GPSFixTime)\nda &lt;- as.POSIXct(strptime(muleys$GPSFixTime,format=\"%Y.%m.%d %H:%M:%S\"))\nmuleys$da &lt;- da\n\n5. For trajectories of type II (time recorded), the conversion of the date to the format POSIX needs to be done to get proper digits of date into R. Then create an sf class of locations.\n\nda &lt;- as.POSIXct(strptime(muleys$GPSFixTime,format=\"%Y.%m.%d %H:%M:%S\"))\nmuleys$da &lt;- da\n\ntimediff &lt;- diff(muleys$da)*60\nmuleys &lt;-muleys[-1,]\nmuleys$timediff &lt;-as.numeric(abs(timediff)) \n\n#Remove outlier locations\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous plot\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\n#Project deer.spdf to Albers as in previous exercise\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers),axes=T)\n\n\n\n\n6. Create an object of class “ltraj” (i.e., trajectory) for all animals\n\nltraj &lt;- as.ltraj(st_coordinates(deer.albers),deer.albers$da,id=deer.albers$id)\nplot(ltraj)\n\n\n\n\n7. Code below actually creates First Passage Time and mean and variance of fpt\n\nplot(ltraj[1])\ni1 &lt;- fpt(ltraj[1], seq(300,1000, length=30))\nplot(i1, scale = 200, warn = FALSE)\n\nplot(ltraj[2])\ni2 &lt;- fpt(ltraj[2], seq(300,1000, length=30))\nplot(i2, scale = 500, warn = FALSE)\n\ntoto2 &lt;- meanfpt(i2)\ntoto2\nattr(toto2, \"radii\")\n\ntoto2 &lt;- varlogfpt(i2)\ntoto2\nattr(toto2, \"radii\")\n\nplot(ltraj[3])\ni3 &lt;- fpt(ltraj[3], seq(300,1000, length=30))\nplot(i3, scale = 500, warn = FALSE)\n\ntoto3 &lt;- meanfpt(i3)\ntoto3\nattr(toto3, \"radii\")\n\ntoto3 &lt;- varlogfpt(i3)\ntoto3\nattr(toto3, \"radii\")\n\nplot(ltraj[4])\ni4 &lt;- fpt(ltraj[4], seq(300,1000, length=30))\nplot(i4, scale = 500, warn = FALSE)\n\ntoto4 &lt;- meanfpt(i4)\ntoto4\nattr(toto4, \"radii\")\n\ntoto4 &lt;- varlogfpt(i4)\ntoto4\nattr(toto4, \"radii\")\n\nplot(ltraj[5])\ni5 &lt;- fpt(ltraj[5], seq(300,1000, length=30))\nplot(i5, scale = 500, warn = FALSE)\n\ntoto5 &lt;- meanfpt(i5)\ntoto5\nattr(toto5, \"radii\")\n\ntoto5 &lt;- varlogfpt(i5)\ntoto5\nattr(toto5, \"radii\")\n\nplot(ltraj[6])\ni6 &lt;- fpt(ltraj[6], seq(300,1000, length=30))\nplot(i6, scale = 500, warn = FALSE)\n\nplot(ltraj[7])\ni7 &lt;- fpt(ltraj[7], seq(300,1000, length=30))\nplot(i7, scale = 500, warn = FALSE)\n\ntoto7 &lt;- meanfpt(i7)\ntoto7\nattr(toto7, \"radii\")\n\ntoto7 &lt;- varlogfpt(i7)\ntoto7\nattr(toto7, \"radii\")\n\n8. Code to export each trajectory as a shapefile if needed\n\ntoto1 &lt;-ltraj2sldf(ltraj[1])\nplot(toto1)\n#st_write(toto1,\"D12.sp\")\nsummary(toto1)\n\n#Write lines and points as a shapefile\ntoto2lines &lt;-ltraj2sldf(ltraj[2],byid=TRUE)\ntoto2pts &lt;- ltraj2spdf(ltraj[2])\n\n#If we want to define projection before making a shapefile\nproj4string &lt;- CRS(\"+proj=utm +zone=13N +ellps=WGS84\")\ntoto2lines@proj4string &lt;- proj4string\ntoto2pts@proj4string &lt;- proj4string\n\nplot(toto2pts)\nplot(toto2lines, add=T)\n\nst_write(toto2pts,\"D15pts.shp\")\nst_write(toto2lines, paste(\"traj_line_\",sep=\"\"))\n\ntoto3 &lt;-ltraj2sldf(ltraj[3])\nplot(toto3)\nst_write(toto3,\"D16.shp\")\n\ntoto4 &lt;-ltraj2sldf(ltraj[4])\nplot(toto4)\nst_write(toto4,\"D19.shp\")\n\ntoto5 &lt;-ltraj2sldf(ltraj[5])\nplot(toto5)\nst_write(toto5,\"D4.shp\")\n\ntoto6 &lt;-ltraj2sldf(ltraj[6])\nplot(toto6)\nst_write(toto6,\"D6.shp\")\n\ntoto7 &lt;-ltraj2sldf(ltraj[7])\nplot(toto7)\nst_write(toto7,\"D8.shp\")"
  },
  {
    "objectID": "RegTrajScript.html",
    "href": "RegTrajScript.html",
    "title": "\n19  Regular Trajectories\n",
    "section": "",
    "text": "1. Open the script “RegTrajScript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatLT)\nlibrary(chron)\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Now read in dataset, extract a single animal and create ltraj as in previous exercise\n\nmuleys &lt;-read.csv(\"data/DCmuleysedited.csv\", header=T)\n\n#CODE FOR AN INDIVIDUAL ANIMAL\nmuley15 &lt;- subset(muleys, id==\"D15\")\nmuley15$id &lt;- factor(muley15$id)\ntable(muley15$id)\n\n\n D15 \n2589 \n\n#Sort data to address error in code and then look at first 10 records of data to confirm\nmuley15 &lt;- muley15[order(muley15$GPSFixTime),]\n\n######################################################\n## Example of a trajectory of type II (time recorded)\n### Conversion of the date to the format POSIX\n#Needs to be done to get proper digits of date into R then POSIXct\n#uses library(chron)\nda &lt;- as.character(muley15$GPSFixTime)\nda &lt;- as.POSIXct(strptime(muley15$GPSFixTime,format=\"%Y.%m.%d %H:%M:%S\"))\n#Attach da to muley15\nmuley15$da &lt;- da\n\ntimediff &lt;- diff(muley15$da)\nmuley15 &lt;-muley15[-1,]\nmuley15$timediff &lt;-as.numeric(abs(timediff)) \n\n#Remove outlier locations\ncoords &lt;- st_as_sf(muley15, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-110.0,xmax=-106.5,ymin=36.8,ymax=39.0)#Visually identified based on previous plot\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\n#Project deer.spdf to Albers as in previous exercise\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers),axes=T)\n\n\n\n#Creation of an object of class \"ltraj\"\nltraj &lt;- as.ltraj(st_coordinates(deer.albers),deer.albers$da,id=deer.albers$id)\nplot(ltraj)\n\n\n\n\n6.We want to study the trajectory of the day at the scale of the day. We define one trajectory per day. The trajectory should begin at 2200 hours so the following function returns TRUE if the date is time between 06H00 and 23H00 (i.e. results in 7-8 locations/day bursts)\n\nfoo &lt;- function(date) {\nda &lt;- as.POSIXlt(date)\nho &lt;- da$hour + da$min\nreturn(ho&gt;18.0&ho&lt;23.9)\n}\ndeer &lt;- cutltraj(ltraj, \"foo(date)\", nextr = TRUE)\n\nWarning in cutltraj(ltraj, \"foo(date)\", nextr = TRUE): At least 3 relocations are needed for a burst\n 27 relocations have been deleted\n\nhead(deer)\n\n\n*********** List of class ltraj ***********\n\nType of the traject: Type II (time recorded)\n* Time zone unspecified: dates printed in user time zone *\nIrregular traject. Variable time lag between two locs\n\nCharacteristics of the bursts:\n   id   burst nb.reloc NAs          date.begin            date.end\n1 D15 D15.001        7   0 2011-10-12 03:00:52 2011-10-12 21:00:40\n2 D15 D15.002        8   0 2011-10-13 00:00:35 2011-10-13 21:00:39\n3 D15 D15.003        8   0 2011-10-14 00:00:42 2011-10-14 21:00:39\n4 D15 D15.004        8   0 2011-10-15 00:00:35 2011-10-15 21:00:51\n5 D15 D15.005        8   0 2011-10-16 00:00:39 2011-10-16 21:00:37\n6 D15 D15.006        8   0 2011-10-17 00:01:07 2011-10-17 21:00:49\n\n\n infolocs provided. The following variables are available:\n[1] \"pkey\"\n\n## Remove the first and last burst if needed?\n#deer2 &lt;- deer[-c(1,length(deer))]\n\n#Bind the trajectories\ndeer3 &lt;- bindltraj(deer)\ndeer3\n\n\n*********** List of class ltraj ***********\n\nType of the traject: Type II (time recorded)\n* Time zone unspecified: dates printed in user time zone *\nIrregular traject. Variable time lag between two locs\n\nCharacteristics of the bursts:\n   id burst nb.reloc NAs          date.begin            date.end\n1 D15   D15     2561   0 2011-10-12 03:00:52 2012-08-31 09:00:51\n\n\n infolocs provided. The following variables are available:\n[1] \"pkey\"\n\nplot(deer3)\n\n\n\nis.regular(deer3)\n\n[1] FALSE\n\nplotltr(deer3, \"dt\")\n\n\n\n## The relocations have been collected every 3 hours, and there are some\n## missing data\n## The reference date: the hour should be exact (i.e. minutes=0):\nrefda &lt;- strptime(\"00:00\", \"%H:%M\")\nrefda\n\n[1] \"2024-01-11 EST\"\n\n## Set the missing values\ndeerset &lt;- setNA(deer3, refda, 3, units = \"hour\")\n## now, look at dt for the bursts:\nplotltr(deerset, \"dt\")\n\n\n\n## dt is nearly regular: round the date:\ndeerset1 &lt;- sett0(deerset, refda, 3, units = \"hour\")\nplotltr(deerset1, \"dt/3600\")\n\n\n\nis.regular(deerset1)\n\n[1] TRUE\n\n## deerset1 is now regular\n\n## Is the resulting object \"sd\" ?\nis.sd(deerset1)\n\n[1] TRUE\n\n#Show the changes in the distance between successive relocations with the time\nplotltr(deerset1, \"dist\")\n\n\n\ndeerset1#Is the trajectory regular now?\n\n\n*********** List of class ltraj ***********\n\nType of the traject: Type II (time recorded)\n* Time zone unspecified: dates printed in user time zone *\nRegular traject. Time lag between two locs: 10800 seconds\n\nCharacteristics of the bursts:\n   id burst nb.reloc NAs          date.begin            date.end\n1 D15   D15     2595  34 2011-10-12 04:00:00 2012-08-31 10:00:00\n\n\n infolocs provided. The following variables are available:\n[1] \"pkey\""
  },
  {
    "objectID": "NSDscript.html",
    "href": "NSDscript.html",
    "title": "\n20  Net Squared Displacement\n",
    "section": "",
    "text": "Net squared displacement (NSD) looks at the movement vectors of animals to determine their use of the landscape (Bunnefeld et al. 2011, Papworth et al. 2012). Bunnefeld et al. (2011) determined a novel method to identify movement patterns using NSD which is the straight line distance between an animals’ starting location and subsequent locations. Movements were then categorized into one of 5 categories based on the top model that describes movement for each individual. In the code for this section, we have updated the code to include the 5 movement equations along with R code provided in Papworth et al. (2012) to enable determination of migratory, mixed migratory, disperser, home range, or nomadic movement behavior. Figure 1 in Bunnefeld et al. (2011) is helpful to interpret the output of this code that identifies the pattern of NSD over time that is determined by which of the 5 movement behaviors the animal follows. Those interested in this section should read the 2 papers cited for more details and specifics of the methods.\n1. Open the script “NSDScript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatHR)\nlibrary(sf)\n#library(trip)\n#library(lattice)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Going to be using previous mule deer dataset from Colorado and clean up the data as in previous exercises\n\nmuleys&lt;-read.csv(\"data/DCmuleysedited.csv\", header=T, sep=\",\")\n\nmuleys$NewDate&lt;-as.POSIXct(muleys$GPSFixTime, format=\"%Y.%m.%d %H:%M:%S\", origin=\"1970-01-01\")\n\n#TIME DIFF NECESSARY\ntimediff &lt;- diff(muleys$NewDate)*60\n# remove first entry without any difference \nmuleys &lt;- muleys[-1,] \nmuleys$timediff &lt;-as.numeric(abs(timediff))\nsummary(muleys$timediff)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n    3581    10792    10800    61399    10809 46399997 \n\n#Remove locations greater then 24 hours apart in time\nmuleys &lt;- subset(muleys, muleys$timediff &lt; 18000)\n\n#Remove outlier locations  then return to a dataframe to use later\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous plot\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\nmuleyscoords &lt;- as.data.frame(sf::st_coordinates(deer.spdf))\nmuleyscoords$Lat &lt;- muleyscoords$X\nmuleyscoords$Long &lt;- muleyscoords$Y\n\nmuleys &lt;- as.data.frame(deer.spdf)\nmuleys &lt;- cbind(st_drop_geometry(muleys),muleyscoords)\nmuleys &lt;- muleys[c(-24:-25)]#get rid of duplicate lat long as X and Y\n#Project deer.spdf to Albers as in previous exercise\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers),axes=T)\n\n\n\n\n5. The key to NSD is proper delineation of movement periods so we can explore a few alternatives here. First we will define deer and year based simply on the Year the location was recorded. Not very biologically meaningful but for simplicity we will start with calender year. Be sure to skip step 6 below and continue on with step 7 to the end of the exercise.\n\nmuleys$Year &lt;- format(muleys$NewDate, \"%Y\")\nmuleys &lt;- subset(muleys, muleys$Year != \"NA\")\nmuleys$YearBurst &lt;- c(paste(muleys$id,muleys$Year,sep=\"_\"))\nmuleys$YearBurst &lt;- as.factor(muleys$YearBurst)\nrange(muleys$NewDate)\n\n6. Alternatively, we could assign Year as when we would expect mule deer to disperse or migrate from summer to winter range. In our Colorado example, the mule deer were captured on 30 September 2011 (winter range) so we will start there and separate out a summer season from May to August using the code below. Be sure to skip step 5 above and run this step instead but do not run both.\n\nrange(muleys$NewDate)\n\n[1] \"2011-10-12 00:00:34 EDT\" \"2012-08-31 09:00:51 EDT\"\n\nmuleys$Year2 &lt;- NULL\nmuleys$Year2[muleys$NewDate &gt; \"2011-09-30 00:30:00\" & \n  muleys$NewDate &lt; \"2012-03-31 23:00:00\"] &lt;- 2011\nmuleys$Year2[muleys$NewDate &gt; \"2012-03-31 23:59:00\" &\n  muleys$NewDate &lt; \"2012-10-01 23:00:00\"] &lt;- 2012\nmuleys$Year2 &lt;- as.factor(muleys$Year2)\nmuleys$YearBurst2 &lt;- c(paste(muleys$id,muleys$Year2,sep=\"_\"))\nmuleys$YearBurst2 &lt;- as.factor(muleys$YearBurst2)\ntable(muleys$YearBurst2)\n\n\nD12_2011 D15_2011 D15_2012 D16_2011 D16_2012  D4_2011  D6_2011  D6_2012 \n      98      619     1178     1048      739      102     1323       84 \n D8_2011 \n     956 \n\n#If needed we can remove deer that don't fit our minimum\n#sample size requirement\nmuleys &lt;- subset(muleys, table(muleys$YearBurst2)[muleys$YearBurst2] &gt; 100)\nmuleys$YearBurst2 &lt;- droplevels(muleys$YearBurst2)\n\n7. Next we are going to rename our dataset to simply follow along with previous code I have created unless you want to change d1 to muleys throughout. This code will separate each deer by year which will make it easier to test NSD for each deer in our study\n\nd1 &lt;- muleys\n#Code separate each animal into a shapefile or text file to use as a \"List\" \n#Start with making an input file\nindata &lt;- d1\ninnames &lt;- unique(d1$YearBurst2)\ninnames &lt;- innames[1:8]#needs to be number of unique IDs\noutnames &lt;- innames\n# begin loop to calculate home ranges\nfor (i in 1:length(innames)){\n  data &lt;- indata[which(indata$YearBurst2==innames[i]),]\n  if(dim(data)[1] != 0){\n    #data &lt;-data[c(-21)]\n    # export the point data into a shp file\n    data.xy = data[c(\"X\", \"Y\")]\n    coordinates(data.xy) &lt;- ~X+Y\n    sppt &lt;- SpatialPointsDataFrame(coordinates(data.xy),data)\n    proj4string(sppt) &lt;- CRS(utm.crs)\n    #writePointsShape(sppt,fn=paste(outnames[i],sep=\"/\"),factor2char=TRUE)\n    #sppt &lt;-data[c(-22,-23)] \n    write.table(sppt, paste(outnames[i],\"txt\",sep=\".\"), sep=\"\\t\", quote=FALSE, row.names=FALSE)\n    write.table(paste(outnames[i],\"txt\",sep=\".\"), sep=\"\\t\", quote=FALSE, row.names=FALSE, \n      col.names=FALSE, \"In_list.txt\", append=TRUE)\n#The write.table line above should only be run once to create the In_list.txt file otherwise \n#it rights all animals each time\n  \n#Note: There are 3 lines of code that are not active that can #be activated to export point \n#shapefiles for all resulting #animals but need to remove as.POSIX column first\n}}\n\n10. Code below will be needed to get NSD and best movement model for each animal\n\n#Reads the List file of GPS datasets\nList&lt;-read.table(\"In_list.txt\",sep=\"\\t\",header=F)\nhead(List) #List contains the filenames of the all the datasets in our study \n#(i.e., By YearBurst we created previoulsy)\n\n# We will start by generating a vector of results we would like as the final\n#output to our analyses\nID &lt;- rep(0,nrow(List))\nLOCS &lt;- rep(0,nrow(List))\nMIGR &lt;- rep(0,nrow(List))\nMIXM &lt;- rep(0,nrow(List))\nDISP &lt;- rep(0,nrow(List))\nHORA &lt;- rep(0,nrow(List))\nNOMA &lt;- rep(0,nrow(List))\nID &lt;- rep(0,nrow(List))\nLOCS &lt;- rep(0,nrow(List))\nMIGR &lt;- rep(0,nrow(List))\nMIXM &lt;- rep(0,nrow(List))\nDISP &lt;- rep(0,nrow(List))\nHORA &lt;- rep(0,nrow(List))\nNOMA &lt;- rep(0,nrow(List))\nAICC_1 &lt;- rep(0,nrow(List))\nAICC_2 &lt;- rep(0,nrow(List))\nAICC_3 &lt;- rep(0,nrow(List))\nAICC_4 &lt;- rep(0,nrow(List))\nAICC_5 &lt;- rep(0,nrow(List))\n\nminAIC &lt;- rep(0,nrow(List))\nd_AICC_1 &lt;- rep(0,nrow(List))\nd_AICC_2 &lt;- rep(0,nrow(List))\nd_AICC_3 &lt;- rep(0,nrow(List))\nd_AICC_4 &lt;- rep(0,nrow(List))\nd_AICC_5 &lt;- rep(0,nrow(List))\nLL_AICC_1 &lt;- rep(0,nrow(List))\nLL_AICC_2 &lt;- rep(0,nrow(List))\nLL_AICC_3 &lt;- rep(0,nrow(List))\nLL_AICC_4 &lt;- rep(0,nrow(List))\nLL_AICC_5 &lt;- rep(0,nrow(List))\nsumLL_AICC &lt;- rep(0,nrow(List))\nwi_AICC_1 &lt;- rep(0,nrow(List))\nwi_AICC_2 &lt;- rep(0,nrow(List))\nwi_AICC_3 &lt;- rep(0,nrow(List))\nwi_AICC_4 &lt;- rep(0,nrow(List))\nwi_AICC_5 &lt;- rep(0,nrow(List))\n\n11. The remainder of the code will be within a loop to run all animals in our dataset individually. The vector above will be populated with our results each time an animal has finished running through all the code\n\nfor(i in 1:nrow(List)) { \n\ncoords&lt;-read.table(as.character(List[i,]),sep=\"\\t\",header=T)\ncoords$DT&lt;-as.POSIXct(coords$NewDate, format=\"%Y-%m-%d %H:%M:%S\")\n\n##Make a data.frame of coordinates. Here the raw values are divided \n#by 1000 so that trajectories are calculated using km as the unit of measurement not meters\ncoord&lt;-data.frame((coords$Y),(coords$X))    \n#Make ltraj: a trajectory of all the relocations\nd2&lt;-as.ltraj(coord,coords$DT,\ncoords$YearBurst2,        #separate your data by individual.  \nburst=coords$YearBurst2, #burst is used to create subdivisions within an individual.\ntypeII=TRUE)       \n\n#you can now make your trajectory regular \n#first, create a reference start time\n#refda &lt;- strptime(\"00:00\", \"%H:%M\")   #all relocations should be altered \n#to occur at 30 seconds past each minute\n#firstly create a reference start time\nrefda &lt;- strptime(\"00:00:30\", \"%H:%M:%S\")\n\n#you can now make your trajectory regular, as radio tracks tend to lose \n#a few seconds / minutes with each relocation\n#firstly add \"NA\" for each missing location in your trajectory\nd3&lt;-setNA(d2,refda,\n#as.POSIXct(\"2007-06-01 06:00:00 EDT\"), #any time before earliest timedate\n10800,            #stating there should be a location every 3 hours\ntol=10800,        #how many time units to search each side of expected location\nunits=\"sec\")   #specifying the time units\n\n#you can now make your trajectory regular \n\n#NOTE: The refda and d3 code above was not run as in Papworth because \n#it results in too many relocations as \"NA\" that get removed below. Not \n#quite sure the reason behind it being included?\n\n#You can now make your trajectory regular \nd4&lt;-sett0(d3, refda, \n10800,                         #stating the interval at which relocations should be\ncorrection.xy =c(\"none\"),   #if \"cs\" performs location correction based on the \n#assumption the individual moves at a constant speed \ntol=10800,       #how many time units to search either side of an expected location\nunits = \"sec\")  #specifying the time units\n                              \n#to view your regular trajectory of points with NA's\nsummary(d4)\n#now calculating NSD for each point\ndatansd&lt;-NULL\nfor(n in 1:length(summary(d4)[,1])) #stating that NSD should be \n#calculated separately for each burst\n{\nnsdall&lt;-d4[[n]][,8]             #extracting the NSD for each location\nnsdtimeall&lt;-d4[[n]][,3]         #extracting the time for each location\nnsdtimestartzero&lt;-d4[[n]][,3]-d4[[n]][1,3]  \n#extracting the time since trip start for each location\nnsdid&lt;-rep(as.vector(summary(d4)[n,1]),\nlength.out=summary(d4)[n,3])     \n#extracting the individual associated with each location\nnsdtrip&lt;-rep(as.vector(summary(d4)[n,2]),length.out=summary(d4)[n,3])\n#extracting the trip associated with each location\ndatansd1&lt;-data.frame(nsdall,nsdtimeall,nsdtimestartzero,nsdid,nsdtrip)                  \n#joining all these variables together in a data frame\ndatansd&lt;-rbind(datansd,datansd1)                                                        \n#joining all the data frames together\n}\ndatansd$zero1&lt;-as.numeric(unclass(datansd$nsdtimestartzero))                            \n# making seconds since trip start numeric\ndatansd$zerostart&lt;-datansd$zero1/60                                                     \n#changing the time since trip start from seconds to minutes\ndatansd$minslitr2&lt;-as.numeric(strftime(as.POSIXlt(datansd$nsdtimeall),\nformat=\"%M\"))     \n#making a vector of the hour of the day a location occured\ndatansd$hdaylitr2&lt;-as.numeric(strftime(as.POSIXlt(datansd$nsdtimeall),\nformat=\"%H\"))     \n#making a vector of the minute in an hour a location occured\ndatansd$minsday&lt;-((datansd$hdaylitr2*60)+datansd$minslitr2)                             \n#calculating the minute in the day a location occured\n\nsummary(datansd)\ndatansd1&lt;-na.omit(datansd)            #remove NA's\n\ndatansd1$coordinates&lt;-coord           #add the coordinates for each point\n#you now have the dataframe you need (datansd) to start analysis\n\n#NSD \n#table(datansd1$nsdid)\n\n#Now you can start modelling NSD using nlme. \n#Equations are from Bunnefeld at al (2011) A model-driven approach to quantify migration \n#patterns:individual, regional and yearly differences. \n#Journal of Animal Ecology 80:466-476\n\n#First we are going to model the data using nls, a least squares method,\n#the simplest method and first method in Bunnefeld et al. 2011 (i.e., MIGRATION)\n#that uses a double sigmoid or s-shaped function. \n\n###########################\n##\n##  MIGRATION\n##\n###########################\n\nm1&lt;-tryCatch(nls(nsdall ~  asym /(1+exp((xmidA-zerostart)/scale1)) + \n(-asym / (1 + exp((xmidB-zerostart)/scale2))), #Equation 1 in Bunnefeld et al. 2011\nstart = c(asym=15000000,xmidA=200000,xmidB=450000,scale1=1000,scale2=1000)                  \n#these are the starting values for each parameter of the equation \n,data=na.omit(datansd1)),error=function(e)99999)   #this is the data\n#summary(m1)        #this will print a summary of the converged model\n#NOTE: The error function is simply to prevent the loop from crashing \n#if model does not converge\n\n###########################\n##\n##  MIXED MIGRATORY\n##\n###########################\n\nm2 &lt;-tryCatch(nls(nsdall ~  asymA /(1+exp((xmidA-zerostart)/scale1)) + \n(-asymB / (1 + exp((xmidB-zerostart)/scale2))), #Equation 2 in Bunnefeld et al. 2011\nstart = c(asymA=15000000,asymB=10000000, xmidA=200000,xmidB=450000,scale1=1000,scale2=1000)                  \n#these are the starting values for each parameter of the equation \n,data=na.omit(datansd1)),error=function(e)99999)   #this is the data \n#summary(m2)\n\n###########################\n##\n##  DISPERSAL\n##\n###########################\n\nm3 &lt;-tryCatch(nls(nsdall ~  asym /(1+exp((xmid-zerostart)/scale)),\nstart = c(asym=15000000,xmid=200000,scale=1000)#Equation 3 in Bunnefeld et al. 2011                  \n#these are the starting values for each parameter of the equation \n,data=na.omit(datansd1)),error=function(e)99999)   #this is the data\n#summary(m3)        \n\n###########################\n##\n## HOME RANGE\n##\n###########################\n\nm4 &lt;- tryCatch(nls(nsdall ~ intercept, data=na.omit(datansd1),start = list(intercept = 0)),\n  error=function(e)99999) #Equation 4 in Bunnefeld et al. 2011\n#where c is a constant\n#summary(m4)\n\n###########################\n##\n## NOMADIC\n##\n###########################\n\nm5 &lt;- tryCatch(nls(nsdall ~ beta*zerostart,start=c(beta=1), data=na.omit(datansd1)),\n  error=function(e)99999) #Equation 5 in Bunnefeld et al. 2011 where beta is a constant \n#and t the number of days since initial start date (i.e., 1 June of each year)\n#summary(m5)\n\n#Below we are going to set up the AIC table \nID[i] &lt;- paste(unique(as.factor(datansd$nsdid)))\nLOCS[i] &lt;- nrow(coords)\nMIGR[i] &lt;- print(tryCatch(AIC(m1),error=function(e)0))\nMIXM[i] &lt;- print(tryCatch(AIC(m2),error=function(e)0))\nDISP[i] &lt;- print(tryCatch(AIC(m3),error=function(e)0))\nHORA[i] &lt;- print(tryCatch(AIC(m4),error=function(e)0))\nNOMA[i] &lt;- print(tryCatch(AIC(m5),error=function(e)0))\n\nAICC_1[i] &lt;- print(tryCatch(AIC(m1),error=function(e)99999))\nAICC_2[i] &lt;- print(tryCatch(AIC(m2),error=function(e)99999))\nAICC_3[i] &lt;- print(tryCatch(AIC(m3),error=function(e)99999))\nAICC_4[i] &lt;- print(tryCatch(AIC(m4),error=function(e)99999))\nAICC_5[i] &lt;- print(tryCatch(AIC(m5),error=function(e)99999))\n\nminAIC[i] &lt;- min(AICC_1[i],AICC_2[i],AICC_3[i],AICC_4[i],AICC_5[i])\n\nd_AICC_1[i] &lt;- (AICC_1[i] - minAIC[i])\nd_AICC_2[i] &lt;- (AICC_2[i] - minAIC[i])\nd_AICC_3[i] &lt;- (AICC_3[i] - minAIC[i])\nd_AICC_4[i] &lt;- (AICC_4[i] - minAIC[i])\nd_AICC_5[i] &lt;- (AICC_5[i] - minAIC[i])\n\nLL_AICC_1[i] &lt;- exp(-0.5*d_AICC_1[i])\nLL_AICC_2[i] &lt;- exp(-0.5*d_AICC_2[i])\nLL_AICC_3[i] &lt;- exp(-0.5*d_AICC_3[i])\nLL_AICC_4[i] &lt;- exp(-0.5*d_AICC_4[i])\nLL_AICC_5[i] &lt;- exp(-0.5*d_AICC_5[i])\n\nsumLL_AICC[i] &lt;- sum(LL_AICC_1[i],LL_AICC_2[i],LL_AICC_3[i],LL_AICC_4[i],LL_AICC_5[i])\n\nwi_AICC_1[i] &lt;- LL_AICC_1[i]/sumLL_AICC[i]\nwi_AICC_2[i] &lt;- LL_AICC_2[i]/sumLL_AICC[i]\nwi_AICC_3[i] &lt;- LL_AICC_3[i]/sumLL_AICC[i]\nwi_AICC_4[i] &lt;- LL_AICC_4[i]/sumLL_AICC[i]\nwi_AICC_5[i] &lt;- LL_AICC_5[i]/sumLL_AICC[i]\n\nfilename&lt;-paste(substr(List[i,],1,8),\"png\",sep=\".\")\n#NOTE:Numbers after \"List[i,] need to encompass possible lengths of output name \n#(i.e., D19.txt is 6 characters)\npng(filename,height=20,width=30,units=\"cm\",res=600)\n#graphical exploration of the data will help you find sensible starting values \n#for each of the parameters asym, xmidA, xmidB, scale1 and scale2. \n#to graph nsd against time, use:\nxyplot(nsdall~zerostart|nsdtrip,data=datansd)\n#str(nsdtest)\n#now plot the data with the predicted curve  \nnsdplot &lt;- xyplot(nsdall ~ zerostart/3600, data=datansd1,\ncol=\"grey\",    #color for the observed locations\ntype='b',      # 'b' shows the locations as dots, with a line connecting \n#successive locations. Can also be 'p' for just the locations, or 'l' for just \n#the line between locations\nylab=expression(paste('Net squared displacement ',' ', (km^2))), #y axis label\nxlab=\"Hours after trip start\")\n\nplot(nsdplot)\n\ndev.off()\n}\n\n#Create table of AIC values with lower AIC identifying best model\nRESULT&lt;-cbind(ID,LOCS,MIGR,MIXM,DISP,HORA,NOMA)\ncolnames(RESULT)&lt;- c(\"ID\",\"LOCS\",\"MIGR\",\"MIXM\",\"DISP\",\"HORA\",\"NOMA\")\n#write.table(RESULT,\"OUT_NSDresults.txt\",sep=\"\\t\")\n\n#Create table of raw values to calculate AICweights\nMigratory &lt;- rbind(ID,AICC_1,d_AICC_1,LL_AICC_1,wi_AICC_1)\nMixedMig &lt;- rbind(ID,AICC_2,d_AICC_2,LL_AICC_2,wi_AICC_2)\nDisperser &lt;- rbind(ID,AICC_3,d_AICC_3,LL_AICC_3,wi_AICC_3)\nHomeRange &lt;- rbind(ID,AICC_4,d_AICC_4,LL_AICC_4,wi_AICC_4)\nNomadic &lt;- rbind(ID,AICC_5,d_AICC_5,LL_AICC_5,wi_AICC_5)\nRESULT2 &lt;- rbind(Migratory,MixedMig,Disperser,HomeRange,Nomadic)\nwrite.csv(RESULT2,\"OUT_NSDresults.csv\")"
  },
  {
    "objectID": "TrajDynScript.html",
    "href": "TrajDynScript.html",
    "title": "\n21  Movement Trajectory Animation\n",
    "section": "",
    "text": "1. Open the script “TrajDynScript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatLT)\nlibrary(chron)\nlibrary(sp)\nlibrary(sf)\nlibrary(terra)\nlibrary(FedData)\nlibrary(stars)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. We will be using the mule deer dataset for this exercise\n\nmuleys &lt;-read.csv(\"data/DCmuleysedited.csv\", header=T)\n\n#CODE FOR AN INDIVIDUAL ANIMAL\nmuley16 &lt;- subset(muleys, id==\"D16\")\nmuley16$id &lt;- factor(muley16$id)\nsummary &lt;- table(muley16$UTM_Zone,muley16$id)\n\n#Sort data to address error in code and then look at first 10 records of data to confirm\nmuley16 &lt;- muley16[order(muley16$GPSFixTime),]\n\n######################################################\n## Example of a trajectory of type II (time recorded)\n### Conversion of the date to the format POSIX\n#Needs to be done to get proper digits of date into R then POSIXct\n#uses library(chron)\nda &lt;- as.character(muley16$GPSFixTime)\nda &lt;- as.POSIXct(strptime(muley16$GPSFixTime,format=\"%Y.%m.%d %H:%M:%S\"))\n#Attach da to muley15\nmuley16$da &lt;- da\n\ntimediff &lt;- diff(muley16$da)\nmuley16 &lt;-muley16[-1,]\nmuley16$timediff &lt;-as.numeric(abs(timediff)) \n\n#Clean up muley15 for outliers\nnewmuleys &lt;-subset(muley16, muley16$X &gt; 599000 & muley16$X &lt; 705000 & muley16$Y &gt; 4167000\n    & muley16$timediff &lt; 14401)\nmuley16 &lt;- newmuleys\n\ncoords &lt;- st_as_sf(muley16, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous plot\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\n#Give dataset projection information then project to Albers\ndeer.albers &lt;- st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers),axes=T)\n\n\n\n\n5. Need to create a movement trajectory as we did in previous exercises\n\nltr.albers &lt;- as.ltraj(st_coordinates(deer.albers),deer.albers$da,id=deer.albers$id)\n\n6. Now let’s have a little fun with these mule deer locations and explore\n\n#Get nlcd with FedDdata package using deer locations\nnlcd &lt;- get_nlcd(template=deer.albers, year = 2019, label = 'nlcd',force.redo = T)\nplot(nlcd)\nplot(st_geometry(deer.albers),add=T, col=\"red\",pch=16)\n\n7. Code below is used to just zoom in on all of our locations and crop within it so just select a study area around your locations using the drawExtent function below\n\nplot(st_geometry(deer.albers))\ne &lt;- draw()#click on top left of crop box and bottom right of crop box to create\n#a polygon around all locations\nnewclip &lt;- crop(nlcd,e)\nplot(newclip)\nplot(st_geometry(deer.albers),add=T, col=\"red\")\nnewclip.df &lt;- as.data.frame(newclip, xy=TRUE)\nnewclip.xy &lt;- data.frame(x=newclip.df$x,y=newclip.df$y)\nvegspdf &lt;- SpatialPixelsDataFrame(newclip.xy,newclip.df)\nplot(ltr.albers, spixdf=vegspdf)\n\n8. Or zoom in even closer on a few areas by repeating the drawExtent function above to a specific area\n\ne2 &lt;- draw()\nnewclip2 &lt;- crop(nlcd,e2)\nplot(newclip2)\nplot(st_geometry(deer.albers),add=T, col=\"red\")\nzoom.df &lt;- as.data.frame(newclip2, xy=TRUE)\nzoom.xy &lt;- data.frame(x=zoom.df$x,y=zoom.df$y)\nzoom.spdf &lt;- SpatialPixelsDataFrame(zoom.xy,zoom.df)\n\nzoom.ltr &lt;- st_crop(deer.albers,newclip2)\nltr.zoom &lt;- as.ltraj(st_coordinates(zoom.ltr),zoom.ltr$da,id=zoom.ltr$id)\nplot(ltr.zoom, spixdf=zoom.spdf)\n\n9. We are first going to randomly select one location day for the calendar year our deer is monitored. This will result in fewer locations to plot overall. Then create an ltraj of the subset locations or all locations if you skipped lines 112-120 below\n\ndeer.albers$Year &lt;- format(deer.albers$da, \"%Y\")\ndeer.albers &lt;- subset(deer.albers,deer.albers$Year != \"NA\")\ndeer.albers$YearBurst &lt;- c(paste(deer.albers$id,deer.albers$Year,sep=\"_\"))\ndeer.albers$YearBurst &lt;- as.factor(deer.albers$YearBurst)\nrange(deer.albers$da)\n\ndeer.albers$subDate &lt;-  as.POSIXct(as.factor(deer.albers$da), format=\"%Y-%m-%d\", tz=\"EST\")\ndeer.albers$Oneperday &lt;- paste(deer.albers$YearBurst,deer.albers$subDate,sep=\"_\")\ndeer.albers2 &lt;- do.call(rbind, lapply(split(deer.albers,deer.albers$Oneperday) , \nfunction(deer.albers) deer.albers[sample(nrow(deer.albers), 1) , ] ))\nltr.year &lt;- as.ltraj(coordinates(deer.albers2),deer.albers2$da,id=deer.albers2$id)\n\n10. Now we can use the function to create movements of our deer over the landscape\n\nwindows() #NOTE: a new window is needed in Rstudio\n#Line of code below plots trajectory one location at a time\ntrajdyn(ltr.year,spixdf=vegspdf)"
  },
  {
    "objectID": "HrefScript.html#kernel-density-estimation-kde-with-reference-bandwidth-selection-href",
    "href": "HrefScript.html#kernel-density-estimation-kde-with-reference-bandwidth-selection-href",
    "title": "\n22  Home Range Estimation\n",
    "section": "\n22.1 Kernel Density Estimation (KDE) with reference bandwidth selection (href)",
    "text": "22.1 Kernel Density Estimation (KDE) with reference bandwidth selection (href)\nIn KDE, a kernel distribution (i.e. a three-dimensional hill or kernel) is placed on each telemetry location. The height of the hill is determined by the bandwidth of the distribution, and many distributions and methods are available (e.g. fixed versus adaptive, univariate versus bivariate bandwidth). We will focus here on “fixed kernel” but will alter the bandwidth selection. Datasets for avian and mammalian species can include as many as 10,000 locations and only the reference or default bandwidth (href) was able to produce KDE in both Home Range Tools and adehabitat or adehabitatHR (Calenge 2007, 2011). Estimation with (href) typically is not reliable for use on multimodal datasets because it results in over-smoothing of home ranges and multimodal distribution of locations is typical for most species (Worton 1995, Seaman et al. 1999).\n1. Open the script “HrefScript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatHR)\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Using the adehabitatHR package requires dataset to be formatted appropriately as a SpatialPointsDataFrame using the sp package\n\n#Let's select only one animal\npanther&lt;-read.csv(\"data/pantherjitter.csv\", header=T)\npanther &lt;- subset(panther, panther$CatID == \"143\")\npanther$CatID &lt;- factor(panther$CatID)\nloc &lt;- data.frame(\"x\"=panther$X,\"y\"=panther$Y)\ncats &lt;- SpatialPointsDataFrame(loc,panther)\nproj4string(cats) &lt;- \"+proj=utm +zone=17 +ellps=WGS84\"\nudbis &lt;- kernelUD(cats[,1], h = \"href\")\nimage(udbis)\n\n\n\nver &lt;- getverticeshr(udbis, standardize = FALSE)\nver50 &lt;- getverticeshr(udbis, percent=50)\nver80 &lt;- getverticeshr(udbis, percent=80)\nver90 &lt;- getverticeshr(udbis, percent=90)\nver95 &lt;- getverticeshr(udbis, percent=95)\nver99 &lt;- getverticeshr(udbis, percent=99)\nver\n\nObject of class \"SpatialPolygonsDataFrame\" (package sp):\n\nNumber of SpatialPolygons:  1\n\nVariables measured:\n     id     area\n143 143 97882.18\n\nplot(ver99, col=\"grey\",axes=T);plot(ver95, add=T);plot(ver90, add=T);plot(ver80, add=T)\nplot(ver50, add=T)\npoints(cats)"
  },
  {
    "objectID": "HlscvScript.html",
    "href": "HlscvScript.html",
    "title": "\n23  Kernel Density Estimation (KDE) with least squares cross validation (lscv)\n",
    "section": "",
    "text": "Both the least squares cross-validation (hlscv) and bias crossed validation (hbcv) have been suggested instead of href in attempts to prevent over-smoothing of KDE (Rodgers and Kie 2010). However, (hlscv) and (hbcv) have been minimally evaluated on GPS datasets because previous literature only evaluated datasets collected on VHF sampling protocols or simulated data that included at most 1,000 locations. Least-squares cross validation, suggested as the most reliable bandwidth for KDE was considered better than plug-in bandwidth selection (hplug-in; for description see section 3.3) at identifying distributions with tight clumps but risk of failure increases with hlscv when a distribution has a “very tight cluster of points” (Gitzen et al. 2006, Pellerin et al. 2008, Walter et al. 2011).\n1. Open the script “HlscvScript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatHR)\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Now we can run fixed kernel home range with hlscv\n\npanther &lt;-read.csv(\"data/pantherjitter.csv\", header=T)\nloc &lt;- data.frame(\"x\"=panther$X,\"y\"=panther$Y)\npantherspdf &lt;- SpatialPointsDataFrame(loc,panther)\nplot(pantherspdf, col=pantherspdf$CatID)\n\n\n\nproj4string(pantherspdf) &lt;- \"+proj=utm +zone=17 +ellps=WGS84\"\n\n\nNote that regardless of change hlim or extent, LSCV will not converge for these animals and defaults to href smoothing.\n\n\n## Example of estimation using LSCV\nudbis2 &lt;- kernelUD(pantherspdf[,1], h = \"LSCV\", hlim = c(10,50),extent=1)\nimage(udbis2)\n\n6. So we can try a trick here. I believe LSCV is a poor estimator with GPS locations being too numerous and very close together compared to traditional VHF datasets which LSCV were originally evaluated. So we will jitter locations 50 meters from their original location and try again.\n\npanther$jitterX &lt;- jitter(panther$X, factor=500)\npanther$jitterY &lt;- jitter(panther$Y, factor=500)\nlocjitter &lt;- data.frame(\"x\"=panther$jitterX,\"y\"=panther$jitterY)\njitterspdf &lt;- SpatialPointsDataFrame(locjitter,panther)\nproj4string(jitterspdf) &lt;- \"+proj=utm +zone=17 +ellps=WGS84\"\nplot(jitterspdf, col=pantherspdf$CatID)\n\n\n\nudbis3 &lt;- kernelUD(jitterspdf[,1], h = \"LSCV\")#, hlim = c(1, 5),extent=1)\nimage(udbis3)\n\n\n\n\n7. Now rerun with jitter factor = 100 instead of 50 and see what happens? Then rerun with jitter factor = 500 instead of 100 and see what happens?"
  },
  {
    "objectID": "Panther_All4.html",
    "href": "Panther_All4.html",
    "title": "\n24  KDE with plug-in bandwidth selection (hplug-in)\n",
    "section": "",
    "text": "Here we will estimate home range using the previous estimators on the same animal. We will conclude with estimating home range using the hplug-in and Brownian Bridge Movement Models (BBMM) for comparison to kernel density estimators.\n1. Open the script “Panther_All4.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatHR)\nlibrary(ks)\nlibrary(stringr)\n#library(BBMM)\n#library(maptools)\nlibrary(PBSmapping)\nlibrary(move)\nlibrary(sf)\nlibrary(terra)\n#library(ctmm)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. We will use an abbreviated dataset to save processing time and the code will also output shapefiles of home ranges\n\npanther&lt;-read.csv(\"data/pantherjitter.csv\",header=T)\npanther$CatID &lt;- as.factor(panther$CatID)\ncat143 &lt;- subset(panther, panther$CatID == \"143\")\ncat143$CatID &lt;- droplevels(cat143$CatID)\n\n5. We will start by running KDE with href similar to exercise 4.1.\n\nloc &lt;- data.frame(\"x\"=cat143$X,\"y\"=cat143$Y)\n\ncats &lt;- SpatialPointsDataFrame(loc,cat143)\nproj4string(cats) &lt;- \"+proj=utm +zone=17 +ellps=WGS84\"\nudbis &lt;- kernelUD(cats[,1], h = \"href\")\n\nver &lt;- getverticeshr(udbis, unin = \"m\", unout = \"km2\", standardize=TRUE)\nver50 &lt;- getverticeshr(udbis, percent=50,unin = \"m\", unout = \"km2\", standardize=TRUE)\nver80 &lt;- getverticeshr(udbis, percent=80,unin = \"m\", unout = \"km2\", standardize=TRUE)\nver90 &lt;- getverticeshr(udbis, percent=90,unin = \"m\", unout = \"km2\", standardize=TRUE)\nver95 &lt;- getverticeshr(udbis, percent=95,unin = \"m\", unout = \"km2\", standardize=TRUE)\nver99 &lt;- getverticeshr(udbis, percent=99,unin = \"m\", unout = \"km2\", standardize=TRUE)\n\nplot(ver99,main=\"KDE-HREF Bandwidth\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(ver95, lty=6, add=TRUE)\nplot(ver90, add=TRUE)\nplot(ver80, add=TRUE)\nplot(ver50, col=\"red\",add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\n\n\n\n6. Next we will run KDE with hlscv similar to exercise 4.2.\n\nudbis2 &lt;- kernelUD(cats[,1], h = \"LSCV\")\n#Notice the error\n# The algorithm did not converge \n# within the specified range of hlim: try to increase it\n## Example of estimation using LSCV\ncat143$jitterX &lt;- jitter(cat143$X, factor=500)\ncat143$jitterY &lt;- jitter(cat143$Y, factor=500)\nlocjitter &lt;- data.frame(\"x\"=cat143$jitterX,\"y\"=cat143$jitterY)\njitterspdf &lt;- SpatialPointsDataFrame(locjitter,cat143)\nproj4string(jitterspdf) &lt;- \"+proj=utm +zone=17 +ellps=WGS84\"\nudbis3 &lt;- kernelUD(jitterspdf[,1], h = \"LSCV\")#, hlim = c(100, 500),extent=1)\n\n#Now rerun with jitter factor = 100 then 500 instead of 50 and see what happens?\n\nver2 &lt;- getverticeshr(udbis3, unin = \"m\", unout = \"km2\", standardize=TRUE)\n#Or individually by isopleth\nver2_50 &lt;- getverticeshr(udbis3, percent=50,unin = \"m\", unout = \"km2\", standardize=TRUE)\nver2_80 &lt;- getverticeshr(udbis3, percent=80,unin = \"m\", unout = \"km2\", standardize=TRUE)\nver2_90 &lt;- getverticeshr(udbis3, percent=90,unin = \"m\", unout = \"km2\", standardize=TRUE)\nver2_95 &lt;- getverticeshr(udbis3, percent=95,unin = \"m\", unout = \"km2\", standardize=TRUE)\nver2_99 &lt;- getverticeshr(udbis3, percent=99,unin = \"m\", unout = \"km2\", standardize=TRUE)\n\nplot(ver2_99,main=\"KDE-LSCV Bandwidth\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(ver2_95, lty=6, add=TRUE)\nplot(ver2_90, add=TRUE)\nplot(ver2_80, add=TRUE)\nplot(ver2_50, col=\"red\",add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\n\n\n\n7. Then we will run KDE with hplug-in similar to exercise 4.3.\n\n##Get only the coordinates\nloc &lt;- data.frame(\"x\"=cat143$X, \"y\"=cat143$Y)\n\n##Make SpatialPointsDataFrame using the XY, attributes, and projection\nspdf &lt;- SpatialPointsDataFrame(loc, cat143)\nproj4string(spdf) &lt;- \"+proj=utm +zone=17 +ellps=WGS84\"\n#Calculate the bandwidth matrix to use later in creating the KDE\nHpi1 &lt;- Hpi(x = loc)\nHpi1\n\n          [,1]      [,2]\n[1,] 1917158.8  277154.8\n[2,]  277154.8 1030023.5\n\n##write out the bandwidth matrix to a file as you might want to refer to it later\n#write.table(Hpi1, paste(\"hpivalue_\", \"143\", \".txt\", sep=\"\"), row.names=FALSE,sep=\"\\t\")\n\n##Create spatial points from just the xy?s\nloc.pts &lt;- SpatialPoints(loc)\nproj4string(loc.pts) &lt;- \"+proj=utm +zone=17 +ellps=WGS84\"\n\n##For home range calculations, ##some packages require evaluation points (ks) while others\n##require grid as spatial pixels (adehabitatHR).\n\n##Set the expansion value for the grid and get the bbox from the SpatialPointsDataFrame\nexpandValue &lt;- 5000 #This value is the amount to add on each side of the bbox.\n#Change to 5000 if error occurs at 99% ud\nboundingVals &lt;- spdf@bbox\n\n##Get the change in x and y and adjust using expansion value\ndeltaX &lt;- as.integer(((boundingVals[1,2]) - (boundingVals[1,1])) + (2*expandValue))\ndeltaY &lt;- as.integer(((boundingVals[2,2]) - (boundingVals[2,1])) + (2*expandValue))\n\n##100 meter grid for data in this exercise\ngridRes &lt;- 100\ngridSizeX &lt;- deltaX / gridRes\ngridSizeY &lt;- deltaY / gridRes\n##Offset the bounding coordinates to account for the additional area\nboundingVals[2,1] &lt;- boundingVals[2,1] - expandValue\nboundingVals[2,2] &lt;- boundingVals[2,2] + expandValue\nboundingVals[1,1] &lt;- boundingVals[1,1] - expandValue\nboundingVals[1,2] &lt;- boundingVals[1,2] + expandValue\n\n##Grid Topology object is basis for sampling grid (offset, cellsize, dim)\ngridTopo &lt;- GridTopology((boundingVals[,1]), c(gridRes,gridRes),c(gridSizeX,gridSizeY))\n\n##Using the Grid Topology and projection create a SpatialGrid class\nsampGrid &lt;- SpatialGrid(gridTopo)\nproj4string(sampGrid) &lt;- \"+proj=utm +zone=17 +ellps=WGS84\"\n##Cast over to Spatial Pixels\nsampSP &lt;- as(sampGrid, \"SpatialPixels\")\n\n##convert the SpatialGrid class to a raster\nsampRaster &lt;- rast(sampGrid)\n\n##set all the raster values to 1 such as to make a data mask\nsampRaster[] &lt;- 1\n\n##Get the center points of the mask raster with values set to 1\nevalPoints &lt;- xyFromCell(sampRaster, 1:ncell(sampRaster))\n\n##Create the KDE using the evaluation points\nhpikde &lt;- kde(x=loc, H=Hpi1, eval.points=evalPoints)\n\n##Create a template raster based upon the mask and then assign the values from the kde\n#to the template\nhpikde.raster &lt;- rast(sampRaster)\n\nhpikde.raster &lt;- setValues(hpikde.raster,hpikde$estimate)\n\n##Lets take this raster and put it back into an adehabitatHR object. This is convenient to use other adehabitatHR capabilities such as overlap indices or percent volume contours\nhpikde.raster.df &lt;- as.data.frame(hpikde.raster, xy=FALSE)\nhpikde.raster.df2 &lt;- as.data.frame(hpikde.raster, xy=TRUE)\nhpikde.raster.xy &lt;- data.frame(x=hpikde.raster.df2$x,y=hpikde.raster.df2$y)\n\n##Cast over to SPxDF\nhpikde.px &lt;- SpatialPixelsDataFrame(hpikde.raster.xy,hpikde.raster.df)\n\n##create new estUD using the SPxDF\nhpikde.ud &lt;- new(\"estUD\", hpikde.px)\n\n##Assign values to a couple slots of the estUD\nhpikde.ud@vol = FALSE\nhpikde.ud@h$meth = \"Plug-in Bandwidth\"\n\n##Convert the UD values to volume using getvolumeUD from adehabitatHR and cast over to a raster\nhpikde.ud.vol &lt;- getvolumeUD(hpikde.ud, standardize=TRUE)\nhpikde.ud.vol.raster &lt;- rast(hpikde.ud.vol)\n\n##Here we generate volume contours using the UD\nhpikde.50vol &lt;- getverticeshr(hpikde.ud, percent = 50)#,unin = \"m\", unout = \"km2\")\nhpikde.80vol &lt;- getverticeshr(hpikde.ud, percent = 80,unin = \"m\", unout = \"km2\")\nhpikde.90vol &lt;- getverticeshr(hpikde.ud, percent = 90,unin = \"m\", unout = \"km2\")\nhpikde.95vol &lt;- getverticeshr(hpikde.ud, percent = 95,unin = \"m\", unout = \"km2\")\nhpikde.99vol &lt;- getverticeshr(hpikde.ud, percent = 99,unin = \"m\", unout = \"km2\")\n\nplot(hpikde.99vol,main=\"KDE-Plug-in Bandwidth\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(hpikde.95vol, lty=6, add=TRUE)\nplot(hpikde.90vol, add=TRUE)\nplot(hpikde.80vol, add=TRUE)\nplot(hpikde.50vol,col=\"red\", add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\n\n\n\n8. We will finish by running BBMM.\n\n#To run BBMM we first need to use the original dataset to calculate time between locations\npanther$NewTime &lt;- str_pad(panther$TIMEET2,4, pad= \"0\")\npanther$NewDate &lt;- paste(panther$DateET2,panther$NewTime)\n#Used to sort data in code below for all deer\npanther$DT &lt;- as.POSIXct(strptime(panther$NewDate, format='%Y %m %d %H%M'))\n#Sort Data\npanther &lt;- panther[order(panther$CatID, panther$DT),]\n#TIME DIFF NECESSARY IN BBMM CODE\ntimediff &lt;- diff(panther$DT)*60\n# remove first entry without any difference \npanther &lt;- panther[-1,] \npanther$timelag &lt;-as.numeric(abs(timediff))\n\ncat143&lt;-subset(panther, panther$CatID == \"143\")\ncat143 &lt;- cat143[-1,] #Remove first record with wrong timelag\ncat143$CatID &lt;- factor(cat143$CatID)\nBBMM = brownian.bridge(x=cat143$X, y=cat143$Y, time.lag=cat143$timelag, location.error=34,\ncell.size=100)\n\nbbmm.summary(BBMM)\n\ncontours = bbmm.contour(BBMM, levels=c(seq(50, 90, by=10), 95, 99), locations=cat143, plot=FALSE)\n\n#Create a new data frame for all contours\nbbmm.contour = data.frame(x = BBMM$x, y = BBMM$y, probability = BBMM$probability)\n\n# Pick a contour for export as Ascii\nbbmm.50 = bbmm.contour[bbmm.contour$probability &gt;= contours$Z[1],]\nbbmm.50$in.out &lt;- 1 \n\nbbmm.50 &lt;-bbmm.50[,-3]\n# Output ascii file for cells within specified contour.\nm50 = SpatialPixelsDataFrame(points = bbmm.50[c(\"x\", \"y\")], data=bbmm.50)\n#m50.g = as(m50, \"SpatialGridDataFrame\")\n#writeAsciiGrid(m50.g, \"50ContourInOut.asc\", attr=ncol(bbmm.50))\n\n# Convert to SpatialPolygonsDataFrame and export as ESRI Shapefile\nshp.50 &lt;- as(m50, \"SpatialPolygonsDataFrame\")\nmap.ps50 &lt;- SpatialPolygons2PolySet(shp.50)\ndiss.map.50 &lt;- joinPolys(map.ps50, operation = 'UNION')\ndiss.map.50 &lt;- as.PolySet(diss.map.50, projection = 'UTM', zone = '17')\ndiss.map.p50 &lt;- PolySet2SpatialPolygons(diss.map.50, close_polys = TRUE)\ndata50 &lt;- data.frame(PID = 1)\ndiss.map.p50 &lt;- SpatialPolygonsDataFrame(diss.map.p50, data = data50)\n# writeOGR(diss.map.p50, dsn = \".\", layer=\"contour50\", driver = \"ESRI Shapefile\")\n# map.50 &lt;- readOGR(dsn=\".\", layer=\"contour50\")\n# plot(map.50)\n\n# Pick a contour for export as Ascii\nbbmm.80 = bbmm.contour[bbmm.contour$probability &gt;= contours$Z[4],]\nbbmm.80$in.out &lt;- 1 \n\nbbmm.80 &lt;-bbmm.80[,-3]\n# Output ascii file for cells within specified contour.\nm80 = SpatialPixelsDataFrame(points = bbmm.80[c(\"x\", \"y\")], data=bbmm.80)\nm80.g = as(m80, \"SpatialGridDataFrame\")\n#writeAsciiGrid(m80.g, \"80ContourInOut.asc\", attr=ncol(bbmm.80))\n\n# Convert to SpatialPolygonsDataFrame and export as ESRI Shapefile\nshp.80 &lt;- as(m80, \"SpatialPolygonsDataFrame\")\nmap.ps80 &lt;- SpatialPolygons2PolySet(shp.80)\ndiss.map.80 &lt;- joinPolys(map.ps80, operation = 'UNION')\ndiss.map.80 &lt;- as.PolySet(diss.map.80, projection = 'UTM', zone = '17')\ndiss.map.p80 &lt;- PolySet2SpatialPolygons(diss.map.80, close_polys = TRUE)\ndata80 &lt;- data.frame(PID = 1)\ndiss.map.p80 &lt;- SpatialPolygonsDataFrame(diss.map.p80, data = data80)\n# writeOGR(diss.map.p80, dsn = \".\", layer=\"contour80\", driver = \"ESRI Shapefile\")\n# map.80 &lt;- readOGR(dsn=\".\", layer=\"contour80\")\n# plot(map.80)\n\n# Pick a contour for export as Ascii\nbbmm.90 = bbmm.contour[bbmm.contour$probability &gt;= contours$Z[5],]\nbbmm.90$in.out &lt;- 1 \n\nbbmm.90 &lt;-bbmm.90[,-3]\n# Output ascii file for cells within specified contour.\nm90 = SpatialPixelsDataFrame(points = bbmm.90[c(\"x\", \"y\")], data=bbmm.90)\n#m90.g = as(m90, \"SpatialGridDataFrame\")\n#writeAsciiGrid(m90.g, \"90ContourInOut.asc\", attr=ncol(bbmm.90))\n\n# Convert to SpatialPolygonsDataFrame and export as ESRI Shapefile\nshp.90 &lt;- as(m90, \"SpatialPolygonsDataFrame\")\nmap.ps90 &lt;- SpatialPolygons2PolySet(shp.90)\ndiss.map.90 &lt;- joinPolys(map.ps90, operation = 'UNION')\ndiss.map.90 &lt;- as.PolySet(diss.map.90, projection = 'UTM', zone = '17')\ndiss.map.p90 &lt;- PolySet2SpatialPolygons(diss.map.90, close_polys = TRUE)\ndata90 &lt;- data.frame(PID = 1)\ndiss.map.p90 &lt;- SpatialPolygonsDataFrame(diss.map.p90, data = data90)\n# writeOGR(diss.map.p90, dsn = \".\", layer=\"contour90\", driver = \"ESRI Shapefile\")\n# map.90 &lt;- readOGR(dsn=\".\", layer=\"contour90\")\n# plot(map.90)\n\n# Pick a contour for export as Ascii\nbbmm.95 = bbmm.contour[bbmm.contour$probability &gt;= contours$Z[6],]\nbbmm.95$in.out &lt;- 1 \n\nbbmm.95 &lt;-bbmm.95[,-3]\n# Output ascii file for cells within specified contour.\nm95 = SpatialPixelsDataFrame(points = bbmm.95[c(\"x\", \"y\")], data=bbmm.95)\n#m95.g = as(m95, \"SpatialGridDataFrame\")\n#writeAsciiGrid(m95.g, \"95ContourInOut.asc\", attr=ncol(bbmm.95))\n\n# Convert to SpatialPolygonsDataFrame and export as ESRI Shapefile\nshp.95 &lt;- as(m95, \"SpatialPolygonsDataFrame\")\nmap.ps95 &lt;- SpatialPolygons2PolySet(shp.95)\ndiss.map.95 &lt;- joinPolys(map.ps95, operation = 'UNION')\ndiss.map.95 &lt;- as.PolySet(diss.map.95, projection = 'UTM', zone = '17')\ndiss.map.p95 &lt;- PolySet2SpatialPolygons(diss.map.95, close_polys = TRUE)\ndata95 &lt;- data.frame(PID = 1)\ndiss.map.p95 &lt;- SpatialPolygonsDataFrame(diss.map.p95, data = data95)\n# writeOGR(diss.map.p95, dsn = \".\", layer=\"contour95\", driver = \"ESRI Shapefile\")\n# map.95 &lt;- readOGR(dsn=\".\", layer=\"contour95\")\n# plot(map.95)\n\n# Pick a contour for export as Ascii\nbbmm.99 = bbmm.contour[bbmm.contour$probability &gt;= contours$Z[7],]\nbbmm.99$in.out &lt;- 1 \n\nbbmm.99 &lt;-bbmm.99[,-3]\n# Output ascii file for cells within specified contour.\nm99 = SpatialPixelsDataFrame(points = bbmm.99[c(\"x\", \"y\")], data=bbmm.99)\n# m99.g = as(m99, \"SpatialGridDataFrame\")\n# writeAsciiGrid(m99.g, \"99ContourInOut.asc\", attr=ncol(bbmm.99))\n\n# Convert to SpatialPolygonsDataFrame and export as ESRI Shapefile\nshp.99 &lt;- as(m99, \"SpatialPolygonsDataFrame\")\nmap.ps99 &lt;- SpatialPolygons2PolySet(shp.99)\ndiss.map.99 &lt;- joinPolys(map.ps99, operation = 'UNION')\ndiss.map.99 &lt;- as.PolySet(diss.map.99, projection = 'UTM', zone = '17')\ndiss.map.p99 &lt;- PolySet2SpatialPolygons(diss.map.99, close_polys = TRUE)\ndata99 &lt;- data.frame(PID = 1)\ndiss.map.p99 &lt;- SpatialPolygonsDataFrame(diss.map.p99, data = data99)\n#writeOGR(diss.map.p99, dsn = \".\", layer=\"contour99\", driver = \"ESRI Shapefile\")\n# map.99 &lt;- readOGR(dsn=\".\", layer=\"contour99\")\n# plot(map.99)\n\n9. Plot BBMM\n\nplot(diss.map.p99,main=\"Brownian Bridge Movement Model\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(diss.map.p95, lty=6, add=TRUE)\nplot(diss.map.p90, add=TRUE)\nplot(diss.map.p80, add=TRUE)\nplot(diss.map.p50,col=\"red\", add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\n10. We can add 4 estimators to the plot window to compare across estimators\n\npar(mfrow=c(2,2))\n\nplot(ver99,main=\"KDE-HREF Bandwidth\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(ver95, lty=6, add=TRUE)\nplot(ver90, add=TRUE)\nplot(ver80, add=TRUE)\nplot(ver50, col=\"red\",add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\nplot(ver2_99,main=\"KDE-LSCV Bandwidth\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(ver2_95, lty=6, add=TRUE)\nplot(ver2_90, add=TRUE)\nplot(ver2_80, add=TRUE)\nplot(ver2_50, col=\"red\",add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\nplot(hpikde.99vol,main=\"KDE-Plug-in Bandwidth\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(hpikde.95vol, lty=6, add=TRUE)\nplot(hpikde.90vol, add=TRUE)\nplot(hpikde.80vol, add=TRUE)\nplot(hpikde.50vol,col=\"red\", add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\n# plot(diss.map.p99,main=\"Brownian Bridge Movement Model\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\n# plot(diss.map.p95, lty=6, add=TRUE)\n# plot(diss.map.p90, add=TRUE)\n# plot(diss.map.p80, add=TRUE)\n# plot(diss.map.p50,col=\"red\", add=TRUE)\n# points(loc, pch=1, cex=0.5)\n\n11. We will quickly explore autocorrelated kernel density estimator. It was easier to convert our dataframe to a move object prior to creating a telemetry object required by package ctmm.\n\ncat.move &lt;- move(x=cat143$X, y=cat143$Y, time=as.POSIXct(cat143$NewDate,\n     format='%Y %m %d %H%M'), proj=utm.crs,data=cat143,\n     animal=cat143$CatID)\n#\n# cat143$timestamp &lt;- cat143$DT\n# cat143$x &lt;- cat143$X\n# cat143$y &lt;- cat143$Y\n\n# cat143$Date.lt &lt;- as.POSIXlt(strptime(cat143$NewDate, format='%Y %m %d %H%M'),tz=\"EDT\")\n#cat143 &lt;- cat143[c(22,23,20,24:25)]\ncat.telem &lt;- as.telemetry(cat.move,timeformat=timestamp, timezone=\"UTC\",projection=utm.crs, na.rm=\"row\",mark.rm=FALSE,keep=FALSE, drop=TRUE)\nGUESS &lt;- ctmm.guess(cat.telem,interactive=FALSE)\nGUESS2 &lt;- ctmm(tau=1)\nFIT &lt;- ctmm.fit(cat.telem,GUESS)\nFIT2 &lt;- ctmm.fit(cat.telem,GUESS2)\nUD &lt;- akde(cat.telem,FIT)\nUD2 &lt;- akde(cat.telem,FIT2)\nplot(cat.telem,UD=UD2)\n\n12. We can then plot them along with other estimators to compare\n\npar(mfrow=c(2,2))\n\nplot(ver99,main=\"KDE-HREF Bandwidth\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(ver95, lty=6, add=TRUE)\nplot(ver90, add=TRUE)\nplot(ver80, add=TRUE)\nplot(ver50, col=\"red\",add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\nplot(hpikde.99vol,main=\"KDE-Plug-in Bandwidth\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(hpikde.95vol, lty=6, add=TRUE)\nplot(hpikde.90vol, add=TRUE)\nplot(hpikde.80vol, add=TRUE)\nplot(hpikde.50vol,col=\"red\", add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\nplot(diss.map.p99,main=\"Brownian Bridge Movement Model\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(diss.map.p95, lty=6, add=TRUE)\nplot(diss.map.p90, add=TRUE)\nplot(diss.map.p80, add=TRUE)\nplot(diss.map.p50,col=\"red\", add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\nplot(cat.telem,UD=UD,main=\"autocorrelated KDE Best\")\n\n13. Or a few other options for autcorrelated KDE\n\npar(mfrow=c(2,2))\n\nplot(ver99,main=\"KDE-HREF Bandwidth\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(ver95, lty=6, add=TRUE)\nplot(ver90, add=TRUE)\nplot(ver80, add=TRUE)\nplot(ver50, col=\"red\",add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\nplot(diss.map.p99,main=\"Brownian Bridge Movement Model\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(diss.map.p95, lty=6, add=TRUE)\nplot(diss.map.p90, add=TRUE)\nplot(diss.map.p80, add=TRUE)\nplot(diss.map.p50,col=\"red\", add=TRUE)\npoints(loc, pch=1, cex=0.5)\n\nplot(cat.telem,UD=UD,main=\"autocorrelated KDE Best\")\n\nplot(cat.telem,UD=UD2,main=\"autocorrelated KDE tau=1\")"
  },
  {
    "objectID": "BBMMscript.html",
    "href": "BBMMscript.html",
    "title": "\n25  Brownian Bridge Movement Models (BBMM)\n",
    "section": "",
    "text": "The BBMM requires (1) sequential location data, (2) estimated error associated with location data, and (3) grid-cell size assigned for the output utilization distribution. The BBMM is based on two assumptions: (1) location errors correspond to a bivariate normal distribution and (2) movement between successive locations is random conditional on the starting and ending location (Horne et al. 2007). Normally distributed errors are common for GPS data and 1 h between locations likely ensured that movement between successive locations was random (Horne et al. 2007). The assumption of conditional random movement between paired locations, however, becomes less realistic as the time interval increases (Horne et al. 2007).\n1. Open the script “BBMMscript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\n#library(BBMM)\nlibrary(stringr)\nlibrary(PBSmapping)\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Load panther dataset\n\npanther&lt;-read.csv(\"data/pantherjitter.csv\",header=T)\npanther$CatID &lt;- as.factor(panther$CatID)\npanther$NewTime &lt;- str_pad(panther$TIMEET2,4, pad= \"0\")\npanther$NewDate &lt;- paste(panther$DateET2,panther$NewTime)\n#Used to sort data in code below for all deer\npanther$DT &lt;- as.POSIXct(strptime(panther$NewDate, format='%Y %m %d %H%M'))\n#Sort Data\npanther &lt;- panther[order(panther$CatID, panther$DT),]\n\ntimediff &lt;- diff(panther$DT)*60\n# remove first entry without any difference \npanther &lt;- panther[-1,] \npanther$timelag &lt;-as.numeric(abs(timediff))\n\ncat143&lt;-subset(panther, panther$CatID == \"143\")\ncat143 &lt;- cat143[-1,] #Remove first record with wrong timelag\ncat143$CatID &lt;- factor(cat143$CatID)\n\n5. Use brownian.bridge function in package BBMM to run home range\n\nBBMM = brownian.bridge(x=cat143$X, y=cat143$Y, time.lag=cat143$timelag, location.error=34, cell.size=100)\nbbmm.summary(BBMM)\n\nNOTE: (a) Time lag refers to the elapsed time between consecutive GPS locations that was presented in section 2.3 (b) GPS collar error can be from error reported by the manufacturer of the GPS collar or from error test conducted at the study site (c) Cell size refers to grid size we want to estimate the BBMM\n10. We need to create output ascii files or shapefiles for graphical representation of size of BBMM (Fig. 4.2). We can also compare BBMM for the Florida panther to KDE using hplug-in (Fig. 4.3) and href (Fig. 4.4). We start by creating a data.frame indicating cells within the contour desired and export as Ascii Grid\n\n# Create data.frame indicating cells within the contour desired and export as Ascii Grid\nbbmm.contour = data.frame(x = BBMM$x, y = BBMM$y, probability = BBMM$probability)\ncontours = bbmm.contour(BBMM, levels=c(seq(50, 90, by=10), 95, 99), locations=cat143, plot=TRUE)\nprint(contours)\n# Pick a contour for export as Ascii\nbbmm.50 = bbmm.contour[bbmm.contour$probability &gt;= contours$Z[1],]\nbbmm.50$in.out &lt;- 1 \nbbmm.50 &lt;-bbmm.50[,-3]\n# Output ascii file for cells within specified contour.\nm50 = SpatialPixelsDataFrame(points = bbmm.50[c(\"x\", \"y\")], data=bbmm.50)\nm50.g = as(m50, \"SpatialGridDataFrame\")\n#writeAsciiGrid(m50.g, \"50ContourInOut.asc\", attr=ncol(bbmm.50))\n\n# Convert to SpatialPolygonsDataFrame and export as ESRI Shapefile\nshp.50 &lt;- as(m50, \"SpatialPolygonsDataFrame\")\nmap.ps50 &lt;- SpatialPolygons2PolySet(shp.50)\ndiss.map.50 &lt;- joinPolys(map.ps50, operation = 'UNION')\ndiss.map.50 &lt;- as.PolySet(diss.map.50, projection = 'UTM', zone = '17')\ndiss.map.p50 &lt;- PolySet2SpatialPolygons(diss.map.50, close_polys = TRUE)\ndata50 &lt;- data.frame(PID = 1)\ndiss.map.p50 &lt;- SpatialPolygonsDataFrame(diss.map.p50, data = data50)\nplot(diss.map.p50)\n\n# writeOGR(diss.map.p50, dsn = \".\", layer=\"contour50\", driver = \"ESRI Shapefile\")\n# map.50 &lt;- readOGR(dsn=\".\", layer=\"contour50\")\n# plot(map.50)\n\n# Pick a contour for export as Ascii\nbbmm.80 = bbmm.contour[bbmm.contour$probability &gt;= contours$Z[4],]\nbbmm.80$in.out &lt;- 1 \n\nbbmm.80 &lt;-bbmm.80[,-3]\n# Output ascii file for cells within specified contour.\nm80 = SpatialPixelsDataFrame(points = bbmm.80[c(\"x\", \"y\")], data=bbmm.80)\nm80.g = as(m80, \"SpatialGridDataFrame\")\n#writeAsciiGrid(m80.g, \"80ContourInOut.asc\", attr=ncol(bbmm.80))\n\n# Convert to SpatialPolygonsDataFrame and export as ESRI Shapefile\nshp.80 &lt;- as(m80, \"SpatialPolygonsDataFrame\")\nmap.ps80 &lt;- SpatialPolygons2PolySet(shp.80)\ndiss.map.80 &lt;- joinPolys(map.ps80, operation = 'UNION')\ndiss.map.80 &lt;- as.PolySet(diss.map.80, projection = 'UTM', zone = '17')\ndiss.map.p80 &lt;- PolySet2SpatialPolygons(diss.map.80, close_polys = TRUE)\ndata80 &lt;- data.frame(PID = 1)\ndiss.map.p80 &lt;- SpatialPolygonsDataFrame(diss.map.p80, data = data80)\nplot(diss.map.p80)\n\n# writeOGR(diss.map.p80, dsn = \".\", layer=\"contour80\", driver = \"ESRI Shapefile\")\n# map.80 &lt;- readOGR(dsn=\".\", layer=\"contour80\")\n# plot(map.80)\n\n# Pick a contour for export as Ascii\nbbmm.90 = bbmm.contour[bbmm.contour$probability &gt;= contours$Z[5],]\nbbmm.90$in.out &lt;- 1 \n\nbbmm.90 &lt;-bbmm.90[,-3]\n# Output ascii file for cells within specified contour.\nm90 = SpatialPixelsDataFrame(points = bbmm.90[c(\"x\", \"y\")], data=bbmm.90)\nm90.g = as(m90, \"SpatialGridDataFrame\")\n#writeAsciiGrid(m90.g, \"90ContourInOut.asc\", attr=ncol(bbmm.90))\n\n# Convert to SpatialPolygonsDataFrame and export as ESRI Shapefile\nshp.90 &lt;- as(m90, \"SpatialPolygonsDataFrame\")\nmap.ps90 &lt;- SpatialPolygons2PolySet(shp.90)\ndiss.map.90 &lt;- joinPolys(map.ps90, operation = 'UNION')\ndiss.map.90 &lt;- as.PolySet(diss.map.90, projection = 'UTM', zone = '17')\ndiss.map.p90 &lt;- PolySet2SpatialPolygons(diss.map.90, close_polys = TRUE)\ndata90 &lt;- data.frame(PID = 1)\ndiss.map.p90 &lt;- SpatialPolygonsDataFrame(diss.map.p90, data = data90)\nplot(diss.map.p90)\n\n# writeOGR(diss.map.p90, dsn = \".\", layer=\"contour90\", driver = \"ESRI Shapefile\")\n# map.90 &lt;- readOGR(dsn=\".\", layer=\"contour90\")\n# plot(map.90)\n\n# Pick a contour for export as Ascii\nbbmm.95 = bbmm.contour[bbmm.contour$probability &gt;= contours$Z[6],]\nbbmm.95$in.out &lt;- 1 \n\nbbmm.95 &lt;-bbmm.95[,-3]\n# Output ascii file for cells within specified contour.\nm95 = SpatialPixelsDataFrame(points = bbmm.95[c(\"x\", \"y\")], data=bbmm.95)\nm95.g = as(m95, \"SpatialGridDataFrame\")\n#writeAsciiGrid(m95.g, \"95ContourInOut.asc\", attr=ncol(bbmm.95))\n\n# Convert to SpatialPolygonsDataFrame and export as ESRI Shapefile\nshp.95 &lt;- as(m95, \"SpatialPolygonsDataFrame\")\nmap.ps95 &lt;- SpatialPolygons2PolySet(shp.95)\ndiss.map.95 &lt;- joinPolys(map.ps95, operation = 'UNION')\ndiss.map.95 &lt;- as.PolySet(diss.map.95, projection = 'UTM', zone = '17')\ndiss.map.p95 &lt;- PolySet2SpatialPolygons(diss.map.95, close_polys = TRUE)\ndata95 &lt;- data.frame(PID = 1)\ndiss.map.p95 &lt;- SpatialPolygonsDataFrame(diss.map.p95, data = data95)\nplot(diss.map.p95)\n\n# writeOGR(diss.map.p95, dsn = \".\", layer=\"contour95\", driver = \"ESRI Shapefile\")\n# map.95 &lt;- readOGR(dsn=\".\", layer=\"contour95\")\n# plot(map.95)\n\n# Pick a contour for export as Ascii\nbbmm.99 = bbmm.contour[bbmm.contour$probability &gt;= contours$Z[7],]\nbbmm.99$in.out &lt;- 1 \n\nbbmm.99 &lt;-bbmm.99[,-3]\n# Output ascii file for cells within specified contour.\nm99 = SpatialPixelsDataFrame(points = bbmm.99[c(\"x\", \"y\")], data=bbmm.99)\nm99.g = as(m99, \"SpatialGridDataFrame\")\n#writeAsciiGrid(m99.g, \"99ContourInOut.asc\", attr=ncol(bbmm.99))\n\n# Convert to SpatialPolygonsDataFrame and export as ESRI Shapefile\nshp.99 &lt;- as(m99, \"SpatialPolygonsDataFrame\")\nmap.ps99 &lt;- SpatialPolygons2PolySet(shp.99)\ndiss.map.99 &lt;- joinPolys(map.ps99, operation = 'UNION')\ndiss.map.99 &lt;- as.PolySet(diss.map.99, projection = 'UTM', zone = '17')\ndiss.map.p99 &lt;- PolySet2SpatialPolygons(diss.map.99, close_polys = TRUE)\ndata99 &lt;- data.frame(PID = 1)\ndiss.map.p99 &lt;- SpatialPolygonsDataFrame(diss.map.p99, data = data99)\nplot(diss.map.p99)\n\n# writeOGR(diss.map.p99, dsn = \".\", layer=\"contour99\", driver = \"ESRI Shapefile\")\n# map.99 &lt;- readOGR(dsn=\".\", layer=\"contour99\")\n# plot(map.99)\n\n11. Another short exercise to subset large dataset by the appropriate time lags in your data but only include locations collected within the 7 hour schedule (i.e., &lt; 421 minutes)\n\nloc &lt;- subset(cat143, cat143$timelag != \"NA\" & cat143$timelag &lt; 421)\nBBMM2 = brownian.bridge(x=loc$X, y=loc$Y, time.lag=loc$timelag, location.error=34, cell.size=100)\nbbmm.summary(BBMM2)\nbbmm.contour1 = data.frame(x = BBMM2$x, y = BBMM2$y, probability = BBMM2$probability)\ncontours1 = bbmm.contour(BBMM2, levels=c(seq(50, 90, by=10), 95, 99), locations=loc, plot=TRUE)\n\n12. Or we could exclude the extreme 1% of locations from cat143 based on time difference. This will result in subsetting data and only calculate BBMM with timediff cutoff of &lt;2940 minutes\n\nfreq &lt;- as.data.frame(table(round(cat143$timelag)))\n# result is Var1 = the time difference, and Freq = its frequency in the data\nfreq$percent &lt;- freq$Freq/dim(cat143)[1]*100\nfreq$sum[1] &lt;- freq$percent[1]\nfor (j in 2:dim(freq)[1]){\nfreq$sum[j] &lt;- freq$percent[j]+freq$sum[j-1]\n}\nindicator &lt;- which(freq$sum&gt;99)\ncutoff &lt;- as.numeric(as.character(freq$Var1[min(indicator)]))\ncutoff\n\nloc2 &lt;- subset(cat143, cat143$timelag &lt; 2940)\nstr(loc2)\nBBMM3 = brownian.bridge(x=loc2$X, y=loc2$Y, time.lag=loc2$timelag, location.error=34, cell.size=100)\nbbmm.summary(BBMM3)\nbbmm.contour2 = data.frame(x = BBMM3$x, y = BBMM3$y, probability = BBMM3$probability)\ncontours2 = bbmm.contour(BBMM3, levels=c(seq(50, 90, by=10), 95, 99), locations=loc2, plot=TRUE)\n\n13.\n\n#Plot results for all contours\ncontours2 = bbmm.contour(BBMM3, levels=c(seq(50, 90, by=10), 95, 99), locations=loc2, plot=TRUE)\n\n#Be sure to change bbmm.contours and rerun Steps 7-11 each time before running each section of code below\n\nraw.df &lt;- data.frame(\"x\"=cat143$X,\"y\"=cat143$Y)\n##Define the projection of the coordinates\n##Make SpatialPointsDataFrame using the XY, attributes, and projection\nspdf &lt;- SpatialPointsDataFrame(raw.df, cat143, proj4string = proj4string)\nplot(map.99, col=\"grey\",axes=T)\nplot(map.95, add=TRUE)\nplot(map.90, add=TRUE)\nplot(map.80, add=TRUE)\nplot(map.50, add=TRUE)\npoints(spdf)\n\nloc.df &lt;- data.frame(\"x\"=loc$X,\"y\"=loc$Y)\n##Define the projection of the coordinates\nproj4string &lt;- CRS(\"+proj=utm +zone=17 +ellps=WGS84\")\n##Make SpatialPointsDataFrame using the XY, attributes, and projection\nspdf2 &lt;- SpatialPointsDataFrame(loc.df, loc, proj4string = proj4string)\nplot(map.99, col=\"grey\",axes=T)\nplot(map.95, add=TRUE)\nplot(map.90, add=TRUE)\nplot(map.80, add=TRUE)\nplot(map.50, add=TRUE)\npoints(spdf2)\n\nloc2.df &lt;- data.frame(\"x\"=loc2$X,\"y\"=loc2$Y)\n##Define the projection of the coordinates\nproj4string &lt;- CRS(\"+proj=utm +zone=17 +ellps=WGS84\")\n##Make SpatialPointsDataFrame using the XY, attributes, and projection\nspdf2 &lt;- SpatialPointsDataFrame(loc2.df, loc2, proj4string = proj4string)\nplot(map.99, col=\"grey\",axes=T)\nplot(map.95, add=TRUE)\nplot(map.90, add=TRUE)\nplot(map.80, add=TRUE)\nplot(map.50, add=TRUE)\npoints(spdf2)"
  },
  {
    "objectID": "MKDEcat_script.html",
    "href": "MKDEcat_script.html",
    "title": "\n26  Movement-based Kernel Density Estimation (MKDE)\n",
    "section": "",
    "text": "If we want to take both BBMM and KDE to a higher level we can incorporate movement-based estimators of home range. Movement-based Kernel Density Estimation (MKDE) incorporates movements trajectories and habitat components of the landscape your animal occupies (Benhamou 2011, Benhamou and Cornelis 2010). This method requires a habitat layer and the adehabitatHR package requires that no duplicate entries exist for a given date so makes estimates of home range with GPS data problematic. Furthermore, after tirelessly trying this method for days using data with time intervals, we changed to data that had dates but then had to remove duplicates. If you have worked out all of these issues, you can skip ahead to MKDE estimates with your data starting at Step 6.\n1. Open the script “MKDEcat_script.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatHR)\nlibrary(adehabitatLT)\nlibrary(sp)\nlibrary(chron) \nlibrary(stringr)\nlibrary(FedData)\nlibrary(sf)\nlibrary(terra)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(26917)\nalbers.crs &lt;- st_crs(5070)\n\n4. We need to import our locations and get them in the form we need for this exercise before we can move forward\n\npanther&lt;-read.csv(\"data/pantherjitter.csv\",header=T)\npanther$CatID &lt;- as.factor(panther$CatID)\n#We first need to use the original dataset to calculate time between locations\npanther$NewTime &lt;- str_pad(panther$TIMEET2,4, pad= \"0\")\npanther$NewDate &lt;- paste(panther$DateET2,panther$NewTime)\n#Used to sort data in code below for all deer\npanther$DT &lt;- as.POSIXct(strptime(panther$NewDate, format='%Y %m %d %H%M'))\n#Sort Data\npanther &lt;- panther[order(panther$CatID, panther$DT),]\n#TIME DIFF NECESSARY IN BBMM CODE\ntimediff &lt;- diff(panther$DT)*60\n# remove first entry without any difference \npanther &lt;- panther[-1,] \npanther$timelag &lt;-as.numeric(abs(timediff))\n\ncat143&lt;-subset(panther, panther$CatID == \"143\")\ncat143 &lt;- cat143[-1,] #Remove first record with wrong timelag\ncat143$CatID &lt;- droplevels(cat143$CatID)\n\n#Remove outlier locations\ncoords &lt;- st_as_sf(cat143, coords = c(\"X\", \"Y\"), crs = utm.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ncat.albers &lt;- st_transform(coords, albers.crs)\n\nNOTE: Two issues at this step held me back with the method for weeks so we will stress them here:\nExtent of the raster layer selected - Although Extent was the lesser problem, it still needs to be address for several reasons. If the extent is too large or raster cell size too small then processing time increases. Although we would not really want to spend the time to clip raster habitat layers for each animal, you may need to have separate rasters for different areas of your study site to cut down on processing time. More importantly, animals need to be within the same grid for analysis using MKDE/BRB home range estimates. This will become more apparent later but preparing habitat or landscape layers for all animals using the same habitat extent will save time in the end.\nProjection of the raster layer and locations - Even we missed this one with all our experiences and constant issues with data layers in different projections. We assumed that defining the projection with R would take care of this issue but we could not have been more wrong. So before we move forward, we want to demonstrate our thought processes here and how we solved this problem.\n5. If we get some NA errors because our grid does not encompass our panther locations then we can expand the grid size extending beyond our locations using methods in an earlier exercise.\n\n# Create vectors of the x and y points using boundary box created around deer locations\nbb1 &lt;- st_bbox(cat.albers)\n     \nincrement = 2000\nminx=(min(bb1$xmin)-(increment))\nmaxx=(max(bb1$xmax)+(increment))\nminy=(min(bb1$ymin)-(increment))\nmaxy=(max(bb1$ymax)+(increment))\n\nmy_bbox = st_bbox(c(xmin = minx, xmax = maxx, \n                    ymin = miny, ymax = maxy),\n                  crs = 5070)\n\nboundary &lt;- st_as_sfc(my_bbox)\n\n6. So the raster layer that is included in this exercise is in UTM Zone 17. Now, import the raster layer to have layers in the same projection.\n\nFLnlcd &lt;- get_nlcd(template=boundary, year = 2006, label = 'Florida',dataset = \"landcover\", force.redo = T)\n\nnlcd.df &lt;- as.data.frame(FLnlcd, xy=FALSE)\nnlcd.df2 &lt;- as.data.frame(FLnlcd, xy=TRUE)\nnlcd.xy &lt;- data.frame(x=nlcd.df2$x,y=nlcd.df2$y)\n##Cast over to SPxDF\nhabitat &lt;- SpatialPixelsDataFrame(nlcd.xy,nlcd.df)\ncrs(habitat) &lt;- st_crs(FLnlcd)$proj4string#\"NAD83 / Conus Albers\"\n#plot(habitat)\n\n#Create an ltraj trajectory object. \nltraj &lt;- as.ltraj(st_coordinates(cat.albers), cat.albers$DT, id = cat.albers$Sex, burst = cat.albers$Sex, \n  typeII = TRUE)\n#plot(ltraj, spixdf=habitat)\n#writeRaster(FLnlcd,\"FL_nlcd.tif\",format=\"GTiff\",datatype = 'INT1U',overwrite=T)\n\n7. With the same projections for our 2 data layers, we can move forward. First we need to create ltraj as in Chapter 3 and use some additional code to overlay the trajectory onto the Spatial Pixels Data Frame using the command “spixdf” as in the code below that results in Fig. 4.11. Basically, if this works then we are on the right path to moving forward with MKDE.\n\n# FLnlcd2 &lt;- rast(\"data/FL_nlcd.tif\")\n# FLnlcd2 &lt;- project(FLnlcd2,FLnlcd,method=\"near\")\n# nlcd.df2 &lt;- as.data.frame(FLnlcd2, xy=FALSE)\n# nlcd.df3 &lt;- as.data.frame(FLnlcd2, xy=TRUE)\n# nlcd.xy &lt;- data.frame(x=nlcd.df3$x,y=nlcd.df3$y)\n# ##Cast over to SPxDF\n# habitat2 &lt;- SpatialPixelsDataFrame(nlcd.xy,nlcd.df)\n# \n# #Create an ltraj trajectory object. \n# ltraj &lt;- as.ltraj(st_coordinates(cat.albers), cat.albers$DT, id = cat.albers$Sex, burst = cat.albers$Sex, \n#   typeII = TRUE)\n# plot(ltraj, spixdf=habitat)\n\n8. Now identify habitats that can and can not be used by panthers\n\n#Be sure to do this step after plotting ltraj onto spixdf or won't work!\n#This step just builds a \"fake\" habitat map with habitat=1\nfullgrid(habitat) &lt;- TRUE\nhab &lt;- habitat\nhab[[1]] &lt;- as.numeric(!is.na(hab[[1]])) \n\n#This step is needed to convert SpatialGrid to SpatialPixels for use in \"ud\" estimation \n#if needed\n#\"habitat\" in \"grid=habitat\" must be of class SpatialPixels\nfullgrid(hab) &lt;- FALSE\nclass(hab)\n\n9. Now we can begin to create Movement-based KDEs using biased random bridges (BRBs)\n\n#Assign parameter values for BRB \n# Parameters for the Biased Random Bridge Kernel approach \ntmax &lt;- 1*(24*60*60) + 1 #set the maximum time between locations to be just more than 1 day\nlmin &lt;- 50 #locations less than 50 meters apart are considered inactive.   \nhmin &lt;- 100 #arbitrarily set to be same as hab grid cell resolution \n\n#Diffusion component for each habitat type using plug-in method\nvv&lt;- BRB.D(ltraj, Tmax = tmax, Lmin = lmin,  habitat = hab)\nvv\n\nud &lt;- BRB(ltraj, D = vv, Tmax = tmax, Lmin = lmin, hmin=hmin, grid = hab, b=TRUE, \n  extent=0.1, tau = 300)\nud\n\n#Address names in ud by assigning them to be the same as the ids in ltraj\n#Must be done before using \"getverticeshr\" function\nnames(ud) &lt;- id(ltraj) \n\n10. Create contours using getverticeshr to display or export as shapefiles (Fig. 4.15).\n\nver1_99 &lt;- getverticeshr(ud, percent=99, standardize = TRUE, whi = id(ltraj))\nplot(ver1_99)\n#ver1_95 &lt;- getverticeshr(ud, percent=95, standardize = TRUE, whi = id(ltraj))\n#ver1_90 &lt;- getverticeshr(ud, percent=90, standardize = TRUE, whi = id(ltraj))\n#ver1_80 &lt;- getverticeshr(ud, percent=80, standardize = TRUE, whi = id(ltraj))\nver1_50 &lt;- getverticeshr(ud, percent=50, standardize = TRUE, whi = id(ltraj))\nplot(ver1_99)\n#plot(ver1_95, add=T)\nplot(ver1_50, add=T)\n\n11. Now let’s create a new UD using an actual habitat layer that has more than “used/unused” such as the 7 habitat categories from original dataset\n\n#Start by importing the habitat layer again and run the following\nm &lt;- c(0, 19, 1, 20, 39, 2, 40, 50, 3, 51, 68, 4, 69,79, 5, 80, 88, 6, 89, 99, 7)\nrclmat &lt;- matrix(m, ncol=3, byrow=TRUE)\nrc &lt;- classify(FLnlcd, rclmat)\n\nrc.df &lt;- as.data.frame(rc, xy=FALSE)\nrc.df2 &lt;- as.data.frame(rc, xy=TRUE)\nrc.xy &lt;- data.frame(x=rc.df2$x,y=rc.df2$y)\n##Cast over to SPxDF\nhabitat2 &lt;- SpatialPixelsDataFrame(rc.xy,rc.df)\ncrs(habitat2) &lt;- st_crs(FLnlcd)$proj4string\nplot(habitat2)\nplot(ltraj, spixdf=habitat2)\n#CODE TO CONDUCT BRB\n#Assign parameter values for BRB \n# Parameters for the Biased Random Bridge Kernel approach \ntmax &lt;- 1*(24*60*60) + 1 #set the maximum time between locations to be just more than 1 day\nlmin &lt;- 50 #locations less than 50 meters apart are considered inactive.   \nhmin &lt;- 100 #arbitrarily set to be same as hab grid cell resolution \n\n#Diffusion component for each habitat type using plug-in method\nvv2&lt;- BRB.D(ltraj, Tmax = tmax, Lmin = lmin,  habitat = habitat2)\nvv2\n\nud2 &lt;- BRB(ltraj, D = vv2, Tmax = tmax, Lmin = lmin, hmin=hmin, habitat = habitat2, b=TRUE, \n  extent=0.1, tau = 300, same4all=FALSE)\n\nnames(ud2) &lt;- id(ltraj) \n\nver2_99 &lt;- getverticeshr(ud2, percent=99, standardize = TRUE, whi = id(ltraj))\n#ver2_95 &lt;- getverticeshr(ud2, percent=95, standardize = TRUE, whi = id(ltraj))\n#ver2_90 &lt;- getverticeshr(ud2, percent=90, standardize = TRUE, whi = id(ltraj))\n#ver2_80 &lt;- getverticeshr(ud2, percent=80, standardize = TRUE, whi = id(ltraj))\nver2_50 &lt;- getverticeshr(ud2, percent=50, standardize = TRUE, whi = id(ltraj))\n\n12. Now let’s create a new UD using an actual habitat layer that has more than “used/unused” such as the 4 habitat categories from original dataset\n\n#4 habitats instead of the 7 above with water, open, and forested\nm1 &lt;- c(0,39, 1, 40, 68, 2, 69,88, 3, 89, 99, 4)\nrclmat1 &lt;- matrix(m1, ncol=3, byrow=TRUE)\nrc1 &lt;- classify(FLnlcd, rclmat1)\nrc1.df &lt;- as.data.frame(rc1, xy=FALSE)\nrc1.df2 &lt;- as.data.frame(rc1, xy=TRUE)\nrc1.xy &lt;- data.frame(x=rc1.df2$x,y=rc1.df2$y)\n##Cast over to SPxDF\nhabitat3 &lt;- SpatialPixelsDataFrame(rc1.xy,rc1.df)\ncrs(habitat3) &lt;- st_crs(FLnlcd)$proj4string\nplot(habitat3)\nplot(ltraj, spixdf=habitat3)\n\n#Diffusion component for each habitat type using plug-in method\nvv3&lt;- BRB.D(ltraj, Tmax = tmax, Lmin = lmin,  habitat = habitat3)\nvv3\n\nud3 &lt;- BRB(ltraj, D = vv3, Tmax = tmax, Lmin = lmin, hmin=hmin, habitat = habitat3, b=TRUE, extent=0.1, tau = 300, same4all=FALSE)\n\nnames(ud3) &lt;- id(ltraj) \n\nver3_99 &lt;- getverticeshr(ud3, percent=99, standardize = TRUE, whi = id(ltraj))\n#ver3_95 &lt;- getverticeshr(ud3, percent=95, standardize = TRUE, whi = id(ltraj))\n#ver3_80 &lt;- getverticeshr(ud3, percent=80, standardize = TRUE, whi = id(ltraj))\nver3_50 &lt;- getverticeshr(ud3, percent=50, standardize = TRUE, whi = id(ltraj))\n\n15. Now we will plot these for comparison with the habitat layer differing by each analysis.\n\npar(mfrow=c(1,3))\nplot(ver1_99,main=\"mKDE no habitat\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\n#plot(ver1_95, lty=6, add=TRUE)\n#plot(ver1_90, add=TRUE)\n#plot(ver1_80, add=TRUE)\nplot(ver1_50, col=\"red\",add=TRUE)\npoints(cat.albers, pch=1, cex=0.5)\n\nplot(ver2_99,main=\"mKDE 7 habitats\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\n#plot(ver2_95, lty=6, add=TRUE)\n#plot(ver2_90, add=TRUE)\n#plot(ver2_80, add=TRUE)\nplot(ver2_50,col=\"red\", add=TRUE)\npoints(cat.albers, pch=1, cex=0.5)\n\nplot(ver3_99,main=\"mKDE 4 habitats\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\n#plot(ver3_95, lty=6, add=TRUE)\n#plot(ver3_80, add=TRUE)\nplot(ver3_50,col=\"red\", add=TRUE)\npoints(cat.albers, pch=1, cex=0.5)"
  },
  {
    "objectID": "CatdBBMM.html",
    "href": "CatdBBMM.html",
    "title": "\n27  Dynamic Brownian Bridge Movement Models (dBBMM)\n",
    "section": "",
    "text": "With the wide-spread use of GPS technology to track animals in near real time, estimators of home range and movement have developed concurrently. Unlike the traditional point-based estimators (i.e., MCP, KDE with href/hplug-in) that only incorporate density of locations into home range estimation, newer estimators incorporate more data provided by GPS technology. While BBMM incorporates a temporal component and GPS error into estimates, dynamic Brownian Bridge Movement Models (dBBMM) incorporate temporal and behavioral characteristics of movement paths into estimation of home range (Kranstauber et al. 2012). Estimating a movement path over the entire trajectory of data, however, should be separated into behavorial movement patterns (i.e., resting, feeding) prior to estimating the variance of the Brownian motion. Overestimating the variance will cause an imprecision in estimation of the utilization distribution that dBBMM seeks to address (Kranstauber et al. 2012).\n1. Open the script “CatdBBMM.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatLT)\nlibrary(adehabitatHR)\nlibrary(stringr)\nlibrary(move)\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(26917)\nalbers.crs &lt;- st_crs(5070)\n\n4. Read in panther dataset we have used previously\n\n#Creates a Spatial Points Data Frame for 2 animals by ID\npanther&lt;-read.csv(\"data/pantherjitter.csv\",header=T)\npanther$CatID &lt;- as.factor(panther$CatID)\n#To run BBMM we first need to use the original dataset to calculate time between locations\npanther$NewTime &lt;- str_pad(panther$TIMEET2,4, pad= \"0\")\npanther$NewDate &lt;- paste(panther$DateET2,panther$NewTime)\n#Used to sort data in code below for all deer\npanther$DT &lt;- as.POSIXct(strptime(panther$NewDate, format='%Y %m %d %H%M'))\n#Sort Data\npanther &lt;- panther[order(panther$CatID, panther$DT),]\n#TIME DIFF NECESSARY IN BBMM CODE\ntimediff &lt;- diff(panther$DT)*60\n# remove first entry without any difference \npanther &lt;- panther[-1,] \npanther$timelag &lt;-as.numeric(abs(timediff))\n\ncat143&lt;-subset(panther, panther$CatID == \"143\")\ncat143 &lt;- cat143[-1,] #Remove first record with wrong timelag\ncat143$CatID &lt;- droplevels(cat143$CatID)\n\n#Remove outlier locations\ncoords &lt;- st_as_sf(cat143, coords = c(\"X\", \"Y\"), crs = utm.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ncat.albers &lt;- st_transform(coords, albers.crs)\n\n7. Create a move object for all deer using the Move package\n\n#Create an ltraj trajectory object. \nltraj &lt;- as.ltraj(st_coordinates(coords), coords$DT, id = coords$Sex, burst = coords$Sex, \n  typeII = TRUE)\n\nloc &lt;- move(ltraj)\n\n8. Now create a dBBMM object\n\ncat_dbbmm &lt;- brownian.bridge.dyn(object=loc, location.error=34, window.size=19, margin=7,\ndimSize=500,time.step=180)\nplot(cat_dbbmm)\n\n9. We can then explore the isopleth sizes and manipulate package move output to plot isopleths like previous exercises and write them out as shapefiles if needed\n\ncontour(cat_dbbmm, levels=c(.5,.9,.95,.99))\nshow(cat_dbbmm)\n\n#Plot the movement of the animal\nplot(loc, type=\"o\", col=3, lwd=2, pch=20, xlab=\"location_east\",ylab=\"location_north\")\n\n#Code below will get area of each isopleth\ncat_cont &lt;- getVolumeUD(cat_dbbmm)\ncat_cont50 &lt;- cat_cont&lt;=.50\ncat_cont95 &lt;- cat_cont&lt;=.95\narea50 &lt;- sum(values(cat_cont50))\narea50\narea95 &lt;- sum(values(cat_cont95))\narea95\n\n##Cast the data over to an adehabitatHR estUD\ndbbmm.px &lt;- as(cat_dbbmm, \"SpatialPixelsDataFrame\")\nimage(dbbmm.px)\ndbbmm.ud &lt;- new(\"estUD\",dbbmm.px)\ndbbmm.ud@vol = FALSE\ndbbmm.ud@h$meth = \"dBBMM\"\n\nshp99 &lt;- getverticeshr(dbbmm.ud, percent=99, standardize=TRUE)\nplot(shp99, add=TRUE)\nmap.ps99 &lt;- SpatialPolygons2PolySet(shp99)\ndiss.map.99 &lt;- as.PolySet(map.ps99, projection = 'UTM', zone = '17')\ndiss.map.p99 &lt;- PolySet2SpatialPolygons(diss.map.99, close_polys = TRUE)\ndata99 &lt;- data.frame(PID = 1)\ndiss.map.p99 &lt;- SpatialPolygonsDataFrame(diss.map.p99, data = data99)\nplot(diss.map.p99)\n# writeOGR(diss.map.p99, dsn = \".\", layer=\"catcontour99\", driver = \"ESRI Shapefile\")\n# catmap.99 &lt;- readOGR(dsn=\".\", layer=\"catcontour99\")\n\nshp95 &lt;- getverticeshr(dbbmm.ud, percent=95, standardize=TRUE)\nplot(shp95, add=TRUE)\nmap.ps95 &lt;- SpatialPolygons2PolySet(shp95)\ndiss.map.95 &lt;- as.PolySet(map.ps95, projection = 'UTM', zone = '17')\ndiss.map.p95 &lt;- PolySet2SpatialPolygons(diss.map.95, close_polys = TRUE)\ndata95 &lt;- data.frame(PID = 1)\ndiss.map.p95 &lt;- SpatialPolygonsDataFrame(diss.map.p95, data = data95)\nplot(diss.map.p95, add=T)\n#writeOGR(diss.map.p95, dsn = \".\", layer=\"catcontour95\", driver = \"ESRI Shapefile\")\n#catmap.95 &lt;- readOGR(dsn=\".\", layer=\"catcontour95\")\n\nshp90 &lt;- getverticeshr(dbbmm.ud, percent=90, standardize=TRUE)\nmap.ps90 &lt;- SpatialPolygons2PolySet(shp90)\ndiss.map.90 &lt;- as.PolySet(map.ps90, projection = 'UTM', zone = '17')\ndiss.map.p90 &lt;- PolySet2SpatialPolygons(diss.map.90, close_polys = TRUE)\ndata90 &lt;- data.frame(PID = 1)\ndiss.map.p90 &lt;- SpatialPolygonsDataFrame(diss.map.p90, data = data90)\nplot(diss.map.p90,add=T)\n# writeOGR(diss.map.p90, dsn = \".\", layer=\"catcontour90\", driver = \"ESRI Shapefile\")\n# catmap.90 &lt;- readOGR(dsn=\".\", layer=\"catcontour90\")\n\nshp80 &lt;- getverticeshr(dbbmm.ud, percent=80, standardize=TRUE)\nmap.ps80 &lt;- SpatialPolygons2PolySet(shp80)\ndiss.map.80 &lt;- as.PolySet(map.ps80, projection = 'UTM', zone = '17')\ndiss.map.p80 &lt;- PolySet2SpatialPolygons(diss.map.80, close_polys = TRUE)\ndata80 &lt;- data.frame(PID = 1)\ndiss.map.p80 &lt;- SpatialPolygonsDataFrame(diss.map.p80, data = data80)\nplot(diss.map.p80,add=T)\n# writeOGR(diss.map.p80, dsn = \".\", layer=\"catcontour80\", driver = \"ESRI Shapefile\")\n# catmap.80 &lt;- readOGR(dsn=\".\", layer=\"catcontour80\")\n\nshp50 &lt;- getverticeshr(dbbmm.ud, percent=50, standardize=TRUE)\nmap.ps50 &lt;- SpatialPolygons2PolySet(shp50)\ndiss.map.50 &lt;- as.PolySet(map.ps50, projection = 'UTM', zone = '17')\ndiss.map.p50 &lt;- PolySet2SpatialPolygons(diss.map.50, close_polys = TRUE)\ndata50 &lt;- data.frame(PID = 1)\ndiss.map.p50 &lt;- SpatialPolygonsDataFrame(diss.map.p50, data = data50)\nplot(diss.map.p50,add=T)\n# writeOGR(diss.map.p50, dsn = \".\", layer=\"catcontour50\", driver = \"ESRI Shapefile\")\n# catmap.50 &lt;- readOGR(dsn=\".\", layer=\"catcontour50\")\n\n10. We can plot out the 50-99% isopleths to compare to previous estimators in size and shape around locations\n\nplot(diss.map.p99)\nplot(diss.map.p95, add=T)\nplot(diss.map.p90,add=T)\nplot(diss.map.p80,add=T)\nplot(diss.map.p50,add=T)\nplot(st_geometry(coords),pch=1, cex=0.5,add=T)\n\n11. Now we will shift towards polgyon-based estimators of home range to compare them to dBBMM. We will start with Characteristic Hull Polygons (CHP) in adehabitatHR package using the CharHull function.\n\nloc &lt;- data.frame(\"x\"=cat143$X,\"y\"=cat143$Y)\ncats &lt;- SpatialPointsDataFrame(loc,cat143)\nproj4string(cats) &lt;- \"+proj=utm +zone=17 +ellps=WGS84\"\n\n#Home Range estimation\nres &lt;- CharHull(cats[,1])\nclass(\"res\")\n\n[1] \"character\"\n\n#Computes the home range size for 20-100 percent\nMCHu2hrsize(res)\n\n\n\n\n            143\n20     144.4545\n30     340.6498\n40     664.8405\n50    1173.5487\n60    1949.0595\n70    3170.9746\n80    5344.8134\n90   10767.9219\n100 103361.1336\n\n#OR use\n\nres_ver99 &lt;- getverticeshr(res, percent=99)\nres_ver95 &lt;- getverticeshr(res, percent=95)\nres_ver90 &lt;- getverticeshr(res, percent=90)\nres_ver80 &lt;- getverticeshr(res, percent=80)\nres_ver50 &lt;- getverticeshr(res, percent=50)\n\nplot(res_ver99)\nplot(res_ver95,add=T)\nplot(res_ver90, add=TRUE, col=\"blue\")\nplot(res_ver80, add=TRUE, col=\"red\")\nplot(res_ver50, add=T, col=\"green\")\nplot(st_geometry(coords),pch=1, cex=0.5, add=T)\n\n\n\n\n11. Next we will estimate home range with the Single-linkage Cluster (SLCA) using the clusthr function\n\nuu &lt;- clusthr(cats[,1])\nclass(uu)\n\n[1] \"MCHu\"\n\nuu_ver99 &lt;- getverticeshr(uu, percent=99)\nuu_ver95 &lt;- getverticeshr(uu, percent=95)\nuu_ver90 &lt;- getverticeshr(uu, percent=90)\nuu_ver80 &lt;- getverticeshr(uu, percent=80)\nuu_ver50 &lt;- getverticeshr(uu, percent=50)\n\nplot(uu_ver99)\nplot(uu_ver95,add=T)\nplot(uu_ver90, add=TRUE, col=\"blue\")\nplot(uu_ver80, add=TRUE, col=\"red\")\nplot(uu_ver50, add=T, col=\"green\")\nplot(st_geometry(coords),pch=1, cex=0.5, add=T)\n\n\n\n\n12. Next we will explore Local Convex Hull (LoCoH)\n\n## Exams the changes in home-range size for various values of k\n## Be patient! the algorithm can be very long\n#LoC.area &lt;- LoCoH.k.area(idsp, k=c(5:40))\n#NOTE: The line of code above does not run for this animal\n\n## the k-LoCoH method:\nnn &lt;- LoCoH.k(cats[,1], k=30)\n\n## Graphical display of the results\nplot(nn, border=NA)\n\n\n\n## the object nn is a list of objects of class\n\n#Save shapefiles of resulting home range\nver &lt;- getverticeshr(nn)\n\n#st_write(ver,dsn=\".\",layer=\"FixedK24.shp\", driver = \"ESRI Shapefile\", overwrite=TRUE)\nnn_ver50 &lt;-getverticeshr(nn, percent=50)\n#st_write(ver50,dsn=\".\",layer=\"50FixedK24\", driver = \"ESRI Shapefile\",overwrite=TRUE)\nnn_ver80 &lt;-getverticeshr(nn, percent=80)\n#st_write(ver80,dsn=\".\",layer=\"80FixedK24\", driver = \"ESRI Shapefile\",overwrite=TRUE)\nnn_ver90 &lt;-getverticeshr(nn, percent=90)\n#st_write(ver90,dsn=\".\",layer=\"90FixedK24\", driver = \"ESRI Shapefile\",overwrite=TRUE)\nnn_ver95 &lt;-getverticeshr(nn, percent=95)\n#st_write(ver95,dsn=\".\",layer=\"95FixedK24\", driver = \"ESRI Shapefile\",overwrite=TRUE)\nnn_ver99 &lt;-getverticeshr(nn, percent=99)\n#st_write(ver99,dsn=\".\",layer=\"99FixedK24\", driver = \"ESRI Shapefile\",overwrite=TRUE)\n\nplot(nn_ver99,main=\"Local Convex Hull\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(nn_ver95,add=T)\nplot(nn_ver90, add=TRUE, col=\"blue\")\nplot(nn_ver80, add=TRUE, col=\"red\")\nplot(nn_ver50, add=T, col=\"green\")\nplot(st_geometry(coords),pch=1, cex=0.5, add=T)\n\n\n\n\n13. We can add 4 estimators to the plot window to compare across estimators\n\npar(mfrow=c(2,2))\n\nplot(diss.map.p99,main=\"dynamic BBMM\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(diss.map.p95, add=T)\nplot(diss.map.p90,add=T)\nplot(diss.map.p80,add=T)\nplot(diss.map.p50,add=T)\nplot(st_geometry(coords),pch=1, cex=0.5, add=T)\n\nplot(res_ver99,main=\"Characteristic Hull Polygons\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(res_ver95,add=T)\nplot(res_ver90, add=TRUE, col=\"blue\")\nplot(res_ver80, add=TRUE, col=\"red\")\nplot(res_ver50, add=T, col=\"green\")\npoints(loc, pch=1, cex=0.5)\nplot(st_geometry(coords),pch=1, cex=0.5, add=T)\n\nplot(uu_ver99,main=\"Single-linkage Cluster\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(uu_ver95,add=T)\nplot(uu_ver90, add=TRUE, col=\"blue\")\nplot(uu_ver80, add=TRUE, col=\"red\")\nplot(uu_ver50, add=T, col=\"green\")\nplot(st_geometry(coords),pch=1, cex=0.5, add=T)\n\nplot(nn_ver99,main=\"Local Convex Hull\",xlab=\"X\", ylab=\"Y\", font=1, cex=0.8, axes=T)\nplot(nn_ver95,add=T)\nplot(nn_ver90, add=TRUE, col=\"blue\")\nplot(nn_ver80, add=TRUE, col=\"red\")\nplot(nn_ver50, add=T, col=\"green\")\nplot(st_geometry(coords),pch=1, cex=0.5, add=T)"
  },
  {
    "objectID": "Frag_Auto.html#and-7.2-fragstats-metrics-within-polygons",
    "href": "Frag_Auto.html#and-7.2-fragstats-metrics-within-polygons",
    "title": "\n28  Landscape Metrics\n",
    "section": "\n28.1 7.1 and 7.2 Fragstats Metrics within Polygons",
    "text": "28.1 7.1 and 7.2 Fragstats Metrics within Polygons\nSome research designs may just need landscape metrics for a single area or several study areas and that is what the SDMToolsl package is able to estimate in the code that follows. While the single area can be defined by the extent of the raster we imported as in previous chapters, the ability of the SDMToolsl package to determine patch and class statistics depends on the area defined by the user from that could be study site, within polygons such as counties or townships, or within buffers around locations.\n1. Open the script “Frag_Auto.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\n#library(SDMTools)\nlibrary(plyr)\nlibrary(FedData)\n#install.packages(\"landscapemetrics\")\nlibrary(landscapemetrics)\nlibrary(ggplot2)\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Load vegetation raster layer clipped in ArcMap\n\ncrops &lt;-rast(\"data/crop2012utm12.tif\")\nplot(crops)\nclass(crops)\n\n# reclassify the values into 9 groups\n# all values between 0 and 20 equal 1, etc.\nm &lt;- c(-Inf,0,NA,2, 7, 2, 20, 60, 3, 60, 70, 4, 110, 132, 5, 133, 150, 6, 151, 191, 7, \n  192,Inf,NA)\nrclmat &lt;- matrix(m, ncol=3, byrow=TRUE)\nrc &lt;- classify(crops, rclmat)\nplot(rc)\nas.matrix(table(values(rc)))\n\ncheck_landscape(rc)\n#Now we get into Landscape Metrics with the SDTM Tool\n#Calculate the Patch statistics\nps.data = lsm_p_enn(rc)\nps.data\n\n#Calculate the Class statistics\ncl.data = calculate_lsm(rc)\ncl.data\n\n5. Some research designs may need landscape metrics for several areas that may be available as a shapefile or some other polygon layer.\n\n#Load raster file into R\n#raster &lt;- rast(\"data/county_hab\")\n\n#Load PA shapefile into R\nHareCounties &lt;- st_read(\"data/Hare_Counties.shp\")\n\ncounty_hab &lt;- get_nlcd(template=HareCounties, year = 2006, label = 'HareCounties',dataset = \"landcover\", force.redo = T)\n\nplot(county_hab)\nplot(st_geometry(HareCounties), add=T)\n\n#Let's project Counties to the same projection as the habitat raster\ncounty &lt;- st_transform(HareCounties, albers.crs)\nHareCounties &lt;- county\n#Matching projections successful!\nplot(county_hab)\nplot(HareCounties, add=T)\n\n#Now add labels to each hexagon for unique ID\ncounty.centroids &lt;- sf::st_point_on_surface(HareCounties)\ncounty.coords &lt;- as.data.frame(sf::st_coordinates(county.centroids))\ncounty.coords$NAME &lt;- HareCounties$COUNTY_NAM\nggplot() +\n  geom_sf(data = HareCounties) +\n  geom_text(data = county.coords, aes(X, Y, label = NAME), colour =\"black\")\n\n6. Now we want to export by County name (i.e., COUNTY_NAM) as individual shapefiles. We will only select the first 2 counties for processing to save time\n\nindata &lt;- HareCounties\ninnames &lt;- unique(HareCounties$COUNTY_NAM)\ninnames &lt;- innames[1:2]#Place the number of unique polygons in your shapefile here\noutnames &lt;- innames\n\n# set up output table\n#output &lt;- as.data.frame(matrix(0,nrow=length(innames),ncol=38))\n\n# begin loop to create separate county shapefiles \nfor (i in 1:length(innames)){\n  data &lt;- indata[which(indata$COUNTY_NAM==innames[i]),]\n  if(dim(data)[1] != 0){\n    st_write(data, dsn = \".\", layer=paste(outnames[i],\".shp\",sep=\"/\"), driver = \"ESRI Shapefile\")\n    write.table(innames, \"List.txt\", col.names=FALSE, quote=FALSE, \n  row.names=FALSE)\n}\n}\n\n#Read in a list of shapefiles files from above\nListshps&lt;-read.table(\"List.txt\",sep=\"\\t\",header=F)\nListshps\n\nshape &lt;- function(Listshps) {\nfile &lt;- as.character(Listshps[1,],\".shp\")\nshp &lt;- st_read(dsn=\".\", layer=file)\nmask &lt;- mask(raster,shp)\n### Calculate the Class statistics in each county\ncl.data &lt;- lsm_c_enn(mask)\n}\n\nresults &lt;- ddply(Listshps, 1, shape)\nresults\n#write.table(results, \"FragCounty.txt\")"
  },
  {
    "objectID": "PatchAnalystScript.html",
    "href": "PatchAnalystScript.html",
    "title": "\n29  Fragstats Metrics within Buffers\n",
    "section": "",
    "text": "Some research designs may just need landscape metrics for a single area or several study areas and that is what the SDMToolsl package is able to estimate in the code that follows. While the single area can be defined by the extent of the raster we imported as in previous chapters, the ability of the SDMToolsl package to determine patch and class statistics depends on the area defined by the user from that could be study site, within polygons such as counties or townships, or within buffers around locations.\n1. Open the script “PatchAnalystScript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\n#library(SDMTools)\nlibrary(plyr)\nlibrary(landscapemetrics)\nlibrary(sf)\nlibrary(terra)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(26912)\nalbers.crs &lt;- st_crs(5070)\n\n4. Load vegetation raster layer created previoulsy. NOTE: The Cropland data layer for this region of Colorado was not available for 2011.\n\ncrops &lt;-rast(\"data/crop2012utm12.tif\")\n\n# reclassify the values into 9 groups\n# all values between 0 and 20 equal 1, etc.\nm &lt;- c(-Inf,0,NA,2, 7, 2, 20, 60, 3, 60, 70, 4, 110, 132, 5, 133, 150, 6, 151, 191, 7, \n  192,Inf,NA)\nrclmat &lt;- matrix(m, ncol=3, byrow=TRUE)\nrc &lt;- classify(crops, rclmat)\nplot(rc)\n\n\n\n\n\nWhat if we wanted to compare difference in patch statistics among all deer with all locations combined?\n\nWe will start by creating buffers around individual locations for our mule deer dataset\n\nmuleys &lt;-read.csv(\"data/muleysexample.csv\", header=T)\n\nmuleys$GPSFixTime&lt;-as.POSIXct(muleys$GPSFixTime, format=\"%Y.%m.%d%H:%M:%S\")\n\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\n#Project deer.spdf to UTM to \ndeer.utm &lt;-st_transform(deer.spdf, crs=st_crs(crops))\nplot(st_geometry(deer.utm),axes=T)\n\n\n\ndeer.utm.df &lt;- as.data.frame(deer.utm)\ndeer.utm.xy &lt;- st_coordinates(deer.utm)\n \nmuleys &lt;- cbind(deer.utm.df,deer.utm.xy)\n#Remove utm xys if needed\nmuleys &lt;- muleys[c(-9:-10)]\n#Only use the 5 lines of code below to subsample for demonstration purposes!\nonemuley &lt;-muleys[1:5,]\ntwomuley &lt;-muleys[202:206,]\nshortmd &lt;- rbind(onemuley, twomuley)\nmuleys &lt;- shortmd\n\n6. Next we need to create a function to extract Fragstats metrics within individual polygons\n\nbuff3rd &lt;- function(muleys) {\n  coords &lt;- st_as_sf(muleys, coords = c(\"X\", \"Y\"), crs = utm.crs)\n  settbuff &lt;- st_buffer(coords,1000) %&gt;% st_as_sfc()\n  settbuff &lt;- vect(settbuff)\n  buffclip &lt;- mask(rc, settbuff)\n  buff.data &lt;- calculate_lsm(buffclip)\n  newline &lt;- coords$id\n  bind &lt;-cbind(newline[1], buff.data)\n}\n\nresults &lt;- ddply(muleys, .(id), buff3rd)\nresults\nclass(results)\n\n7. Code above looks at patch and class metrics for each deer by combining all buffers into one polygon for each deer (i.e., comparable to defining available habitat in 3rd order selection. However, what if we wanted to compare difference in patch statistics among all deer by averaging metrics across buffers?\n\ncoords&lt;-data.frame(x = muleys$X, y = muleys$Y)\ndeer.spdf &lt;- SpatialPointsDataFrame(coords=coords, data = muleys, proj4string = utm12.crs)\nsetbuff &lt;- gBuffer(deer.spdf, width=1000, byid=TRUE)\nmuleys$newID &lt;- paste(muleys$id, setbuff@plotOrder, sep=\"_\")\nmuleys$newID &lt;- as.factor(muleys$newID)\n\nbuff3rdA &lt;- function(muleys) {\n  bufclip &lt;- mask(rc, setbuff)\n  buf.data &lt;- PatchStat(bufclip)\n}\n\nresults2 &lt;- ddply(muleys, .(newID), buff3rdA)\nresults2"
  },
  {
    "objectID": "MCPscript.html#minimum-convex-polygon-mcp",
    "href": "MCPscript.html#minimum-convex-polygon-mcp",
    "title": "\n30  Resource Selection\n",
    "section": "\n30.1 Minimum Convex Polygon (MCP)",
    "text": "30.1 Minimum Convex Polygon (MCP)\nMinimum Convex Polygon (MCP) estimation was considered a home range originally described for use with identifying animals recaptured along a trapping grid (Mohr 1947). The reason we removed this from the Home Range Section is because MCP can be used to describe the extent of distribution of locations of an animal but NOT as an estimation of home range size. In fact, reporting size of home range using MCP should be avoided at all costs unless you can justify its use as opposed to the plethora of other estimators we have learned in the previous section. We may use MCP within resource selection function analysis as it has been suggested as a method to describe the extent of area occupied by a species that would be available to animals using either second or third order selection of habitat (Johnson 1980), although this should also be avoided unless specifically justified as to why MCP is better than an alternate home range estimator. The extent of an area an animal uses (i.e., habitat available) should be determined for each species and the most appropriate estimator should be used.\n1. Open the script “MCPscript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatHR)\nlibrary(sf)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269) \nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Load in a mule deer dataset we have used in previous exercises\n\nmuleys &lt;-read.csv(\"data/muleysexample.csv\", header=T)\n\n#Remove outlier locations\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous plot\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\nmuleys &lt;- as.data.frame(deer.spdf)\nloc &lt;- data.frame(\"x\"=deer.spdf$X,\"y\"=deer.spdf$Y)\ndeer.spdf &lt;- SpatialPointsDataFrame(loc,muleys)\nproj4string(deer.spdf) &lt;- \"+proj=utm +zone=17 +ellps=WGS84\"\n\n5. We are now ready to create MCPs for our new dataset “merge” by individual animal ID (Fig. 8.1).\n\ncp &lt;- mcp(deer.spdf[,2], percent=95)#(95% is the default)\n## The size of the bounding polygon\nas.data.frame(cp)\n\n     id      area\nD12 D12   5.29995\nD8   D8 515.42075\n\n## Plot the home ranges\nplot(cp)\n## ... And the relocations\nplot(deer.spdf, add=TRUE)\n\n\n\n\n6. Use sf package to write to a shapefile\n\ncp &lt;- as(cp,\"sf\")\nst_write(cp, \"MCPhomerange.shp\")\n\n7. We have chosen to exclude 5% of the most extreme relocations, but we could have made another choice. We may compute the MCP for various choices of the number of extreme relocations to be excluded, using the function mcp.area:\n\nhrs &lt;- mcp.area(deer.spdf[,2], percent=seq(50, 100, by = 5))\n\n\n\nhrs\n\n         D12       D8\n50   0.01405 300.3191\n55   0.01480 300.6203\n60   0.01600 300.6898\n65   0.01670 300.7083\n70   0.01725 302.6114\n75   0.02005 302.6438\n80   0.02340 359.7976\n85   0.04850 427.8537\n90   0.06580 468.4115\n95   5.29995 515.4207\n100 15.24355 669.3478"
  },
  {
    "objectID": "LinearDistScript.html",
    "href": "LinearDistScript.html",
    "title": "\n31  Preparing Linear Measures\n",
    "section": "",
    "text": "First we will begin with determining the distance between several features. In our first example, we want to measure distance from each mule deer location to the nearest stream if it is determined a priori that water or riparian habitats influence mule deer distribution in our study area. While this may not seem like a very complicated process, there are numerous steps needed to achieve this feat. We will need to use the package spatstat that will help us in creating individual segments with nodes for linear features such as roads and streams/rivers.\n1. Open the script LinearDistscript.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(spatstat)\n#library(maptools)#Needed for spatstat class \"owin\"\nlibrary(polyCub)#replaces gpclib function\nlibrary(OneR)#bin function\nlibrary(sf)\nlibrary(terra)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Load the mule deer dataset we used in the previous exercise\n\nmuleys &lt;-read.csv(\"data/muleysexample.csv\", header=T)\n#Remove outlier locations\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous plot\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\n#Project deer.spdf to Albers as in previous exercise\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers,axes=T))\n\n\n\n\n\nOnly use code in next section for example exercise so fewer locations are used.\n\n\nmuleys &lt;- muleys[sample(nrow(muleys), 100),]\n\n#Make a spatial data frame of locations after removing outliers\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous \n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n#Project deer.spdf to Albers as in previous exercise\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers),axes=T)\n\n\n\n\n6. If we get some NA errors because our grid does not encompass our panther locations then we can expand the grid size extending beyond our locations using methods in an earlier exercise.\n\n# Create vectors of the x and y points using boundary box created around deer locations\nbb1 &lt;- st_bbox(deer.albers)\n     \nincrement = 1000\nminx=(min(bb1$xmin)-(increment))\nmaxx=(max(bb1$xmax)+(increment))\nminy=(min(bb1$ymin)-(increment))\nmaxy=(max(bb1$ymax)+(increment))\n\nmy_bbox = st_bbox(c(xmin = minx, xmax = maxx, \n                    ymin = miny, ymax = maxy),\n                  crs = 5070)\n\nAlbersSP &lt;- st_as_sfc(my_bbox)\n\n7. Load the necessary road and rivers shapefiles already in Albers projection to match previous vegetation raster.\n\nroads&lt;-st_read(\"data/AlbersRoads.shp\")\n\nReading layer `AlbersRoads' from data source \n  `/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-of-Applied-Spatial-Ecology/data/AlbersRoads.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 42674 features and 6 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -1300892 ymin: 1621719 xmax: -989792.6 ymax: 1808098\nProjected CRS: NAD_1983_Albers\n\nrivers&lt;-st_read(\"data/AlbersRivers.shp\")\n\nReading layer `AlbersRivers' from data source \n  `/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-of-Applied-Spatial-Ecology/data/AlbersRivers.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5853 features and 7 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -1333589 ymin: 1622164 xmax: -988898 ymax: 1807267\nProjected CRS: NAD_1983_Albers\n\n\n10. Load vegetation raster layer tif that came in the Albers projection from the online source.\n\nveg &lt;-rast(\"data/cropnlcd.tif\")\n\n\n#Check to see all our layers are now in Albers projection\nst_crs(veg)\n\nCoordinate Reference System:\n  User input: unnamed \n  wkt:\nBOUNDCRS[\n    SOURCECRS[\n        PROJCRS[\"unnamed\",\n            BASEGEOGCRS[\"GRS 1980(IUGG, 1980)\",\n                DATUM[\"unknown\",\n                    ELLIPSOID[\"GRS80\",6378137,298.257222101,\n                        LENGTHUNIT[\"metre\",1,\n                            ID[\"EPSG\",9001]]]],\n                PRIMEM[\"Greenwich\",0,\n                    ANGLEUNIT[\"degree\",0.0174532925199433,\n                        ID[\"EPSG\",9122]]]],\n            CONVERSION[\"Albers Equal Area\",\n                METHOD[\"Albers Equal Area\",\n                    ID[\"EPSG\",9822]],\n                PARAMETER[\"Latitude of false origin\",23,\n                    ANGLEUNIT[\"degree\",0.0174532925199433],\n                    ID[\"EPSG\",8821]],\n                PARAMETER[\"Longitude of false origin\",-96,\n                    ANGLEUNIT[\"degree\",0.0174532925199433],\n                    ID[\"EPSG\",8822]],\n                PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n                    ANGLEUNIT[\"degree\",0.0174532925199433],\n                    ID[\"EPSG\",8823]],\n                PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n                    ANGLEUNIT[\"degree\",0.0174532925199433],\n                    ID[\"EPSG\",8824]],\n                PARAMETER[\"Easting at false origin\",0,\n                    LENGTHUNIT[\"metre\",1],\n                    ID[\"EPSG\",8826]],\n                PARAMETER[\"Northing at false origin\",0,\n                    LENGTHUNIT[\"metre\",1],\n                    ID[\"EPSG\",8827]]],\n            CS[Cartesian,2],\n                AXIS[\"easting\",east,\n                    ORDER[1],\n                    LENGTHUNIT[\"metre\",1,\n                        ID[\"EPSG\",9001]]],\n                AXIS[\"northing\",north,\n                    ORDER[2],\n                    LENGTHUNIT[\"metre\",1,\n                        ID[\"EPSG\",9001]]]]],\n    TARGETCRS[\n        GEOGCRS[\"WGS 84\",\n            DATUM[\"World Geodetic System 1984\",\n                ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                    LENGTHUNIT[\"metre\",1]]],\n            PRIMEM[\"Greenwich\",0,\n                ANGLEUNIT[\"degree\",0.0174532925199433]],\n            CS[ellipsoidal,2],\n                AXIS[\"geodetic latitude (Lat)\",north,\n                    ORDER[1],\n                    ANGLEUNIT[\"degree\",0.0174532925199433]],\n                AXIS[\"geodetic longitude (Lon)\",east,\n                    ORDER[2],\n                    ANGLEUNIT[\"degree\",0.0174532925199433]],\n            USAGE[\n                SCOPE[\"Horizontal component of 3D system.\"],\n                AREA[\"World.\"],\n                BBOX[-90,-180,90,180]],\n            ID[\"EPSG\",4326]]],\n    ABRIDGEDTRANSFORMATION[\"Transformation to WGS84\",\n        METHOD[\"Position Vector transformation (geog2D domain)\",\n            ID[\"EPSG\",9606]],\n        PARAMETER[\"X-axis translation\",0,\n            ID[\"EPSG\",8605]],\n        PARAMETER[\"Y-axis translation\",0,\n            ID[\"EPSG\",8606]],\n        PARAMETER[\"Z-axis translation\",0,\n            ID[\"EPSG\",8607]],\n        PARAMETER[\"X-axis rotation\",0,\n            ID[\"EPSG\",8608]],\n        PARAMETER[\"Y-axis rotation\",0,\n            ID[\"EPSG\",8609]],\n        PARAMETER[\"Z-axis rotation\",0,\n            ID[\"EPSG\",8610]],\n        PARAMETER[\"Scale difference\",1,\n            ID[\"EPSG\",8611]]]]\n\nst_crs(deer.albers)\n\nCoordinate Reference System:\n  User input: EPSG:5070 \n  wkt:\nPROJCRS[\"NAD83 / Conus Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Conus Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",23,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-96,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Data analysis and small scale data presentation for contiguous lower 48 states.\"],\n        AREA[\"United States (USA) - CONUS onshore - Alabama; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming.\"],\n        BBOX[24.41,-124.79,49.38,-66.91]],\n    ID[\"EPSG\",5070]]\n\nst_crs(AlbersSP)\n\nCoordinate Reference System:\n  User input: EPSG:5070 \n  wkt:\nPROJCRS[\"NAD83 / Conus Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"Conus Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",23,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-96,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",29.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",45.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Data analysis and small scale data presentation for contiguous lower 48 states.\"],\n        AREA[\"United States (USA) - CONUS onshore - Alabama; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming.\"],\n        BBOX[24.41,-124.79,49.38,-66.91]],\n    ID[\"EPSG\",5070]]\n\n\n\nplot(veg)\nplot(st_geometry(deer.albers),add=T, col=\"red\")\n\n\n\n\n11. Then we need to expand the bounding polygon so all locations are included. We can then make the bounding polygon (AlbersSP) a class owin in order to proceed with functions in package spatstat.\n\nbuffSP &lt;- st_buffer(AlbersSP,1000)\nplot(st_geometry(buffSP))\nplot(st_geometry(deer.albers),add=T,col=\"red\")\n\n\n\n\n12. Code below will be for use with the spatstat package to convert segments of line layers (e.g., roads, rivers) to lines to enable distance to feature from deer locations. Most calculations with spatstat require 3 new classes so most code is created to achieve this goal:\n“owin” Observation windows “ppp” Planar point patterns “psp” Planar segment patterns\n\n#Replace AlbersSP with buffSP if using a subsample of deer\nbdy.owin &lt;- as.owin(buffSP)\nis.owin(bdy.owin)\n\n[1] TRUE\n\n#It is TRUE so now we can move forward with the analysis\n\n12. Now clip the raster using the buffered bounding box (buffSP) created in step 5.\n\nbbclip &lt;- crop(veg, buffSP) \ncliproads &lt;- st_intersection(roads, buffSP, byid=TRUE)\ncliprivers &lt;- st_intersection(rivers, buffSP, byid=TRUE)\n\nplot(bbclip)\nplot(st_geometry(buffSP),add=T)\nplot(st_geometry(deer.albers),add=T,col=\"red\")\nplot(st_geometry(cliproads),add=T)\nplot(st_geometry(cliprivers), col=\"blue\",add=T)\n\n\n\n\n13. We will start with the road layer by converting a single line to a set of segments packaged as a function.\n\nfoo &lt;- function(cliproads){\nx &lt;- cliproads@Lines[[1]]@coords\ncbind(\nhead(x,-1),\ntail(x,-1))}\n#The function can be applied successively to each line in the list we extracted from roads. \n#Results are output as a list, then converted to a matrix.\n\nsegs.lst &lt;- lapply(cliproads@lines,foo)\nsegs &lt;- do.call(rbind,segs.lst)\n\nsegs.x &lt;- c(segs[,c(1,3)])\nsegs.y &lt;- c(segs[,c(2,4)])\nsegs.owin &lt;- as.owin(c(range(segs.x),range(segs.y)))#create a new \"owin\" class because \n#roads occur outside our bdy.owin created above\n\n#The segments as a planar segment pattern:\nsegs.psp &lt;- as.psp(segs, window=segs.owin)\nplot(segs.psp)\npoints(deer.albers)\nsegs.psp[1:5]\n#lengths.psp(segs.psp[1:10])\n\n#We can cut road segments into distances we control\ndist &lt;- pointsOnLines(segs.psp, eps=1000)\n\n14. We first need to handle the mule deer locations. We need to make mule deer xy coordinates a planar point pattern (i.e., ppp) for use in package spatstat.\n\ndeer2 &lt;-as.data.frame(deer.albers)\nnewdeer &lt;-cbind(deer2$x,deer2$y)\nxy.ppp &lt;- as.ppp(newdeer,W=bdy.owin)\nplot(xy.ppp)\n\n15. Now we can determine the distance from mule deer locations (xy.ppp) to the nearest road\n\nroaddist &lt;- nncross(xy.ppp, segs.psp)$dist\n#Or identify segment number closest to each point\nv &lt;- nearestsegment(xy.ppp,segs.psp)#Identifies segment number not a distance\nplot(segs.psp)\nplot(xy.ppp[101], add=TRUE, col=\"red\")\nplot(segs.psp[v[101]], add=TRUE, lwd=5, col=\"red\")\n\n16. Now we do the same to a river layer by converting a single line to a set of segments packaged as a function.\n\nfoo &lt;- function(cliprivers){\nx &lt;- cliprivers@Lines[[1]]@coords\ncbind(\nhead(x,-1),\ntail(x,-1))}\n#The function can be applied successively to each line in the list we extracted from roads. \n#Results are output as a list, then converted to a matrix.\nrivs.lst &lt;- lapply(cliprivers@lines,foo)\nrivs &lt;- do.call(rbind,rivs.lst)\nrivs.x &lt;- c(rivs[,c(1,3)])\nrivs.y &lt;- c(rivs[,c(2,4)])\nrivs.owin &lt;- as.owin(c(range(rivs.x),range(rivs.y)))\n\n#The segments as a planar segment pattern:\nrivs.psp &lt;- as.psp(rivs, window=rivs.owin)\nplot(rivs.psp)\npoints(deer.albers)\nis.psp(rivs.psp)\n#All is TRUE so now we can move forward with the analysis\n\n17. Now we can determine the distance from mule deer locations (xy.ppp) to the nearest river.\n\nrivdist &lt;- nncross(xy.ppp, rivs.psp)$dist\n\n#Or identify segment number closest to each point\nriv &lt;- nearestsegment(xy.ppp,rivs.psp)\nplot(rivs.psp, lwd=1)\nplot(xy.ppp[1], add=TRUE, col=\"red\")\nplot(rivs.psp[riv[1]], add=TRUE, lwd=5, col=\"red\")\nplot(xy.ppp[290], add=TRUE, col=\"blue\")\nplot(rivs.psp[riv[290]], add=TRUE, lwd=5, col=\"blue\")\npoints(deer.albers)\n\n18. We can then summarize the distances in some meaningful way for analysis. Instead of representing distance to road as individual numerical values we can bin the distances in some categories we determine appropriate for our research objective.\n\nbr &lt;- seq(0,1000,200)\nlbl &lt;- paste(head(br,-1),tail(br,-1),sep=\"-\")\nroad.tbl &lt;- table(cut(roaddist,breaks=br,labels=lbl))\nRdresults &lt;- road.tbl/sum(road.tbl)\nRdresults\n\nbr1 &lt;- seq(0,4000,500)\nlbl1 &lt;- paste(head(br1,-1),tail(br1,-1),sep=\"-\")\nriver.tbl &lt;- table(cut(rivdist,breaks=br1,labels=lbl1))\nRivresults &lt;- river.tbl/sum(river.tbl)\nRivresults\n\n19. Or we can place each distance into a category or Bin for each deer\n\nBinRoad &lt;- bin(roaddist, nbins=5, method='content', labels=c('1','2','3','4','5'))\nBinRoad2 &lt;- cut(roaddist, 5, method='intervals', include.lowest=TRUE, labels=c('1','2','3'\n  ,'4','5'))\ntable(BinRoad)\n\nBinRivers &lt;- bin(rivdist, nbins=5, method='content', labels=c('1','2','3','4','5'))\nBinRivers &lt;- cut(rivdist, 5, method='intervals', include.lowest=TRUE, labels=c('1','2','3'\n    ,'4','5'))\ntable(BinRivers)\n\n#Now use cbind function to add binned distances to muleys dataset.\nDist &lt;- cbind(BinRoad,BinRivers)\nmuleys &lt;- cbind(muleys, Dist)"
  },
  {
    "objectID": "MD_DataPrep.html",
    "href": "MD_DataPrep.html",
    "title": "\n32  Preparing Additional Covariates\n",
    "section": "",
    "text": "We may often be interested in assessing various covariates that may influence resource selection of our study animals. If we have a priori knowledge that elevation or slope may influence selection for or use of portions of the landscape then we need to create these layers for analysis. While this may not seem like a very complicated process because it is routinely done in ArcMap, those same available layers can be used and manipulated in R as in Chapter 1. We can then create slope, aspect, hillshade or other variables within R using concepts in earlier chapters and extract those covariates for use in modeling all within the R environment.\n1. Open the script MD_DataPrep.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatHR)\nlibrary(FedData)\nlibrary(sf)\nlibrary(terra)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269)\nutm.crs &lt;- st_crs(9001)\nalbers.crs &lt;- st_crs(5070)\n\n4. Load the mule deer dataset we used in the previous exercise\n\nmuleys &lt;-read.csv(\"data/muleysexample.csv\", header=T)\n#Remove outlier locations\ncoords &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(coords),axes=T)\n\n\n\ndeer.spdf &lt;- st_crop(coords, xmin=-107.0,xmax=-110.5,ymin=37.8,ymax=39.0)#Visually identified based on previous plot\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot(st_geometry(deer.spdf),axes=T)\n\n\n\n\n\nOnly use code in this section for example exercise so fewer locations are used.\n\n\ndeer.spdf &lt;- deer.spdf[sample(nrow(deer.spdf),100),]\n\n#Project deer.spdf to Albers as in previous exercise\ndeer.albers &lt;-st_transform(deer.spdf, crs=albers.crs)\nplot(st_geometry(deer.albers),axes=T)\n\n\n\n\n6. If we get some NA errors because our grid does not encompass our panther locations then we can expand the grid size extending beyond our locations using methods in an earlier exercise.\n\n# Create vectors of the x and y points using boundary box created around deer locations\nbb1 &lt;- st_bbox(deer.albers)\n     \nincrement = 1000\nminx=(min(bb1$xmin)-(increment))\nmaxx=(max(bb1$xmax)+(increment))\nminy=(min(bb1$ymin)-(increment))\nmaxy=(max(bb1$ymax)+(increment))\n\nmy_bbox = st_bbox(c(xmin = minx, xmax = maxx, \n                    ymin = miny, ymax = maxy),\n                  crs = 5070)\n\nAlbersSP &lt;- st_as_sfc(my_bbox)\n\n7. Now it is time to import some raster layers of the covariates we are interested in for our analysis. Start with raster of vegetation from the 2012 NRCS Crop data that is a nice dataset that is crop specific for each year. Crop data can be found at the NRCS webpage Cropland Data Layer that can be accessed for each county of each state.\n\ncrops &lt;-rast(\"data/crop12clip.tif\")\n\n# Reclassify crops raster from above into 9 groups\n# all values between 0 and 20 equal 1, etc.\nm &lt;- c(-Inf,0,NA,2, 7, 2, 20, 60, 3, 60, 70, 4, 110, 132, 5, 133, 150, 6, 151, 172, 7, \n       180, 183, 8, 189, 191, 9,192,205,10)\nrclmat &lt;- matrix(m, ncol=3, byrow=TRUE)\nrc &lt;- classify(crops, rclmat)\nplot(rc)\n\n\n\n#Crop using AlbersSP polygon created earlier to reduce size of raster (if needed).\nbbclip &lt;- crop(rc, AlbersSP)\n\nas.matrix(table(values(bbclip)))#Identifies the number of cells in each category\n\n    [,1]\n1      5\n2   1076\n3  21283\n4   9510\n5   2081\n6  18564\n7  58266\n9      9\n10    14\n\nplot(bbclip)\nplot(st_geometry(deer.albers), add=T,col=\"red\")\nplot(st_geometry(AlbersSP), add=T,lwd=2)\n\n\n\n\n8. We also want to look at elevation and covariates related to elevation (e.g., slope, aspect). These can be created directly in R using the terrain function in the raster package. We will use the FedData package to get Digital Elevation Models for the large study area\n\nDEM &lt;- get_ned(template=AlbersSP, label = 'COdem',force.redo = T)\nimage(DEM, col=terrain.colors(10))\ncontour(DEM, add=TRUE)\n\n\n\nslope &lt;- terrain(DEM,\"slope\", neighbor=8, unit='degrees')\naspect = terrain(DEM,\"aspect\", neighbor=8, unit='degrees')\n\nelevation &lt;- project(DEM, bbclip, mask=TRUE)\nslope &lt;- project(slope, bbclip, mask=TRUE)\naspect &lt;- project(aspect, bbclip, mask=TRUE)\n\nnlcdclip &lt;- crop(bbclip, slope)\ndemclip &lt;- crop(elevation, slope)\nsloclip &lt;- crop(slope, slope)\naspclip &lt;- crop(aspect, slope)\n\n9. Cast over all 4 layers to a Spatial Grid Data Frame to permit combining into one layer.\n\nnlcd &lt;- as.data.frame(nlcdclip, xy=TRUE) \nelev &lt;- as.data.frame(demclip, xy=TRUE) \nslo &lt;- as.data.frame(sloclip, xy=TRUE)\nasp &lt;- as.data.frame(aspclip, xy=TRUE) \n\n#Now check to be sure the number of cells in each layer are the same before proceeding the\n#next step of combining layers.\nstr(nlcd)\n\n'data.frame':   110808 obs. of  3 variables:\n $ x         : num  -1128900 -1128870 -1128840 -1128810 -1128780 ...\n $ y         : num  1725120 1725120 1725120 1725120 1725120 ...\n $ CLASS_NAME: num  7 5 7 7 7 7 7 7 7 7 ...\n\nstr(elev)\n\n'data.frame':   110808 obs. of  3 variables:\n $ x             : num  -1128900 -1128870 -1128840 -1128810 -1128780 ...\n $ y             : num  1725120 1725120 1725120 1725120 1725120 ...\n $ USGS_1_n38w109: num  2066 2066 2066 2066 2066 ...\n\nstr(slo)\n\n'data.frame':   110803 obs. of  3 variables:\n $ x    : num  -1128900 -1128870 -1128840 -1128810 -1128780 ...\n $ y    : num  1725120 1725120 1725120 1725120 1725120 ...\n $ slope: num  2.92 2.93 2.81 2.52 1.95 ...\n\nstr(asp)\n\n'data.frame':   110803 obs. of  3 variables:\n $ x     : num  -1128900 -1128870 -1128840 -1128810 -1128780 ...\n $ y     : num  1725120 1725120 1725120 1725120 1725120 ...\n $ aspect: num  170 169 179 201 225 ...\n\n\n\nTo combine all data frames, we need to match xys so we have the same number of rows of data in each raster. We will use the code below to do this.\n\n\nfor (z in length(asp)){\n    asp$xy &lt;- paste(asp$x, asp$y, sep=\"\")\n  }\n  for (z in length(slo)){\n    slo$xy &lt;- paste(slo$x, slo$y, sep=\"\")\n  }\n  for (z in length(elev)){\n    elev$xy &lt;- paste(elev$x, elev$y, sep=\"\")\n  }\n  for (z in length(nlcd)){\n    nlcd$xy &lt;- paste(nlcd$x, nlcd$y, sep=\"\")\n  }\n  \n  elev &lt;- elev[elev$xy %in% slo$xy,]\n  asp &lt;- asp[asp$xy %in% asp$xy,]\n  nlcd &lt;- nlcd[nlcd$xy %in% asp$xy,]\n\n11. Combine elevation, slope, and aspect into one layer.\n\nlayers = cbind(nlcd, elev, asp, slo)\nhead(layers)\n\n         x       y CLASS_NAME              xy        x       y USGS_1_n38w109\n1 -1128900 1725120          7 -11289001725120 -1128900 1725120       2066.122\n2 -1128870 1725120          5 -11288701725120 -1128870 1725120       2065.845\n3 -1128840 1725120          7 -11288401725120 -1128840 1725120       2065.617\n4 -1128810 1725120          7 -11288101725120 -1128810 1725120       2065.518\n5 -1128780 1725120          7 -11287801725120 -1128780 1725120       2065.893\n6 -1128750 1725120          7 -11287501725120 -1128750 1725120       2066.677\n               xy        x       y   aspect              xy        x       y\n1 -11289001725120 -1128900 1725120 169.6976 -11289001725120 -1128900 1725120\n2 -11288701725120 -1128870 1725120 168.8512 -11288701725120 -1128870 1725120\n3 -11288401725120 -1128840 1725120 179.2474 -11288401725120 -1128840 1725120\n4 -11288101725120 -1128810 1725120 200.8665 -11288101725120 -1128810 1725120\n5 -11287801725120 -1128780 1725120 224.8503 -11287801725120 -1128780 1725120\n6 -11287501725120 -1128750 1725120 254.8164 -11287501725120 -1128750 1725120\n     slope              xy\n1 2.924126 -11289001725120\n2 2.929391 -11288701725120\n3 2.805793 -11288401725120\n4 2.517779 -11288101725120\n5 1.947378 -11287801725120\n6 1.820867 -11287501725120\n\n#Remove xys that are repeated in each layer\nlayers = layers[c(3,7,11,15,13:14)]\nnames(layers) = c(\"nlcd\", \"elevation\", \"aspect\", \"slope\",\"x\", \"y\")\n\n# turn aspect into categorical\naspect_categorical = rep(NA, nrow(layers))\naspect_categorical[layers$aspect &lt; 45 | layers$aspect &gt;= 315] = \"N\"\naspect_categorical[layers$aspect &gt;= 45 & layers$aspect &lt; 135] = \"E\"\naspect_categorical[layers$aspect &gt;= 135 & layers$aspect &lt; 225] = \"S\"\naspect_categorical[layers$aspect &gt;= 225 & layers$aspect &lt; 315] = \"W\"\ntable(aspect_categorical)\n\naspect_categorical\n    E     N     S     W \n 6392  7829 31078 65504 \n\ntable(is.na(aspect_categorical))\n\n\n FALSE \n110803 \n\nlayers$aspect_categorical = aspect_categorical\nhead(layers)\n\n  nlcd elevation   aspect    slope        x       y aspect_categorical\n1    7  2066.122 169.6976 2.924126 -1128900 1725120                  S\n2    5  2065.845 168.8512 2.929391 -1128870 1725120                  S\n3    7  2065.617 179.2474 2.805793 -1128840 1725120                  S\n4    7  2065.518 200.8665 2.517779 -1128810 1725120                  S\n5    7  2065.893 224.8503 1.947378 -1128780 1725120                  S\n6    7  2066.677 254.8164 1.820867 -1128750 1725120                  W\n\n#write.table(layers,\"layer1.txt\",sep=\",\",col.names=TRUE, quote=FALSE)\n\nlayers2 &lt;- layers #change to layers2 simply to avoid confusion with \"layer\" term in function \n#below\n\n\n#NOTE: Script may contains Demonstration code that will subset number of locations to speed up processing of data during a course exercise. To prevent this, skip this line of code above.\n12. We can now begin the task of sampling each of our locations using the code below. This code was created by Ryan Nielsen of West Inc. and was very helpful in this exercise. Alternatively, we could have extracted each covariate layer by layer and included it in our dataset.\n\n# grab values for points created above\ngrab.values = function(layer, x, y){\n    # layer is data.frame of spatial layer, with values 'x', 'y', and ____?\n    # x is a vector \n    # y is a vector\n    if(length(x) != length(y)) stop(\"x and y lengths differ\")\n    z = NULL\n    for(i in 1:length(x)){\n        dist = sqrt((layer$x - x[i])^2 + (layer$y-y[i])^2) \n        #Could adjust this line or add another line to calculate moving window or \n        #distance to nearest feature\n        z = rbind(z, layer[dist == min(dist),][1,])\n    }\n    return(z)\n}\n\n#Grab all values from muleys for each layer in r\ntest = grab.values(layers2, muleys$X, muleys$Y)\nhead(test)\n\n     nlcd elevation   aspect   slope        x       y aspect_categorical\n912     6  2411.335 273.0425 2.57144 -1115250 1725090                  W\n9121    6  2411.335 273.0425 2.57144 -1115250 1725090                  W\n9122    6  2411.335 273.0425 2.57144 -1115250 1725090                  W\n9123    6  2411.335 273.0425 2.57144 -1115250 1725090                  W\n9124    6  2411.335 273.0425 2.57144 -1115250 1725090                  W\n9125    6  2411.335 273.0425 2.57144 -1115250 1725090                  W\n\n##NOTE that all values are the same but this is not correct.\n##What is the problem here and how do we fix it?\n\n#Need to grab Albers XY not UTM as in muleys above\nmuleys &lt;- as.data.frame(sf::st_coordinates(deer.albers))\n\n# grab all values for used and available points based on combined layer data set\n# can take 5+ minutes\nused = grab.values(layers2, muleys$X, muleys$Y)\n# used$x = muleys$X\n# used$y = muleys$Y\n# used$animal_id = muleys$id\nused$use = 1\nhead(used)\n\n       nlcd elevation   aspect    slope        x       y aspect_categorical use\n15082     7  2072.580 100.5536 4.617643 -1127910 1724130                  E   1\n44173     7  2351.774 216.1922 2.682198 -1117020 1722240                  S   1\n441731    7  2351.774 216.1922 2.682198 -1117020 1722240                  S   1\n441732    7  2351.774 216.1922 2.682198 -1117020 1722240                  S   1\n441733    7  2351.774 216.1922 2.682198 -1117020 1722240                  S   1\n87850     7  2208.532 188.2723 4.401105 -1119990 1719360                  S   1\n\n\n13. We also need to get some measure of what is available for our mule deer population (2nd order selection) or for each mule deer (3rd order selection). We really do not understand the need for 2nd order selection unless you are looking at deer across different landscapes but hardly seems necessary for deer occupying similar areas such as our mule deer in southwestern Colorado. Below we will focus on 3rd order selection with used locations for each deer being compared to available locations randomly determined within each deer’s MCP.\n\n#Create MCP for all locations for each deer by ID (3nd order selection).\n#3 lines below needed for mcp function to work\nmuleys2 &lt;- as.data.frame(deer.albers)\ndeer.spdf &lt;- SpatialPointsDataFrame(muleys,muleys2)\n\ncp = mcp(deer.spdf[,2],percent=100)\nas.data.frame(cp)\n\n     id         area\nD12 D12 3.385437e-03\nD8   D8 3.452912e+02\n\n#Determine the habitat available using all code below\n#First create random sample of points in each polygon\nrandom &lt;- sapply(slot(cp, 'polygons'), function(i) spsample(i, n=50, type='random', offset=c(0,0)))\nplot(cp) ; points(random[[2]], col='red', pch=3, cex=.5)#The number in double brackets changes polygons stack into a single SpatialPoints object\n\n\n\nrandom.merged &lt;- do.call('rbind', random)\n#Extract the original IDs\nids &lt;- sapply(slot(cp, 'polygons'), function(i) slot(i, 'ID'))\n#Determine the number of ACTUAL sample points generated for each polygon\nnewpts &lt;- sapply(random, function(i) nrow(i@coords))\nnewpts #Nice check of how many points generated per polygon \n\n[1] 50 50\n\n# generate a reconstituted vector of point IDs\npt_id &lt;- rep(ids, newpts)\n \n# promote to SpatialPointsDataFrame\nrandom.final &lt;- SpatialPointsDataFrame(random.merged, data=data.frame(poly_id=pt_id))\n \n# make 'random.final' a data.frame\nrandom.df = as.data.frame(random.final)\nnames(random.df) = c(\"ID\",\"x\",\"y\")\n# can take 5+ minutes\navailable = grab.values(layers2, random.df$x, random.df$y)\n# available$x = random.df$x\n# available$y = random.df$y\n# available$animal_id = pt_id\navailable$use = 0\nhead(available)\n\n       nlcd elevation    aspect    slope        x       y aspect_categorical\n15082     7  2072.580 100.55357 4.617643 -1127910 1724130                  E\n15081     6  2074.966  90.18315 5.736788 -1127940 1724130                  E\n150821    7  2072.580 100.55357 4.617643 -1127910 1724130                  E\n150811    6  2074.966  90.18315 5.736788 -1127940 1724130                  E\n150812    6  2074.966  90.18315 5.736788 -1127940 1724130                  E\n150822    7  2072.580 100.55357 4.617643 -1127910 1724130                  E\n       use\n15082    0\n15081    0\n150821   0\n150811   0\n150812   0\n150822   0\n\n\n14. Bind together mule deer locations with covariates extracted (used) and random locations within each polygon by deer ID (available) into a master dataset for modeling (data). The (use) column identifies 1 as (used) and 0 as (available)\n\ndata = rbind(available, used)\n##A quick check of the data to determine if correct number of records.\n#''data.frame': 200 obs. of  9 variables:\n#100 locations used +\n#100 locations available (2 animals X 50 random locations)\n#= 100 #Confirmed in code below\n# nlcd              : num  7 6 7 7 7 7 7 7 7 6 ...\n# elevation         : int  2058 2058 2068 2068 2070 2072 2076 2062 2071 2071 ...\n# aspect            : num  105 278 105 80 135 ...\n# slope             : num  2.72 3.37 4.68 4.11 6.05 ...\n# x                 : num  -1127639 -1127610 -1127864 -1127805 -1127862 ...\n# y                 : num  1724257 1724271 1724091 1724218 1724174 ...\n# aspect_categorical: chr  \"E\" \"W\" \"E\" \"E\" ...\n# animal_id         : chr  \"D12\" \"D12\" \"D12\" \"D12\" ...\n# use               : num  0 0 0 0 0 0 0 0 0 0 ...\n\n15. The above code is for 3rd order selection within home range of each deer. We could also look at 3rd order selection within a buffered circle around each mule deer location that is common in Discrete Choice Models. The code is similar except the initial steps of creating buffered polygons and obviously includes a lot more polygons than simply MCPs for each deer. Determining the daily distance moved was done in Chapter 3 but new code is available to estimate for each deer or all deer combined.\n\nsettbuff=st_buffer(deer.albers,500) %&gt;% st_as_sfc()\n\n#Determine the habitat available using all code below\n#First create random sample of points in each polygon\n\nranbuff &lt;- st_sample(settbuff,size=c(3,3),exact = T, type=\"random\", by_polygon = TRUE)\n#ranbuff &lt;- st_sample(settbuff,size=c(3,3), type=\"random\", by_polygon = TRUE)\nplot(st_geometry(settbuff)) ; plot(st_geometry(ranbuff),add=T,col='red', pch=3, cex=.5)\n\n#Now to assign IDs, we needed help (https://github.com/r-spatial/sf/issues/1014) so thank you @edzer!\nn = c(1,2,3)\nranbuffwIDs &lt;- st_sf(id = rep(seq_along(n), n), geom = ranbuff)\n\nranbuff.merged &lt;- do.call('rbind', ranbuffwIDs)\n\nst_sf(id= rep(seq_along(ranbuff.merged),ranbuff.merged), geom = randbuff)\npoints_sf_joined &lt;- \n  st_join(ranbuff, settbuff) %&gt;%  # spatial join to get intersection of points and poly\n  filter(!is.na(num)) # 'rgn_name'num' just one col from the polygon data that I chose to filter on, could use any. The idea is to get only the points that fall in the polygon\n\nbuff_ids &lt;- sf::st_join(settbuff,ranbuff.merged)\n#Extract the original IDs\nbuff_ids &lt;- rownames(ranbuff.merged) #sapply(slot(settbuff, 'polygons'), function(i) slot(i, 'ID'))\nbuff_ids &lt;- paste(settbuff$id, buff_ids, sep=\"_\")\n#Determine the number of ACTUAL sample points generated for each polygon\nbuffpts &lt;- sapply(ranbuff, function(i) nrow(i@coords))\nbuffpts[1:20] #Nice check of how many points generated per polygon \n# generate a reconstituted vector of point IDs\nbuffpt_id &lt;- rep(buff_ids, buffpts)\n \n# promote to SpatialPointsDataFrame\nbuff.final &lt;- SpatialPointsDataFrame(ranbuff.merged, data=data.frame(poly_id=buffpt_id))\n \n#Plot buff.final on buffered circles\nplot(settbuff) ; points(buff.final, col=\"red\",pch=3, cex=0.5)\n\n# make 'buff.final' a data.frame\nbuffer.df = as.data.frame(buff.final)\nnames(buffer.df) = c(\"ID\",\"x\", \"y\")\nhead(buffer.df)\nstr(random.df)\nstr(buffer.df)\n\n# can take 5+ minutes\nbuff_avail = grab.values(layers2, buffer.df$x, buffer.df$y)\nbuff_avail$x = buffer.df$x\nbuff_avail$y = buffer.df$y\nbuff_avail$animal_id = buffpt_id\nbuff_avail$use = 0\n\ndata2 = rbind(buff_avail, used)\n\n#Save workspace so all analysis are available\nsave.image(\"RSF_dataprep.RData\")\n\n#Before closing, let's save the \"used\" and available data set to use in the next exercise\nwrite.table(used, \"MD_used.txt\")\nwrite.table(available, \"MD_avail.txt\")\n\n16. We are going to focus the remainder of this chapter on Selection Ratios and Resource Selection Functions (RSFs) because Selection Ratios identify a general use of habitat given what is available that can be further explored and studied through use of RSFs. Resource Selection Functions are spatially-explicit models that predict the (relative) probability of use by an animal at a given area/location during a given time, based on the environmental conditions that influence or account for selection. There are numerous types of RSFs that can be performed based on the availability of data collected during the study and there are volumes of literature devoted to the topic of resource selection and sampling designs for radiotelemetry studies (Manly et al. 2002, Cooper and Millspaugh 2001, Erickson et al. 2001, Leban et al. 2001)."
  },
  {
    "objectID": "MuledeerSR.html",
    "href": "MuledeerSR.html",
    "title": "\n33  Selection Ratios\n",
    "section": "",
    "text": "We are going to focus the remainder of this chapter on Selection Ratios and Resource Selection Functions (RSFs) because Selection Ratios identify a general use of habitat given what is available that can be further explored and studied through use of RSFs. Resource Selection Functions are spatially-explicit models that predict the (relative) probability of use by an animal at a given area/location during a given time, based on the environmental conditions that influence or account for selection. There are numerous types of RSFs that can be performed based on the availability of data collected during the study and there are volumes of literature devoted to the topic of resource selection and sampling designs for radiotelemetry studies (Manly et al. 2002, Cooper and Millspaugh 2001, Erickson et al. 2001, Leban et al. 2001).\nSelection Ratio basic functions\nwidesI may be used to explore resource selection by animals when designs I occur (i.e., habitat use and availability are measured at the population level because individual animals are not identified). The Manly selectivity measure (selection ratio = used/available) is computed and preference/avoidance is tested for each habitat, and the differences between selection ratios are computed and tested (Manly et al. 2002).\nwidesII computes the selection ratios with design II data (i.e., the same availability for all animals, but use is measured for each one). An example would be to place a minimum convex polygon around all animal locations throughout a study site and define this as “available” to all animals.\nwidesIII computes the selection ratios for design III data (i.e., use and the availability are measured for each animal with use and availability unique to each individuals movements and habitat use).\nNote that all these methods rely on the following hypotheses: (i) independence between animals, and (ii) all animals are selecting habitat in the same way (in addition to “traditional” hypotheses in these kinds of studies: no territoriality, all animals having equal access to all available resource units, etc. (Manly et al. 2002).\n1. Open the script MuleDeerSR.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(adehabitatHS)\n\n3. Load the mule deer dataset we used in the previous exercise with 5 habitat categories: 1 = Sunflower,summer crops, random crops, grassland 2 = Winter crops 3 = Alfalfa 4 = Forest 5 = Shrubland\n\nMDsr &lt;- read.csv(\"data/MD_winter12.csv\",header=T)\n#Remove deer that cause errors in plot function later\nMDsr &lt;- subset(MDsr,MDsr$animal_id !=\"647579A\")\nMDsr$animal_id &lt;- factor(MDsr$animal_id)\nused &lt;- subset(MDsr, MDsr$use == 1)\nused &lt;- used[c(-2,-3,-5:-6,-8:-15)]\nused &lt;- xtabs(~used$animal_id + used$crop, used)\nused &lt;- as.data.frame.matrix(used[1:13, 1:5])\n\nrand &lt;- subset(MDsr, MDsr$use == 0)\nrand &lt;- rand[c(-2,-3,-5:-6,-8:-15)]\nrand &lt;- xtabs(~rand$animal_id + rand$crop, rand)\nrand &lt;- as.data.frame.matrix(rand[1:13, 1:5])\n\n# PVT Code for VegRSF #\npvt.W &lt;- widesIII(used,rand,avknown = FALSE, alpha = 0.1)\npvt.W\nplot(pvt.W)\n\nNext we will run on distance to road binned into 10 categories\n\n#Now run on distance to roads binned into 10 categories\nMDsr_road &lt;- read.csv(\"MD_winter12.csv\",header=T)\n#Delete deer that have limited data and will result in errors in code below\nMDsr_road &lt;- subset(MDsr_road,MDsr_road$animal_id !=\"647582A\" & MDsr_road$animal_id \n!=\"647584A\" & MDsr_road$animal_id !=\"647572A\" & MDsr_road$animal_id !=\"647574A\" &\nMDsr_road$animal_id !=\"647593A\" )\nMDsr_road$animal_id &lt;- factor(MDsr_road$animal_id)\n\n#Bin roads into 4 categories instead of 10\nMDsr_road$NewRoad &lt;- as.factor(MDsr_road$BinRoad)\nlevels(MDsr_road$NewRoad)&lt;-list(class1=c(\"0-200\",\"200-400\"), class2=c(\"400-600\",\"600-800\"),\nclass3=c(\"800-1000\",\"1000-12000\",\"1200-1400\"),class4=c(\"1400-1600\",\"1600-1800\",\"1800-2000\"))\n\nused_road &lt;- subset(MDsr_road, MDsr_road$use == 1)\nused_road &lt;- used_road[c(-2,-3,-5:-6,-8:-12)]\nused_road &lt;- xtabs(~used_road$animal_id + used_road$NewRoad, used_road)\nused_road &lt;- as.data.frame.matrix(used_road[1:9, 1:4])\n\nrand_road &lt;- subset(MDsr_road, MDsr_road$use == 0)\nrand_road &lt;- rand_road[c(-2,-3,-5:-6,-8:-12)]\nrand_road &lt;- xtabs(~rand_road$animal_id + rand_road$NewRoad, rand_road)\nrand_road &lt;- as.data.frame.matrix(rand_road[1:9, 1:4])\n\npvt.road &lt;- widesIII(used_road,rand_road,avknown = FALSE, alpha = 0.1)\npvt.road\nplot(pvt.road)"
  },
  {
    "objectID": "LogisticRSF.html",
    "href": "LogisticRSF.html",
    "title": "\n34  Logistic Regression\n",
    "section": "",
    "text": "Resource selection requires “used” and “available” habitats and the study designs would take up an entire course all on there own. In this section, we hope to show how we can go about this approach all in R and not need to involve excel spreadsheets with multiple columns of data. More details on methods to estimate resource selection functions (RSFs) or resource selection probability functions (RSPFs) can be found in the literature (Manly et al. 2002, Millspaugh et al. 2006, Johnson et al. 2006). We do not expect you to be experts in RSFs after this section but we want you to be able to implement these methods in R after determining study design, data collection protocol, and methodology to best achieve your research objectives.\n8.4.1 Logistic regression\nAs we move forward in this section, we are going to assume that your study design and data assessment prior to this section addresses any collinearity in predictor variables and a priori hypothesis were used to generate your models used in logistic regression. There are several ways to to calculate RSFs in R using logistic functions that can assess population level or intra-population variation. The use of General Linear Models with various function using the lme4 package is often used for estimating population-level models only. Alternatively, we can assess intra-population variation using the glmer function. Assessing intra-population variation is a mixed-model approach that provides a powerful and flexible tool for the analysis of balanced and unbalanced grouped data that are often common in wildlife studies that have correlation between observations within the same group or variation among individuals at the same site (Gillies et al. 2006).\n1. Open the script LogisticRSF.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(lme4)\nlibrary(AICcmodavg)\nlibrary(adehabitatHR)\n\n3. Load files for each season in the mule deer dataset We may need to identify some numerical data as factors for analysis prior to implementing resource selection analysis.\n\ndata_1 &lt;- read.csv(\"data/MD_winter12.csv\",header=T)\n\n############################### MODELING ###############################\ndata_1$crop=as.factor(data_1$crop)\n#dataset$SEASON=factor(dataset$SEASON)\ndata_1[,3:4]=scale(data_1[,3:4],scale=TRUE)#standardize data to mean of zero\n\n4. We may need to use code that changes Reference Categories of our data. For our analysis we are going to define reference category of habitat as crop= 1. Crop category 1 is sunflower which was the crop of interest but was not selected for based on Selection Ratios in Exercise 8.4.\n\nfit1 = glmer(use ~ relevel(crop,\"1\")+(1|animal_id), data=data_1, family=binomial(link=\n  \"logit\"),nAGQ = 0)#Sunflower and cover model\nfit2 = glmer(use ~ d_cover+(1|animal_id), data=data_1, family=binomial(link=\"logit\"),nAGQ\n  = 0)#Distance to cover only model\nfit3 = glmer(use ~ d_roads+(1|animal_id), data=data_1, family=binomial(link=\"logit\"),nAGQ\n  = 0)#Distance to roads only model\nfit4 = glmer(use ~ d_cover+d_roads+(1|animal_id), data=data_1, family=binomial(link=\"logit\"),\n  nAGQ = 0)#Distance to cover and roads model\nfit5 = glmer(use ~ 1|animal_id, data=data_1, family=binomial(link=\"logit\"),nAGQ = 0)#Intercept model\n\n5. We can view the results of our modeling procedure to select the best model using Akaike’s Information Criteria (AIC; Burnham and Anderson 2002).\n\nfit1\nfit2\nfit3\nfit4\nfit5\n\nAIC(fit1,fit2,fit3,fit4,fit5)\n\nmynames &lt;- paste(\"fit\", as.character(1:5), sep = \"\")\nmyaicc &lt;- aictab(list(fit1,fit2,fit3,fit4,fit5), modnames = mynames)\nprint(myaicc, LL = FALSE)\n\n6. Our top model (fit 1) has all the support in this case indicating that during winter 2012 the mule deer were selecting for each habitat over sunflower. Considering sunflower is not available during the winter months this makes perfect sense. Looking at parameter estimates and confidence intervals for the additional habitat categories in fit 1 we see that forest (category 4) is most selected habitat followed by shrub (category 5). This is only a simply way to look at habitat, however, we used more animals that were on the air for several years and also could look at distance to habitat instead of representing habitat as categorical data.\n\nper1_se &lt;- sqrt(diag(vcov(fit1)))\n# table of estimates with 95% CI\ntab_per1 &lt;- cbind(Est = fixef(fit1), LL = fixef(fit1) - 1.96 * per1_se, UL = fixef(fit1)\n  + 1.96 * per1_se)\ntab_per1\n\n7. We can then create a surface of predictions from our top model indicating where in our study site we might find the highest probability of use. To do this, we need to export a text file from our “layer” created in Exercise 8.3.\n\nlayer1 &lt;- read.table(\"layer1.txt\",sep=\",\")\nnames(layer1) = c(\"crop\", \"d_cover\", \"d_roads\",\"x\", \"y\")\n#Need to standardize the raw distance rasters first to match what we modeled\nlayer1[,2:3]=scale(layer1[,2:3],scale=TRUE)\nhead(layer1)\nlayer1$crop &lt;- as.factor(layer1$crop)\n\n# predictions based on best model \npredictions = predict(fit1, newdata=layer1, re.form=NA, type=\"link\")#based on the scale of the\n  #linear predictors\npredictions = exp(predictions)\nrange(predictions)\n\n#-----------------------------------------------------------------------------------\n# create Ascii grid of raw predictions if needed\nlayer1$predictions = predictions\n#preds = layer1\n#preds = SpatialPixelsDataFrame(points=preds[c(\"x\", \"y\")], data=preds)\n#preds = as(preds, \"SpatialGridDataFrame\")\n#names(preds)\n#writeAsciiGrid(preds, \"predictions.asc\", attr=13)#attr should be column number for 'predictions'\n\n#-----------------------------------------------------------------------------------\n# assign each cell or habitat unit to a 'prediction class'.\n# classes have (nearly) equal area, if the cells or habitat units have equal areas.\n# output is a vector of class assignments (higher is better).\nF.prediction.classes &lt;- function(raw.prediction, n.classes){\n  # raw.prediction = vector of raw (or scaled) RSF predictions\n  # n.classes = number of prediction classes.\n  pred.quantiles = quantile(raw.prediction, probs=seq(1/n.classes, 1-1/n.classes, \n  by=1/n.classes))\n  ans = rep(n.classes, length(raw.prediction))\n  for(i in (n.classes-1):1){\n    ans[raw.prediction &lt; pred.quantiles[i]] = i\n  }\n  return(ans)\n}\n\nlayer1$prediction.class = F.prediction.classes(layer1$predictions, 5)\ntable(layer1$prediction.class)\n\n##############################################\n# create map of RSF prediction classes in R\n# m = SpatialPixelsDataFrame(points = layer1[c(\"x\", \"y\")], data=layer1)\n# names(m)\n# par(mar=c(0,0,0,0))\n# image(m, attr=7, col=c(\"grey90\", \"grey70\", \"grey50\", \"grey30\", \"grey10\"))\n# par(lend=1)\n# legend(\"bottomright\", col=rev(c(\"grey90\", \"grey70\", \"grey50\", \"grey30\", \"grey10\")),\n#        legend=c(\"High\", \"Medium-high\", \"Medium\", \"Medium-low\", \"Low\"),\n#        title=\"Prediction Class\", pch=15, cex=1.0,bty != \"n\", bg=\"white\")\n\n# create Ascii grid of prediction classes\n#m = as(m, \"SpatialGridDataFrame\")\n#names(m)\n#writeAsciiGrid(m, \"PredictionClassess.asc\", attr=7)"
  },
  {
    "objectID": "NegBinomial.html",
    "href": "NegBinomial.html",
    "title": "\n35  Negative Binomial\n",
    "section": "",
    "text": "1. Open the script NegBinomial.Rmd” and run code directly from the script\n2. First we need to load the packages needed for the exercise\n\nlibrary(plyr)\nlibrary(adehabitatHR)\nlibrary(zoo)\nlibrary(MASS)#For nb models\nlibrary(sf)\nlibrary(terra)\nlibrary(sfheaders)\nlibrary(exactextractr)\nlibrary(dplyr)\n\n3. Now let’s have a separate section of code to include projection information we will use throughout the exercise. In previous versions, these lines of code were within each block of code\n\nll.crs &lt;- st_crs(4269) \nutm.crs &lt;- st_crs(9001) \nalbers.crs &lt;- st_crs(5070)\n\n4. Load files for the mule deer dataset and clean up the dataset as we have done in previous exercises\n\nmuleys&lt;-read.csv(\"data/DCmuleysedited.csv\", header=T, sep=\",\")\n\nmuleys$NewDate&lt;-as.POSIXct(muleys$GPSFixTime, format=\"%Y.%m.%d %H:%M:%S\", origin=\"1970-01-01\")\nmuleys &lt;- subset(muleys, muleys$id != \"D19\")\n\n##Sort Data\nmuleys &lt;- muleys[order(muleys$id, muleys$NewDate),]\n\nfix_rate &lt;- function(x){\n  print(paste(\"Individual:\", x$id[1]))\n  dates=range(x$NewDate)\n  print(paste(\"Range:\", dates))\n  days=as.numeric(round(diff(range(x$NewDate)), digits=0))\n  print(paste(\"Number of monitoring days:\", days))\n  sched=as.numeric(round(median(abs(diff(sapply(x$NewDate[2:nrow(x)], difftime, time1 = x$NewDate[1], units = \"mins\", simplify = T))))))\n  print(paste(\"Scheduled fix rate (min):\", sched))\n  expected=as.numeric(days)*round(1440/(as.numeric(sched)), digits=0)#1440 minutes in a day\n  print(paste(\"Expected number of positions:\", expected))\n  success &lt;- nrow(x)\n  print(paste(\"Number of recorded positions\", success))\n  percentfix &lt;- round(success/expected*100)\n  print(paste(\"Fix rate success = \", percentfix,\"%\", sep=\" \"))\n}\n\nfix_deer &lt;- ddply(muleys, .(as.factor(id)), fix_rate)\n\n[1] \"Individual: D12\"\n[1] \"Range: 2011-08-12 15:01:53\" \"Range: 2011-10-24 21:00:48\"\n[1] \"Number of monitoring days: 73\"\n[1] \"Scheduled fix rate (min): 180\"\n[1] \"Expected number of positions: 584\"\n[1] \"Number of recorded positions 120\"\n[1] \"Fix rate success =  21 %\"\n[1] \"Individual: D15\"\n[1] \"Range: 2011-10-12 00:02:03\" \"Range: 2012-08-31 09:00:51\"\n[1] \"Number of monitoring days: 324\"\n[1] \"Scheduled fix rate (min): 180\"\n[1] \"Expected number of positions: 2592\"\n[1] \"Number of recorded positions 2589\"\n[1] \"Fix rate success =  100 %\"\n[1] \"Individual: D16\"\n[1] \"Range: 2011-10-11 21:00:36\" \"Range: 2012-07-08 12:01:46\"\n[1] \"Number of monitoring days: 271\"\n[1] \"Scheduled fix rate (min): 180\"\n[1] \"Expected number of positions: 2168\"\n[1] \"Number of recorded positions 2157\"\n[1] \"Fix rate success =  99 %\"\n[1] \"Individual: D4\"\n[1] \"Range: 2010-09-13 21:08:35\" \"Range: 2012-03-22 15:01:40\"\n[1] \"Number of monitoring days: 556\"\n[1] \"Scheduled fix rate (min): 180\"\n[1] \"Expected number of positions: 4448\"\n[1] \"Number of recorded positions 1304\"\n[1] \"Fix rate success =  29 %\"\n[1] \"Individual: D6\"\n[1] \"Range: 2011-10-11 21:00:39\" \"Range: 2012-04-11 15:00:48\"\n[1] \"Number of monitoring days: 183\"\n[1] \"Scheduled fix rate (min): 180\"\n[1] \"Expected number of positions: 1464\"\n[1] \"Number of recorded positions 1455\"\n[1] \"Fix rate success =  99 %\"\n[1] \"Individual: D8\"\n[1] \"Range: 2010-09-13 22:07:35\" \"Range: 2012-03-01 00:02:08\"\n[1] \"Number of monitoring days: 534\"\n[1] \"Scheduled fix rate (min): 180\"\n[1] \"Expected number of positions: 4272\"\n[1] \"Number of recorded positions 971\"\n[1] \"Fix rate success =  23 %\"\n\n##TIME DIFF NECESSARY IN BBMM CODE\ntimediff &lt;- diff(muleys$NewDate)*60\n## remove first entry without any difference \nmuleys &lt;- muleys[-1,] \nmuleys$timelag &lt;-as.numeric(abs(timediff))\n##Remove locations greater than 5.5 hours apart in time\nmuleys &lt;- subset(muleys, muleys$timelag &lt; 19800)\nsummary(muleys$timelag)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3581   10792   10800   10795   10808   14401 \n\nmuleys &lt;- subset(muleys, muleys$X &gt; 680000)# & muleys$GPS.UTM.Easting != \"NA\")\nmuleys$id &lt;- factor(muleys$id)\n\n##Make a spatial data frame of locations after removing outliers\nmuleysSPDF &lt;- st_as_sf(muleys, coords = c(\"Long\", \"Lat\"), crs = ll.crs)\nplot(st_geometry(muleysSPDF), axes=T)#To visualize all locations\n\n\n\nutm.spdf &lt;- st_transform(muleysSPDF, data = muleys, crs = utm.crs)\nplot(st_geometry(utm.spdf), axes=T)#To visualize all locations\n\n\n\n##change muleysSPDF from UTM to Albers\nmuleys.spdf &lt;-st_transform(muleysSPDF, crs=albers.crs)\n#Subset locations by year for season-specific RSFs if needed\nwinter2012 &lt;- muleys\n# winter2012 &lt;- crop(muleys.spdf,muleysbuffSP)\n# winter2012$id &lt;- droplevels(winter2012$id)\n\n5. If we get some NA errors because our grid does not encompass our panther locations then we can expand the grid size extending beyond our locations using methods in an earlier exercise.\n\n# Create vectors of the x and y points using boundary box created around deer locations\nbb &lt;- st_bbox(muleys.spdf)\n     \nincrement = 1000\nminx=(min(bb$xmin)-(increment))\nmaxx=(max(bb$xmax)+(increment))\nminy=(min(bb$ymin)-(increment))\nmaxy=(max(bb$ymax)+(increment))\n\nmy_bbox = st_bbox(c(xmin = minx, xmax = maxx, \n                    ymin = miny, ymax = maxy),\n                  crs = 5070)\n\nAlbersSP &lt;- st_as_sfc(my_bbox)\n\n6. Now we need to set up our sample circles across our study area keeping in mind our discussions on “available” habitats. For this exercise, we will keep it simple by only including a polygon around our mule deer locations. This is for demonstration only, the appropriate study area should be specific to your study design and objectives. We also need to determine what is the appropriate size of our sample circles. In this case, we will use the mean daily movement distance for mule deer we determined to be 628 meters. This will be the radius of our sample circles.\n\ng = expand.grid(\n    x = seq(my_bbox$xmin, my_bbox$xmax, by = increment),\n    y = seq(my_bbox$ymin, my_bbox$ymax, by = increment))\n\ng.grid &lt;- st_as_sf(g, coords = c(\"x\", \"y\"), crs = albers.crs)\n\n#g_sfc &lt;- sfc_point(as.matrix(g)) %&gt;% \n#  st_set_crs(5070)\nplot(g)  \n\n\n\nmuleysbuffSP2=st_buffer(g.grid,500) %&gt;% st_as_sfc()\nplot(st_geometry(muleysbuffSP2))\nplot(AlbersSP, add=T, lty=2)\n\n\n\n\n7. We will start here by creating covariate layers specifically for the year of interest, in this case crop data from NRCS for 2011\n\n# Then clip buffer from crop11 layer\ncrop11 &lt;- rast(\"data/crop11clip.tif\")\nmulcrop11clip&lt;-crop(crop11, muleysbuffSP2)\n#Crop categories\n#1 = Sunflower,summer crops, random crops, grassland\n#2 = Winter crops\n#3 = Alfalfa\n#4 = Forest\n#5 = Shrubland\n\n#Reclassify into 5 habitat categories\nm11 &lt;- c(-Inf,0,NA, 5.5, 6.5, 1, 22.5, 24.5, 2, 26.5, 27.5, 2, 29.5, 30.5, 2, 35.5, 36.5, \n  3, 3.5, 4.5, 1, .5, 1.5, 1, 11.5, 12.5, 1, 27.5, 28.5, 1, 31.5, 33.5, 1,  42.5, 43.5, 1, \n  47.5, 49.5, 1, 58.5, 59.5, 1, 60.5, 61.5, 1, 65.5, 69.5, 1, 76.5, 77.5, 1, 110.5, \n  111.5, 1, 120.5, 124.5, 1, 130.5, 131.5, 1, 189.5, 190.5, 1, 194.5, 195.5, 1, 228.5,\n  229.5, 1, 140.5, 143.5, 4, 170.5, 171.5, 1, 180.5, 181.5, 1, 36.5, 37.5, 1, 151.5, \n  152.5, 5, 41.5, 42.5, 1, 204.5, 205.5, 1, 230,Inf,NA)\nrclmat11 &lt;- matrix(m11, ncol=3, byrow=TRUE)\ncrop11rc &lt;- classify(mulcrop11clip, rclmat11)\nplot(crop11rc)\n\n\n\n##Create cover layer\ncov &lt;- c(-Inf,0,NA, 1, 140, 0, 140.5, 143.5, 1, 151.5,\n  205.5, 0, 230,Inf,NA)#cover=forest only\nrclmatcov &lt;- matrix(cov, ncol=3, byrow=TRUE)\ncover &lt;- classify(mulcrop11clip, rclmatcov)\nplot(cover)\n\n\n\nd_cover &lt;- distance(crop11rc,target=\"1\")\nplot(d_cover)\n\n\n\n##Bring in roads layer\nroads&lt;-st_read(\"data/AlbersRoads.shp\")\n\nReading layer `AlbersRoads' from data source \n  `/Users/davidwalter/Library/CloudStorage/OneDrive-ThePennsylvaniaStateUniversity/WalterRprojects/Manual-of-Applied-Spatial-Ecology/data/AlbersRoads.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 42674 features and 6 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -1300892 ymin: 1621719 xmax: -989792.6 ymax: 1808098\nProjected CRS: NAD_1983_Albers\n\n#Clip using AlbersSP\ncliproads &lt;- st_intersection(roads, AlbersSP)\n\nplot(st_geometry(cliproads))\nplot(st_geometry(muleys.spdf),add=T,col=\"red\")\nplot(muleysbuffSP2, add=TRUE)\n\n\n\n\n8. The downside of creating distance to roads with spatstat in Exercise 8.2 is that it is not in a raster so we need to create a raster of distance to roads for every raster cell in layer1 before grabbing values or making our predictive surface.\nWe will start by using the Rasterize function to create a raster of the road shapefile with crop data used as a mask. A mask will give the spatial resolution and projection information to the raster you plan to create.\n\nroadrast &lt;- rasterize(cliproads,crop11rc, mask=TRUE)\nimage(roadrast)\n\n\n\nd_roadrast &lt;- distance(roadrast,target=\"1\")\n\n##MAKE ALL RASTER LAYERS DATAFRAMES TO COMBINE LATER\ncrop11df &lt;- as.data.frame(crop11rc, xy=TRUE)\n#Distance to cover\nd_covdf &lt;- as.data.frame(d_cover, xy=TRUE)\n#Distance to roads\nfinal_roaddf &lt;- as.data.frame(d_roadrast, xy=TRUE)\n\n#Combine data frames for Crop and Distance to Cover and Roads\nlayers1 = cbind(crop11df, d_covdf,final_roaddf)\nlayers1 = layers1[,c(3,6,9,7:8)]\nnames(layers1) = c(\"crop\",\"d_cover\",\"d_roads\",\"x\", \"y\")\n#write.table(layers1,\"layer1.txt\",sep=\",\",col.names=TRUE, quote=FALSE)\n\ncompareGeom(crop11rc,d_cover,d_roadrast)\n\n[1] TRUE\n\n#Now we need to extract all raster layers, grid and create a stack of all rasters\nr &lt;- c(crop11rc, d_cover, d_roadrast)\nnames(r) &lt;- c(\"crop11\",\"d_cover\",\"d_roads\")\nplot(r)\n\n\n\nnames(r)\n\n[1] \"crop11\"  \"d_cover\" \"d_roads\"\n\next &lt;- exact_extract(r, muleysbuffSP2, function(values, coverage_fraction) weighted.mean(values,coverage_fraction,na.rm=TRUE), stack_apply=TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n#NOTE above that for each buffered circle in the study area, the \"exact_extract\" function resulted in means for distance to cover and roads for all sample circles but \"crop\" resulted in mean cover categories so need to run separate with more appropriate code (see below).\n\n9. Code below extracts by land cover category and determines how many cells of each type were in each sample circle.\n\nsum_cover &lt;- function(x){\n  list(x %&gt;%\n    group_by(value) %&gt;%\n    summarize(total_area = sum(coverage_area)) %&gt;%\n    mutate(proportion = total_area/sum(total_area)))\n  \n}\n\n#extract the area of each raster cell covered by the plot and summarize\nex_crop &lt;- exact_extract(crop11rc, muleysbuffSP2, coverage_area = TRUE, summarize_df = TRUE, fun = sum_cover)\n\n\nex_crop &lt;- exact_extract(crop11rc, muleysbuffSP2, coverage_area = TRUE)\n# \n# #add polygon names that the results will be grouped by\n# names(ex_crop) &lt;- muleysbuffSP2$ID\nmuleysbuffSP3 &lt;- vect(muleysbuffSP2)\nex_crop &lt;- terra::extract(crop11rc,muleysbuffSP3,list=TRUE)\n# \nex_crop[11:15,]\n\n#What issue do you notice with habitat categories?\n\n############################################\n#Code here thanks to Tyler Wagner, PA Coop Unit, for creating this loop to summarize \n#proportions of habitat within each grid cell\n############################################\n##Created land use categories\nlus &lt;- 1:5\n##### Loop through and append missing land use categories to each grid cell\nex_crop_new &lt;- list()\nfor(i in 1:length(ex_crop_new)[1] ){\n  # Land use cats in a given cell\n  temp1 &lt;- unique(ex_crop$value[[i]])\n  # Give missing category 999 value\n  ma1 &lt;- match(lus, temp1, nomatch = 999, incomparables = NULL)\n  # Get location (category of missing land use type)\n  miss &lt;- which(ma1%in%999)\n  ex_crop_new[[i]] &lt;- c(ex_crop[[i]], miss)\n}\n\n# New summary of land use in a grid cell\ntab2 &lt;- lapply(ex_crop_new, table)\ntab2[[18]]\ntab[[18]]\n\ntab &lt;- list(ex_crop)\n\n# Proportions of all land cover types per grid cell\nprop &lt;- list()\nfor(i in 1:length(ex_crop)[1] ){\n  prop[[i]] &lt;- round((margin.table(ex_crop[[i]],1)/margin.table(ex_crop[[i]])),digits = 6)\n}\n#Function coredata is from the zoo package to convert the proportions from a list \n#to a matrix\nM &lt;- coredata(do.call(cbind, lapply(prop, zoo)))\ncolnames(M) &lt;- NULL\n#Transpose matrix so land cover become separate columns of data\nmatrix &lt;- t(M)\n#Now convert the matrix to a data frame so it is easier to manipulate\ndfland &lt;- as.data.frame(matrix)\n#Assing column names to land cover\ncolnames(dfland) &lt;- c(\"sunflower\",\"wintercrop\",\"alfalfa\",\"forest\",\"shrub\")\n#Write out csv with new nlcd circle percents\n#write.csv(dfland,paste(\".\", \"circl_perc_nlcd.csv\",sep=\"\"))\n\n10. Now that we have Land Cover in a similar format as the distance-to-derived data, we want to convert ext(the combined extracted rasters) into a data frame so it is easier to manipulate as well. The “extract” function in the raster package is supposed to be able to do this but does not work for some reason.\n\na &lt;- as.data.frame(ext)\nhabitat_units_buffwin12 &lt;- cbind(dfland, a)\n\n11. Now we need to convert to a data frame for nb modeling. Read in animal_locations.txt or convert to data frame from above\n\n#locations = read.table(\"deer_locations.txt\", sep='\\t', header=T)\nlocations.spdf &lt;- crop(muleys.spdf,muleysbuffSP)\nlocations.df = as.data.frame(locations.spdf)\n#locations.df = as.data.frame(winter2012)\nlocations &lt;- locations.df[c(-1,-3:-24)]\n\n#Add xy columns of circle centroids\nmbuff.xy &lt;- as.data.frame(grid.pts)\nstr(mbuff.xy)\nhabitat_units_buffwin12$x &lt;- mbuff.xy$x\nhabitat_units_buffwin12$y &lt;- mbuff.xy$y\n\nplot(habitat_units_buffwin12$x, habitat_units_buffwin12$y, type='p', cex=1.5)\npoints(locations$x, locations$y, col=\"red\", cex=0.5, pch=19)\nplot(mbuff.spdf, add=T)\n\n12. Calculate number of animal locations in each sampled habitat unit(see code in “count_locations.R”).\n\n# Source code file containing functions used below.\nsource(\"count_locations.R\")\n\npooled.locations = locations\ncolnames(pooled.locations) &lt;- c(\"ID\",\"x\",\"y\")\npooled.locations$ID = 1\nNB = F.count.relocations(locations.df = pooled.locations, \n    habitat.units.df = habitat_units_buffwin12, \n    habitat.unit.size = 628)\n# List of column names:\nnames(NB)\n\n#Look at the range in number of locations in our sample circles\nsummary(NB$n.locations)\n\n#Now run a population-level model for a few covariates (forest, road). NOTE: If you run models for each animal, will have to average coefficients across animals and top model(s)\nnb = glm.nb(n.locations ~ offset(log(total)) + forest + road, data=NB)\nsummary(nb)\n\n#-----------------------------------------------------------------------------------\n# Proportion of 0 counts in data\nsum(NB$n.locations == 0)/nrow(NB)\n\nnb.density = structure(\nfunction # Probability mass function for NB2\n\n# Description: This function gives the probability that a discrete random variable, X, \n#is exactly equal to some value  according to a NB2 distribution.\n# Returns: Pr(X=k)\n\n(k, \n### value at which to estimate probability\n### Pr(X=k)\nmu, \n### NB2 estimate of mu\ntheta\n### NB2 estimate of theta\n){\n\n    (gamma(theta+k)/(gamma(theta)*factorial(k)))*\n        (mu^k)*(theta^theta)/\n        ((mu+theta)^(theta+k))\n        \n})\n# Expected proportion under NB2 model\nnb.density(k=0, mu=mean(NB$n.locations), theta=0.1861) \n             # (Note: use estimated theta of the model output found in summary statement above)\n# The value above can be interpreted as: \n# \"A NB2 distribution with theta=0.1861 and mu=0.9196 should have an average of 32% zero values\"\n\n#Observed\nzero = NB$n.locations == 0\nsum(zero)   #total number of zeros\nmean(zero)  #proportion that are zeros\n\n#Expected based on NB distribution and our observed over-dispersions\ntheta = mean(NB$n.locations)^2 / (var(NB$n.locations) - mean(NB$n.locations)) \ncheck = rnegbin(n=10000, mu=mean(NB$n.locations), theta=theta)\ncheck.zeros = check == 0\nmean(check.zeros)\n\n#saveRDS(habitat_units_buffwin12, \"Exercise.8.6.rds\")"
  }
]